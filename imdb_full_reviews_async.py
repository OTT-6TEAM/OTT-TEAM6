# ==========================================================
# IMDB ì „ì²´ ë¦¬ë·° í¬ë¡¤ëŸ¬ - ë¹„ë™ê¸° ìµœì í™” ë²„ì „
# Pagination Keyë¥¼ ì´ìš©í•œ ì „ì²´ ë¦¬ë·° ìˆ˜ì§‘ + ë¹„ë™ê¸° ì²˜ë¦¬
# ==========================================================

import asyncio
import aiohttp
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
from pathlib import Path
import random
import json

# ==========================================================
# ì„¤ì •
# ==========================================================

# Rate Limiting (IMDBëŠ” ì—„ê²©í•˜ë¯€ë¡œ ë³´ìˆ˜ì ìœ¼ë¡œ)
MAX_CALLS_PER_SECOND = 2
TIMEOUT = aiohttp.ClientTimeout(total=30, connect=10)
MAX_RETRIES = 3

# User-Agent Pool
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
]

# ì¶œë ¥ íŒŒì¼
OUTPUT_CSV = "imdb_reviews_full_async.csv"
OUTPUT_PARQUET = "imdb_reviews_full_async.parquet"
CHECKPOINT_FILE = "imdb_reviews_checkpoint.json"

# í†µê³„
stats = {
    "series_total": 0,
    "series_success": 0,
    "series_failed": 0,
    "reviews_total": 0,
    "requests": 0,
    "start_time": None
}

# ==========================================================
# Rate Limiter
# ==========================================================
class RateLimiter:
    def __init__(self, rate):
        self.rate = rate
        self.tokens = rate
        self.updated_at = time.monotonic()
        self.lock = asyncio.Lock()
    
    async def acquire(self):
        async with self.lock:
            now = time.monotonic()
            elapsed = now - self.updated_at
            self.tokens = min(self.rate, self.tokens + elapsed * self.rate)
            self.updated_at = now
            
            if self.tokens < 1:
                sleep_time = (1 - self.tokens) / self.rate
                await asyncio.sleep(sleep_time)
                self.tokens = 1
            
            self.tokens -= 1

rate_limiter = RateLimiter(MAX_CALLS_PER_SECOND)

# ==========================================================
# HTML ê°€ì ¸ì˜¤ê¸°
# ==========================================================
async def fetch_html(session, url, retry=0, method='GET', data=None):
    """HTML í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (GET/POST ì§€ì›)"""
    if retry >= MAX_RETRIES:
        return None
    
    await rate_limiter.acquire()
    stats["requests"] += 1
    
    headers = {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
    }
    
    try:
        if method == 'POST':
            async with session.post(url, headers=headers, data=data, timeout=TIMEOUT) as resp:
                return await handle_response(session, url, resp, retry, method, data)
        else:
            async with session.get(url, headers=headers, timeout=TIMEOUT) as resp:
                return await handle_response(session, url, resp, retry, method, data)
    
    except asyncio.TimeoutError:
        if retry < MAX_RETRIES - 1:
            await asyncio.sleep(2 ** retry)
            return await fetch_html(session, url, retry + 1, method, data)
        return None
    
    except Exception as e:
        if retry < MAX_RETRIES - 1:
            await asyncio.sleep(2 ** retry)
            return await fetch_html(session, url, retry + 1, method, data)
        return None

async def handle_response(session, url, resp, retry, method, data):
    """ì‘ë‹µ ì²˜ë¦¬"""
    if resp.status == 429 or resp.status == 503:
        wait_time = 5 * (retry + 1)
        print(f"âš ï¸  Rate limited, waiting {wait_time}s...")
        await asyncio.sleep(wait_time)
        return await fetch_html(session, url, retry + 1, method, data)
    
    if resp.status == 404:
        return None
    
    if resp.status != 200:
        if retry < MAX_RETRIES - 1:
            await asyncio.sleep(2 ** retry)
            return await fetch_html(session, url, retry + 1, method, data)
        return None
    
    return await resp.text()

# ==========================================================
# ë¦¬ë·° íŒŒì‹±
# ==========================================================
def parse_review_block(soup, imdb_id):
    """soupì—ì„œ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ íŒŒì‹±"""
    review_blocks = soup.select(".review-container")
    reviews = []
    
    for block in review_blocks:
        # ì œëª©
        title_elem = block.select_one(".title")
        title = title_elem.get_text(strip=True) if title_elem else None
        
        # ë‚´ìš©
        content_elem = block.select_one(".text.show-more__control")
        if not content_elem:
            content_elem = block.select_one(".text")
        content = content_elem.get_text(strip=True) if content_elem else None
        
        # í‰ì 
        rating_elem = block.select_one(".rating-other-user-rating span")
        rating = rating_elem.get_text(strip=True) if rating_elem else None
        
        # ì‘ì„±ì
        author_elem = block.select_one(".display-name-link a")
        if not author_elem:
            author_elem = block.select_one(".display-name-link")
        author = author_elem.get_text(strip=True) if author_elem else None
        
        # ë‚ ì§œ
        date_elem = block.select_one(".review-date")
        date = date_elem.get_text(strip=True) if date_elem else None
        
        # Helpful íˆ¬í‘œ
        helpful_elem = block.select_one(".actions")
        helpful = None
        if helpful_elem:
            import re
            helpful_text = helpful_elem.get_text()
            match = re.search(r'(\d+)\s+out of\s+(\d+)', helpful_text)
            if match:
                helpful = f"{match.group(1)}/{match.group(2)}"
        
        # Spoiler ì—¬ë¶€
        spoiler = "spoiler-warning" in str(block)
        
        reviews.append({
            "imdb_id": imdb_id,
            "review_title": title,
            "review_text": content,
            "review_rating": rating,
            "review_author": author,
            "review_date": date,
            "helpful_votes": helpful,
            "is_spoiler": spoiler
        })
    
    return reviews

# ==========================================================
# ì „ì²´ ë¦¬ë·° ìˆ˜ì§‘ (Pagination ì§€ì›)
# ==========================================================
async def fetch_all_reviews_for_series(session, imdb_id, series_title="", max_pages=None):
    """
    í•œ ì‹œë¦¬ì¦ˆì˜ ëª¨ë“  ë¦¬ë·° ìˆ˜ì§‘ (paginationKey ì´ìš©)
    
    Args:
        max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ì œí•œ (Noneì´ë©´ ì „ì²´)
    """
    base_url = f"https://www.imdb.com/title/{imdb_id}/reviews"
    ajax_url = f"https://www.imdb.com/title/{imdb_id}/reviews/_ajax"
    
    all_reviews = []
    page_count = 0
    
    try:
        # 1. ì²« í˜ì´ì§€ (GET)
        html = await fetch_html(session, base_url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        all_reviews.extend(parse_review_block(soup, imdb_id))
        page_count += 1
        
        # 2. Pagination Key ì°¾ê¸°
        load_more = soup.select_one("div.load-more-data")
        if not load_more:
            return all_reviews
        
        pagination_key = load_more.get("data-key")
        
        # 3. Ajax í˜ì´ì§€ ìˆœíšŒ (POST)
        while pagination_key:
            if max_pages and page_count >= max_pages:
                break
            
            # POST ìš”ì²­
            payload = {"paginationKey": pagination_key}
            html = await fetch_html(session, ajax_url, method='POST', data=payload)
            
            if not html:
                break
            
            ajax_soup = BeautifulSoup(html, 'html.parser')
            new_reviews = parse_review_block(ajax_soup, imdb_id)
            
            if not new_reviews:  # ë” ì´ìƒ ë¦¬ë·° ì—†ìŒ
                break
            
            all_reviews.extend(new_reviews)
            page_count += 1
            
            # ë‹¤ìŒ í‚¤ ì°¾ê¸°
            load_more = ajax_soup.select_one("div.load-more-data")
            pagination_key = load_more.get("data-key") if load_more else None
            
            # ì•½ê°„ì˜ ë”œë ˆì´ (rate limit ë°©ì§€)
            await asyncio.sleep(0.5)
        
        stats["reviews_total"] += len(all_reviews)
        return all_reviews
    
    except Exception as e:
        print(f"âŒ Error for {imdb_id} ({series_title}): {e}")
        return all_reviews

# ==========================================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
# ==========================================================
def save_checkpoint(processed_ids, results):
    """ì¤‘ê°„ ì €ì¥"""
    checkpoint = {
        'processed_ids': list(processed_ids),
        'stats': stats.copy(),
        'timestamp': datetime.now().isoformat()
    }
    with open(CHECKPOINT_FILE, 'w') as f:
        json.dump(checkpoint, f)
    
    # ì¤‘ê°„ ê²°ê³¼ë„ ì €ì¥
    if results:
        df = pd.DataFrame(results)
        df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')

def load_checkpoint():
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    if Path(CHECKPOINT_FILE).exists():
        with open(CHECKPOINT_FILE, 'r') as f:
            checkpoint = json.load(f)
            return set(checkpoint.get('processed_ids', []))
    return set()

# ==========================================================
# ë©”ì¸ ì‹¤í–‰
# ==========================================================
async def main(input_csv_path, vote_threshold=10, max_reviews_per_page=None):
    """
    ì „ì²´ ë¦¬ë·° ìˆ˜ì§‘
    
    Args:
        input_csv_path: TMDB CSV íŒŒì¼
        vote_threshold: ìµœì†Œ vote_count
        max_reviews_per_page: í˜ì´ì§€ë‹¹ ìµœëŒ€ ë¦¬ë·° ìˆ˜ (Noneì´ë©´ ì „ì²´)
    """
    print("=" * 90)
    print("ğŸ¬ IMDB ì „ì²´ ë¦¬ë·° í¬ë¡¤ëŸ¬ (ë¹„ë™ê¸° ìµœì í™”)")
    print("=" * 90)
    
    stats["start_time"] = datetime.now()
    t0 = datetime.now()
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df = pd.read_csv(input_csv_path)
    df_filtered = df[(df['vote_count'] >= vote_threshold) & (df['imdb_id'].notna())]
    
    print(f"âœ… ì „ì²´ ì‹œë¦¬ì¦ˆ: {len(df):,}ê°œ")
    print(f"âœ… í•„í„°ë§ (vote_count>={vote_threshold} & imdb_id ì¡´ì¬): {len(df_filtered):,}ê°œ")
    
    if len(df_filtered) == 0:
        print("âš ï¸  ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    processed_ids = load_checkpoint()
    series_list = df_filtered[['id', 'title', 'imdb_id']].to_dict('records')
    
    if processed_ids:
        print(f"ğŸ“Œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {len(processed_ids):,}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
        series_list = [s for s in series_list if s['imdb_id'] not in processed_ids]
        print(f"ğŸ“Œ ë‚¨ì€ ì‘ì—…: {len(series_list):,}ê°œ")
    
    if len(series_list) == 0:
        print("âœ… ëª¨ë“  ë°ì´í„°ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    stats["series_total"] = len(series_list)
    
    # 3. í¬ë¡¤ë§
    print(f"\nğŸš€ í¬ë¡¤ë§ ì‹œì‘")
    print(f"âš™ï¸  Rate Limit: {MAX_CALLS_PER_SECOND}íšŒ/ì´ˆ")
    print(f"â±ï¸  ì˜ˆìƒ ì‹œê°„: ì‹œë¦¬ì¦ˆë‹¹ í‰ê·  30ì´ˆ â†’ ì´ {len(series_list)*30/60:.0f}ë¶„")
    
    connector = aiohttp.TCPConnector(
        limit=10,
        force_close=False,
        enable_cleanup_closed=True
    )
    
    all_results = []
    batch_size = 10  # í•œ ë²ˆì— ì²˜ë¦¬í•  ì‹œë¦¬ì¦ˆ ìˆ˜
    
    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as session:
        for i in range(0, len(series_list), batch_size):
            batch = series_list[i:i+batch_size]
            
            # ë°°ì¹˜ ì²˜ë¦¬
            tasks = [
                fetch_all_reviews_for_series(session, s['imdb_id'], s['title'], max_reviews_per_page)
                for s in batch
            ]
            
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for series, reviews in zip(batch, batch_results):
                if isinstance(reviews, list):
                    all_results.extend(reviews)
                    processed_ids.add(series['imdb_id'])
                    stats["series_success"] += 1
                    print(f"âœ… {series['title']}: {len(reviews):,}ê°œ ë¦¬ë·°")
                else:
                    stats["series_failed"] += 1
                    print(f"âŒ {series['title']}: ì‹¤íŒ¨")
            
            # ì£¼ê¸°ì  ì €ì¥
            if (i + batch_size) % 50 == 0:
                save_checkpoint(processed_ids, all_results)
            
            # ì§„í–‰ ìƒí™©
            elapsed = (datetime.now() - t0).total_seconds() / 60
            progress = stats["series_success"] + stats["series_failed"]
            rate = progress / elapsed if elapsed > 0 else 0
            eta = (stats["series_total"] - progress) / rate if rate > 0 else 0
            
            print(f"\nğŸ“Š ì§„í–‰: {progress}/{stats['series_total']} ({progress/stats['series_total']*100:.1f}%) | "
                  f"ì„±ê³µ: {stats['series_success']} | ì‹¤íŒ¨: {stats['series_failed']} | "
                  f"ì´ ë¦¬ë·°: {stats['reviews_total']:,}ê°œ | "
                  f"ìš”ì²­: {stats['requests']:,}íšŒ | "
                  f"ETA: {eta:.0f}ë¶„\n")
    
    # 4. ìµœì¢… ì €ì¥
    print("\nğŸ’¾ ìµœì¢… ì €ì¥ ì¤‘...")
    df_results = pd.DataFrame(all_results)
    df_results.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
    
    try:
        df_results.to_parquet(OUTPUT_PARQUET, index=False)
    except:
        pass
    
    if Path(CHECKPOINT_FILE).exists():
        Path(CHECKPOINT_FILE).unlink()
    
    # 5. í†µê³„
    elapsed = (datetime.now() - t0).total_seconds() / 60
    
    print("\n" + "=" * 90)
    print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 90)
    print(f"ğŸ“Œ ì‹œë¦¬ì¦ˆ: {stats['series_success']:,}/{stats['series_total']:,}ê°œ ì„±ê³µ")
    print(f"ğŸ“Œ ì´ ë¦¬ë·°: {len(df_results):,}ê°œ")
    print(f"ğŸ“Œ í‰ê· : {len(df_results)/stats['series_success']:.1f}ê°œ/ì‹œë¦¬ì¦ˆ")
    print(f"ğŸ“Œ ì´ ìš”ì²­: {stats['requests']:,}íšŒ")
    print(f"â±ï¸  ì´ ì‹œê°„: {elapsed:.1f}ë¶„ ({elapsed/60:.2f}ì‹œê°„)")
    print(f"ğŸ“Š ì†ë„: {stats['series_success']/elapsed:.1f}ê°œ/ë¶„")
    print("=" * 90)
    
    # ìƒ˜í”Œ
    print("\nğŸ“Š ìƒ˜í”Œ ë°ì´í„°:")
    print(df_results.head(3).to_string())
    print(f"\nâœ… ê²°ê³¼ íŒŒì¼: {OUTPUT_CSV}")

# ==========================================================
# ì‹¤í–‰
# ==========================================================
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='IMDB ì „ì²´ ë¦¬ë·° í¬ë¡¤ëŸ¬')
    parser.add_argument('--input', '-i', default='tv_series_2013_0101_0215_FULL.csv',
                        help='ì…ë ¥ CSV íŒŒì¼')
    parser.add_argument('--vote', '-v', type=int, default=10,
                        help='ìµœì†Œ vote_count (ê¸°ë³¸: 10)')
    parser.add_argument('--max-pages', '-m', type=int, default=None,
                        help='ì‹œë¦¬ì¦ˆë‹¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: ì „ì²´)')
    
    args = parser.parse_args()
    
    if not Path(args.input).exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
    else:
        asyncio.run(main(args.input, args.vote, args.max_pages))
