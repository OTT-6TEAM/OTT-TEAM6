{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfd840ed",
   "metadata": {},
   "source": [
    "## step 1 드라마 줄거리 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4099195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# RESET RUN (DRAMA ONLY) - Plot Topic (overview) / GPU SAFE\n",
    "# - 드라마 데이터셋만 따로: 임베딩/토픽 모델 분리\n",
    "# - max_df < min_df 에러 원천 차단 (토픽 내부 문서수까지 고려)\n",
    "# - outlier(-1) 과도하면 파라미터만 조금씩 완화하는 방식으로 튜닝\n",
    "# =========================================================\n",
    "\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# -------------------------\n",
    "# 0) 경로/설정\n",
    "# -------------------------\n",
    "PATH_DRAMA = f\"{}00_drama_main.parquet\"   # 파일명 그대로\n",
    "OUT_DRAMA  = \"drama_with_plot_topic.parquet\"\n",
    "OUT_INFO   = \"plot_topic_info_drama.csv\"\n",
    "OUT_KW     = \"plot_keywords_drama.parquet\"\n",
    "\n",
    "SEED = 42\n",
    "MIN_OVERVIEW_LEN = 30\n",
    "TOP_N_WORDS = 30\n",
    "\n",
    "# -------------------------\n",
    "# 1) 전처리\n",
    "# -------------------------\n",
    "def clean_text(s: str) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# -------------------------\n",
    "# 2) 로드 + 필터\n",
    "# -------------------------\n",
    "drama = pd.read_parquet(PATH_DRAMA).copy()\n",
    "\n",
    "need_cols = [\"imdb_id\", \"overview\"]\n",
    "missing = [c for c in need_cols if c not in drama.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"드라마 데이터에 필요한 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "drama[\"overview_clean\"] = drama[\"overview\"].apply(clean_text)\n",
    "drama = drama[drama[\"overview_clean\"].str.len() >= MIN_OVERVIEW_LEN].copy()\n",
    "docs = drama[\"overview_clean\"].tolist()\n",
    "\n",
    "print(\"✅ drama docs(after filter):\", len(docs))\n",
    "if len(docs) < 200:\n",
    "    print(\"⚠️ 드라마 문서 수가 너무 적으면 토픽 품질이 낮아질 수 있어요.\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) GPU 임베딩 모델\n",
    "# -------------------------\n",
    "embedding_model = SentenceTransformer(\n",
    "    \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    device=\"cuda\"  # GPU 사용\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4) 드라마 전용 파라미터(안정 + 토픽 뭉개짐 방지)\n",
    "# -------------------------\n",
    "# ✅ 핵심: 여기서는 '토픽 내부 문서수'까지 고려해서 에러가 절대 안 나게 설정\n",
    "# - min_df=1, max_df=1.0  (토픽이 1개 문서여도 CountVectorizer가 안 터짐)\n",
    "# - 대신 너무 흔한 단어는 stop_words=\"english\" + token_pattern으로 1차 걸러짐\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r\"\\b[a-zA-Z]{3,}\\b\",\n",
    "    min_df=1,\n",
    "    max_df=1.0\n",
    ")\n",
    "\n",
    "# 드라마는 문서 수가 작으니 클러스터를 조금 더 잘게 쪼개도록 완화\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,     # 작을수록 더 로컬(토픽 더 잘게)\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=20,  # 작게: 토픽 수 늘리는 방향\n",
    "    min_samples=5,\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    top_n_words=TOP_N_WORDS,\n",
    "    min_topic_size=20,        # 토픽 최소 문서수\n",
    "    nr_topics=None,\n",
    "    calculate_probabilities=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 5) 학습\n",
    "# -------------------------\n",
    "topics, _ = topic_model.fit_transform(docs)\n",
    "drama[\"plot_topic\"] = topics\n",
    "\n",
    "# -------------------------\n",
    "# 6) 저장(요약표/키워드/데이터)\n",
    "# -------------------------\n",
    "info = topic_model.get_topic_info()\n",
    "info.to_csv(OUT_INFO, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "kw_rows = []\n",
    "for tid, tuples in topic_model.get_topics().items():\n",
    "    words = [w for w, _ in tuples if w]\n",
    "    kw_rows.append({\"plot_topic\": tid, \"keywords\": words})\n",
    "pd.DataFrame(kw_rows).to_parquet(OUT_KW, index=False)\n",
    "\n",
    "drama.to_parquet(OUT_DRAMA, index=False)\n",
    "\n",
    "# -------------------------\n",
    "# 7) 바로 검증(너가 봐야 할 3개만)\n",
    "# -------------------------\n",
    "outlier_ratio = (drama[\"plot_topic\"] == -1).mean()\n",
    "n_topics_incl = drama[\"plot_topic\"].nunique()\n",
    "n_topics_excl = drama.loc[drama[\"plot_topic\"] != -1, \"plot_topic\"].nunique()\n",
    "\n",
    "print(\"\\n================ CHECK ================\")\n",
    "print(\"outlier(-1) ratio:\", float(outlier_ratio))\n",
    "print(\"n_topics (incl -1):\", int(n_topics_incl))\n",
    "print(\"n_topics (excl -1):\", int(n_topics_excl))\n",
    "print(\"top topic sizes:\\n\", pd.Series(topics).value_counts().head(10))\n",
    "print(\"saved:\", OUT_DRAMA, OUT_INFO, OUT_KW)\n",
    "print(\"✅ DONE (DRAMA)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b5973",
   "metadata": {},
   "source": [
    "## step 2 영화 줄거리 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# MOVIE RUN (GPU) - Plot Topic (overview) [분리 임베딩]\n",
    "# - input : 00_movie_main.parquet  (or 네 영화 parquet 경로)\n",
    "# - output:\n",
    "#   1) movie_with_plot_topic.parquet\n",
    "#   2) plot_topic_info_movie.csv\n",
    "#   3) plot_keywords_movie.parquet\n",
    "# =========================================================\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# -------------------------\n",
    "# 0) 경로/설정\n",
    "# -------------------------\n",
    "MOVIE_PATH = \"00_movie_main.parquet\"   \n",
    "OUT_MOVIE  = \"movie_with_plot_topic.parquet\"\n",
    "OUT_INFO   = \"plot_topic_info_movie.csv\"\n",
    "OUT_KW     = \"plot_keywords_movie.parquet\"\n",
    "\n",
    "SEED = 42\n",
    "MIN_OVERVIEW_LEN = 30\n",
    "TOP_N_WORDS = 30\n",
    "\n",
    "# -------------------------\n",
    "# 1) GPU 임베딩 모델\n",
    "# -------------------------\n",
    "embedding_model = SentenceTransformer(\n",
    "    \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 2) 텍스트 클린\n",
    "# -------------------------\n",
    "def clean_text(s: str) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# -------------------------\n",
    "# 3) 로드 + 필터\n",
    "# -------------------------\n",
    "movie = pd.read_parquet(MOVIE_PATH).copy()\n",
    "\n",
    "need_cols = [\"imdb_id\", \"overview\"]\n",
    "missing = [c for c in need_cols if c not in movie.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"movie에 필요한 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "movie[\"overview_clean\"] = movie[\"overview\"].apply(clean_text)\n",
    "movie = movie[movie[\"overview_clean\"].str.len() >= MIN_OVERVIEW_LEN].copy()\n",
    "\n",
    "docs = movie[\"overview_clean\"].tolist()\n",
    "n_docs = len(docs)\n",
    "print(\"movie docs(after filter):\", n_docs)\n",
    "\n",
    "# -------------------------\n",
    "# 4) 영화용 파라미터 (드라마보다 보수적으로)\n",
    "# -------------------------\n",
    "# vectorizer: 문서 수 비례 min_df (너무 빡세면 토픽이 말라죽음)\n",
    "MIN_DF = max(5, int(n_docs * 0.002))   # 0.2% ≈ 27k면 ~54\n",
    "MAX_DF = 0.90\n",
    "\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r\"\\b[a-zA-Z]{3,}\\b\",\n",
    "    min_df=MIN_DF,\n",
    "    max_df=MAX_DF\n",
    ")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=20,\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50,\n",
    "    min_samples=10,\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    top_n_words=TOP_N_WORDS,\n",
    "    min_topic_size=50,\n",
    "    nr_topics=None,\n",
    "    calculate_probabilities=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n[MOVIE] vectorizer params:\", {\"min_df\": MIN_DF, \"max_df\": MAX_DF})\n",
    "print(\"[MOVIE] cluster/topic params:\", {\"min_cluster_size\": 50, \"min_samples\": 10, \"min_topic_size\": 50, \"n_neighbors\": 20, \"n_components\": 5})\n",
    "\n",
    "# -------------------------\n",
    "# 5) 학습\n",
    "# -------------------------\n",
    "topics, _ = topic_model.fit_transform(docs)\n",
    "movie[\"plot_topic\"] = topics\n",
    "\n",
    "# -------------------------\n",
    "# 6) 저장 (분리)\n",
    "# -------------------------\n",
    "movie.to_parquet(OUT_MOVIE, index=False)\n",
    "topic_model.get_topic_info().to_csv(OUT_INFO, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "kw_rows = []\n",
    "for tid, tuples in topic_model.get_topics().items():\n",
    "    if tuples is None:\n",
    "        continue\n",
    "    words = [w for w, _ in tuples if w]\n",
    "    kw_rows.append({\"plot_topic\": tid, \"keywords\": words})\n",
    "pd.DataFrame(kw_rows).to_parquet(OUT_KW, index=False)\n",
    "\n",
    "# -------------------------\n",
    "# 7) 간단 체크\n",
    "# -------------------------\n",
    "outlier_ratio = (movie[\"plot_topic\"] == -1).mean()\n",
    "n_topics_incl = movie[\"plot_topic\"].nunique()\n",
    "n_topics_excl = movie.loc[movie[\"plot_topic\"] != -1, \"plot_topic\"].nunique()\n",
    "\n",
    "print(\"\\n============== CHECK ==============\")\n",
    "print(\"outlier(-1) ratio:\", float(outlier_ratio))\n",
    "print(\"n_topics (incl -1):\", int(n_topics_incl))\n",
    "print(\"n_topics (excl -1):\", int(n_topics_excl))\n",
    "print(\"top topic sizes:\\n\", movie[\"plot_topic\"].value_counts().head(10))\n",
    "print(\"saved:\", OUT_MOVIE, OUT_INFO, OUT_KW)\n",
    "print(\"✅ DONE (MOVIE)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd2383",
   "metadata": {},
   "source": [
    "## step 3 드라마 리뷰 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96dfc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Drama Review BERTopic (50k) + Plot Stopwords 적용\n",
    "# - plot_keywords_drama.parquet 에서 \"모든\" 줄거리 키워드 모아서 stopwords로 사용\n",
    "# - 8GB(4070 laptop) 기준: batch 작게 + 오류시 CPU fallback\n",
    "# - output:\n",
    "#   1) review_topic_info_drama_50k.csv\n",
    "#   2) review_topic_map_drama_50k.parquet\n",
    "# =========================================\n",
    "\n",
    "import os, gc, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# -------------------------\n",
    "# 0) 설정\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "REVIEW_PATH  = \"drama_review_final.parquet\"        # ✅ 드라마 리뷰 파일명\n",
    "PLOT_KW_PATH = \"plot_keywords_drama.parquet\"       # ✅ 드라마 줄거리 키워드 파일\n",
    "TEXT_COL = \"review_text\"\n",
    "IMDB_COL = \"imdb_id\"\n",
    "\n",
    "SAMPLE_N = 50_000\n",
    "LEN_MIN, LEN_MAX = 50, 2000\n",
    "\n",
    "OUT_INFO = \"review_topic_info_drama_50k.csv\"\n",
    "OUT_MAP  = \"review_topic_map_drama_50k.parquet\"\n",
    "\n",
    "# GPU 환경 안정화(가끔 \"CUDA unknown error\" 완화용)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# -------------------------\n",
    "# 1) Plot stopwords 만들기 (핵심!)\n",
    "# -------------------------\n",
    "pk = pd.read_parquet(PLOT_KW_PATH)\n",
    "\n",
    "# keywords 컬럼이 ndarray/list 형태여도 flatten 되게 처리\n",
    "plot_words = set()\n",
    "for kws in pk[\"keywords\"].tolist():\n",
    "    if kws is None:\n",
    "        continue\n",
    "    for w in list(kws):\n",
    "        if not w:\n",
    "            continue\n",
    "        w = str(w).strip().lower()\n",
    "        if w:\n",
    "            plot_words.add(w)\n",
    "\n",
    "# 줄거리에서 너무 흔해서 빼는 게 오히려 더 좋은 \"범용 서사/형식 단어\"는 추가로 제거(선택)\n",
    "# ✅ 드라마는 시즌/에피소드 언급이 많아서 여기 ban_words를 좀 더 강하게 둠\n",
    "ban_words = {\n",
    "    \"film\",\"movie\",\"movies\",\n",
    "    \"story\",\"stories\",\"plot\",\n",
    "    \"series\",\"show\",\"shows\",\n",
    "    \"season\",\"seasons\",\"episode\",\"episodes\",\n",
    "    \"character\",\"characters\",\n",
    "    # 드라마 리뷰에서 특히 흔한 단어들(필요 없으면 빼도 됨)\n",
    "    \"drama\",\"dramas\",\"tv\",\"television\",\n",
    "}\n",
    "plot_words = plot_words - ban_words\n",
    "\n",
    "base_sw = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# 리뷰에서만 제거하고 싶은 \"평가/관용어\"\n",
    "review_generic_sw = {\n",
    "    \"like\",\"just\",\"good\",\"really\",\"time\",\"way\",\"watch\",\"watched\",\"watching\",\"people\",\n",
    "    \"dont\",\"didnt\",\"doesnt\",\"isnt\",\"wasnt\",\"werent\",\"cant\",\"couldnt\",\"wouldnt\",\n",
    "    \"im\",\"ive\",\"youre\",\"theyre\",\"thats\",\"theres\",\"hes\",\"shes\",\"weve\",\"id\",\n",
    "    # 형식/대상 단어(드라마 쪽 강화)\n",
    "    \"film\",\"films\",\"movie\",\"movies\",\"show\",\"shows\",\"series\",\"season\",\"seasons\",\"episode\",\"episodes\",\n",
    "    \"drama\",\"dramas\",\"tv\",\"television\",\n",
    "    \"story\",\"plot\",\"character\",\"characters\",\n",
    "}\n",
    "\n",
    "combined_sw = sorted(list(base_sw | plot_words | review_generic_sw))\n",
    "\n",
    "print(f\"[Stopwords] ENGLISH={len(base_sw)}, plot_words={len(plot_words)}, total={len(combined_sw)}\")\n",
    "print(\"plot_words sample:\", list(sorted(list(plot_words)))[:30])\n",
    "\n",
    "# -------------------------\n",
    "# 2) 리뷰 로드 + 필터 + 샘플\n",
    "# -------------------------\n",
    "df = pd.read_parquet(REVIEW_PATH)\n",
    "\n",
    "# 텍스트 유효\n",
    "df = df[df[TEXT_COL].notna()].copy()\n",
    "df[TEXT_COL] = df[TEXT_COL].astype(str)\n",
    "\n",
    "# 길이 컷\n",
    "df[\"__len\"] = df[TEXT_COL].str.len()\n",
    "df = df[(df[\"__len\"] >= LEN_MIN) & (df[\"__len\"] <= LEN_MAX)].copy()\n",
    "\n",
    "print(\"after filter:\", len(df))\n",
    "\n",
    "# 샘플\n",
    "df_s = df.sample(n=min(SAMPLE_N, len(df)), random_state=SEED).copy()\n",
    "docs = df_s[TEXT_COL].tolist()\n",
    "\n",
    "print(\"sample docs:\", len(docs))\n",
    "\n",
    "# 메모리 정리\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "# -------------------------\n",
    "# 3) BERTopic 구성\n",
    "# -------------------------\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=combined_sw,\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r\"\\b[a-zA-Z]{3,}\\b\",\n",
    "    min_df=20,       # 50k면 10~30 사이 추천\n",
    "    max_df=0.95,\n",
    ")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,     # 안정\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50,\n",
    "    min_samples=10,\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4) 임베딩 + fit_transform (GPU 우선, 실패하면 CPU fallback)\n",
    "# -------------------------\n",
    "def run_bertopic(docs, device=\"cuda\", enc_batch=16):\n",
    "    # ✅ Qwen 임베딩 모델\n",
    "    embedder = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\", device=device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        embeddings = embedder.encode(\n",
    "            docs,\n",
    "            batch_size=enc_batch,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=None,      # embeddings를 직접 넣을 거라 None\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        calculate_probabilities=False,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "    return topic_model, topics\n",
    "\n",
    "# 1차: GPU 시도\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "    model, topics = run_bertopic(docs, device=\"cuda\", enc_batch=16)\n",
    "except Exception as e:\n",
    "    print(\"\\n⚠️ GPU 실패 -> CPU로 fallback\")\n",
    "    print(\"GPU error:\", repr(e))\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model, topics = run_bertopic(docs, device=\"cpu\", enc_batch=32)\n",
    "\n",
    "# -------------------------\n",
    "# 5) 결과 저장\n",
    "# -------------------------\n",
    "info = model.get_topic_info()\n",
    "info.to_csv(OUT_INFO, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "df_s[\"review_topic\"] = topics\n",
    "df_s.drop(columns=[\"__len\"], inplace=True, errors=\"ignore\")\n",
    "df_s.to_parquet(OUT_MAP, index=False)\n",
    "\n",
    "outlier_ratio = (df_s[\"review_topic\"] == -1).mean()\n",
    "print(\"\\n===== DONE =====\")\n",
    "print(\"outlier(-1) ratio:\", float(outlier_ratio))\n",
    "print(\"n_topics(excl -1):\", int((info[\"Topic\"] != -1).sum()))\n",
    "print(\"saved:\", OUT_INFO, OUT_MAP)\n",
    "\n",
    "# -------------------------\n",
    "# 6) 간단 검증(토픽 분포 Top15)\n",
    "# -------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tmp = df_s[df_s[\"review_topic\"] != -1][\"review_topic\"].value_counts().head(15).sort_index()\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(tmp.index.astype(str), tmp.values)\n",
    "plt.title(\"Drama Review Topic Distribution (Top 15, excl -1)\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Review Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0e35c",
   "metadata": {},
   "source": [
    "## step 4 영화 리뷰 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b063c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Movie Review BERTopic (50k) + Plot Stopwords 적용\n",
    "# - plot_keywords_movie.parquet 에서 \"모든\" 줄거리 키워드 모아서 stopwords로 사용\n",
    "# - 8GB(4070 laptop) 기준: batch 작게 + fp16 + 오류시 CPU fallback\n",
    "# - output:\n",
    "#   1) review_topic_info_movie_50k.csv\n",
    "#   2) review_topic_map_movie_50k.parquet\n",
    "# =========================================\n",
    "\n",
    "import os, gc, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# -------------------------\n",
    "# 0) 설정\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "REVIEW_PATH = \"movie_review_final.parquet\"        # ✅ 네 영화 리뷰 파일명에 맞게 수정\n",
    "PLOT_KW_PATH = \"plot_keywords_movie.parquet\"  # ✅ 이미 만든 줄거리 키워드 파일\n",
    "TEXT_COL = \"review_text\"\n",
    "IMDB_COL = \"imdb_id\"\n",
    "\n",
    "SAMPLE_N = 50_000\n",
    "LEN_MIN, LEN_MAX = 50, 2000\n",
    "\n",
    "OUT_INFO = \"review_topic_info_movie_50k.csv\"\n",
    "OUT_MAP  = \"review_topic_map_movie_50k.parquet\"\n",
    "\n",
    "# GPU 환경 안정화(가끔 \"CUDA unknown error\" 완화용)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# -------------------------\n",
    "# 1) Plot stopwords 만들기 (핵심!)\n",
    "# -------------------------\n",
    "pk = pd.read_parquet(PLOT_KW_PATH)\n",
    "\n",
    "# keywords 컬럼이 ndarray/list 형태여도 flatten 되게 처리\n",
    "plot_words = set()\n",
    "for kws in pk[\"keywords\"].tolist():\n",
    "    if kws is None:\n",
    "        continue\n",
    "    for w in list(kws):\n",
    "        if not w:\n",
    "            continue\n",
    "        w = str(w).strip().lower()\n",
    "        if w:\n",
    "            plot_words.add(w)\n",
    "\n",
    "# 줄거리에서 너무 흔해서 빼는 게 오히려 더 좋은 \"범용 서사/형식 단어\"는 추가로 제거(선택)\n",
    "ban_words = {\n",
    "    \"film\",\"movie\",\"movies\",\"story\",\"stories\",\"series\",\"season\",\"seasons\",\"episode\",\"episodes\",\n",
    "    \"character\",\"characters\",\"plot\",\"show\",\"shows\"\n",
    "}\n",
    "plot_words = plot_words - ban_words\n",
    "\n",
    "base_sw = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# 리뷰에서만 제거하고 싶은 \"평가/관용어\" (너가 쓰던 것 있으면 여기 더 붙여도 됨)\n",
    "review_generic_sw = {\n",
    "    \"like\",\"just\",\"good\",\"really\",\"time\",\"way\",\"watch\",\"watched\",\"watching\",\"people\",\n",
    "    \"dont\",\"didnt\",\"doesnt\",\"isnt\",\"wasnt\",\"werent\",\"cant\",\"couldnt\",\"wouldnt\",\n",
    "    \"im\",\"ive\",\"youre\",\"theyre\",\"thats\",\"theres\",\"hes\",\"shes\",\"weve\",\"id\",\n",
    "    \"film\",\"films\",\"movie\",\"movies\",\"show\",\"shows\",\"series\",\"season\",\"seasons\",\"episode\",\"episodes\",\n",
    "    \"story\",\"plot\",\"character\",\"characters\",\n",
    "}\n",
    "\n",
    "combined_sw = sorted(list(base_sw | plot_words | review_generic_sw))\n",
    "\n",
    "print(f\"[Stopwords] ENGLISH={len(base_sw)}, plot_words={len(plot_words)}, total={len(combined_sw)}\")\n",
    "print(\"plot_words sample:\", list(sorted(list(plot_words)))[:30])\n",
    "\n",
    "# -------------------------\n",
    "# 2) 리뷰 로드 + 필터 + 샘플\n",
    "# -------------------------\n",
    "df = pd.read_parquet(REVIEW_PATH)\n",
    "\n",
    "# 텍스트 유효\n",
    "df = df[df[TEXT_COL].notna()].copy()\n",
    "df[TEXT_COL] = df[TEXT_COL].astype(str)\n",
    "\n",
    "# 길이 컷\n",
    "df[\"__len\"] = df[TEXT_COL].str.len()\n",
    "df = df[(df[\"__len\"] >= LEN_MIN) & (df[\"__len\"] <= LEN_MAX)].copy()\n",
    "\n",
    "print(\"after filter:\", len(df))\n",
    "\n",
    "# 샘플\n",
    "df_s = df.sample(n=min(SAMPLE_N, len(df)), random_state=SEED).copy()\n",
    "docs = df_s[TEXT_COL].tolist()\n",
    "\n",
    "print(\"sample docs:\", len(docs))\n",
    "\n",
    "# 메모리 정리\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "# -------------------------\n",
    "# 3) BERTopic 구성\n",
    "# -------------------------\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=combined_sw,\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r\"\\b[a-zA-Z]{3,}\\b\",\n",
    "    min_df=20,       # 50k면 10~30 사이 추천\n",
    "    max_df=0.95,\n",
    ")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,     # 안정\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50,\n",
    "    min_samples=10,\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4) 임베딩 + fit_transform (GPU 우선, 실패하면 CPU fallback)\n",
    "# -------------------------\n",
    "def run_bertopic(docs, device=\"cuda\", enc_batch=16):\n",
    "    # ✅ Qwen 임베딩 모델 (너가 쓰던 그대로)\n",
    "    embedder = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\", device=device)\n",
    "\n",
    "    # 안정: fp16 + 작은 배치\n",
    "    with torch.inference_mode():\n",
    "        embeddings = embedder.encode(\n",
    "            docs,\n",
    "            batch_size=enc_batch,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=None,      # embeddings를 직접 넣을 거라 None\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        calculate_probabilities=False,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "    return topic_model, topics\n",
    "\n",
    "# 1차: GPU 시도\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "    model, topics = run_bertopic(docs, device=\"cuda\", enc_batch=16)\n",
    "except Exception as e:\n",
    "    print(\"\\n⚠️ GPU 실패 -> CPU로 fallback\")\n",
    "    print(\"GPU error:\", repr(e))\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model, topics = run_bertopic(docs, device=\"cpu\", enc_batch=32)\n",
    "\n",
    "# -------------------------\n",
    "# 5) 결과 저장\n",
    "# -------------------------\n",
    "info = model.get_topic_info()\n",
    "info.to_csv(OUT_INFO, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "df_s[\"review_topic\"] = topics\n",
    "df_s.drop(columns=[\"__len\"], inplace=True, errors=\"ignore\")\n",
    "df_s.to_parquet(OUT_MAP, index=False)\n",
    "\n",
    "outlier_ratio = (df_s[\"review_topic\"] == -1).mean()\n",
    "print(\"\\n===== DONE =====\")\n",
    "print(\"outlier(-1) ratio:\", float(outlier_ratio))\n",
    "print(\"n_topics(excl -1):\", int((info[\"Topic\"] != -1).sum()))\n",
    "print(\"saved:\", OUT_INFO, OUT_MAP)\n",
    "\n",
    "# -------------------------\n",
    "# 6) 간단 검증(토픽 분포 Top15)\n",
    "# -------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tmp = df_s[df_s[\"review_topic\"] != -1][\"review_topic\"].value_counts().head(15).sort_index()\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(tmp.index.astype(str), tmp.values)\n",
    "plt.title(\"Review Topic Distribution (Top 15, excl -1)\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Review Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a3c5ff",
   "metadata": {},
   "source": [
    "## setp5 영화+드라마 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90688ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Topic Clustering (INFO only)\n",
    "# - movie_info / drama_info 각각 따로 클러스터링\n",
    "# - map 데이터는 건드리지 않음\n",
    "# - output: *_info_clustered.csv + *_cluster_summary.csv\n",
    "# =========================================\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import Counter\n",
    "\n",
    "# ---- 1) 임베딩 모델 (가벼움 + 성능 좋음) ----\n",
    "# pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0) 공통 유틸\n",
    "# =========================\n",
    "def parse_representation(x):\n",
    "    \"\"\"\n",
    "    Representation 컬럼이\n",
    "    - 이미 list 이거나\n",
    "    - 문자열로 저장된 list (\"['a','b',...]\") 인 경우\n",
    "    둘 다 안전하게 list[str]로 변환\n",
    "    \"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return [str(w) for w in x]\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    # CSV에서 list가 문자열로 저장된 경우가 많음\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return [str(w) for w in v]\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 그냥 문자열이면 공백 split\n",
    "    return [w for w in s.replace(\",\", \" \").split() if w]\n",
    "\n",
    "\n",
    "def make_topic_text(row, topk=20):\n",
    "    \"\"\"\n",
    "    토픽을 대표하는 텍스트 만들기:\n",
    "    Representation(키워드) 중심 + Name 보조\n",
    "    \"\"\"\n",
    "    reps = parse_representation(row.get(\"Representation\", []))\n",
    "    name = str(row.get(\"Name\", \"\")).replace(\"_\", \" \")\n",
    "    reps = reps[:topk]\n",
    "    # reps가 비면 name이라도 사용\n",
    "    if reps:\n",
    "        return \" \".join(reps) + \" || \" + name\n",
    "    return name\n",
    "\n",
    "\n",
    "def cluster_keywords(rep_lists, topn=12):\n",
    "    \"\"\"\n",
    "    클러스터 안 토픽들의 Representation 키워드를 모아서\n",
    "    가장 자주 등장하는 단어 TopN을 클러스터 대표 키워드로 생성\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    for reps in rep_lists:\n",
    "        for w in reps:\n",
    "            w = str(w).strip().lower()\n",
    "            if w:\n",
    "                c[w] += 1\n",
    "    return [w for w, _ in c.most_common(topn)]\n",
    "\n",
    "\n",
    "def run_topic_clustering(info_df, k=10, seed=42, text_topk=20, kw_topn=12):\n",
    "    \"\"\"\n",
    "    info_df: topic info 데이터프레임 (movie/drama 각각)\n",
    "    k: 원하는 클러스터 수(8~12 추천)\n",
    "    반환:\n",
    "      clustered_df: topic별 cluster_id 포함\n",
    "      summary_df: cluster별 요약(크기/대표키워드/예시 토픽)\n",
    "    \"\"\"\n",
    "    df = info_df.copy()\n",
    "\n",
    "    # (1) 표준화: topic 컬럼명 통일\n",
    "    if \"review_topic\" not in df.columns and \"Topic\" in df.columns:\n",
    "        df = df.rename(columns={\"Topic\": \"review_topic\"})\n",
    "\n",
    "    # (2) 토픽 텍스트 생성\n",
    "    df[\"__topic_text\"] = df.apply(lambda r: make_topic_text(r, topk=text_topk), axis=1)\n",
    "\n",
    "    # (3) 임베딩\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    emb = model.encode(df[\"__topic_text\"].tolist(), show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "    # (4) 계층 클러스터링\n",
    "    # cosine 거리 기반(정규화했으니 euclidean로도 잘 맞음)\n",
    "    clt = AgglomerativeClustering(n_clusters=k, linkage=\"average\")\n",
    "    labels = clt.fit_predict(emb)\n",
    "\n",
    "    df[\"cluster_id\"] = labels\n",
    "\n",
    "    # (5) 클러스터 요약 생성\n",
    "    # Representation 파싱\n",
    "    df[\"__rep_list\"] = df[\"Representation\"].apply(parse_representation) if \"Representation\" in df.columns else [[]]*len(df)\n",
    "\n",
    "    rows = []\n",
    "    for cid, g in df.groupby(\"cluster_id\"):\n",
    "        rep_kw = cluster_keywords(g[\"__rep_list\"].tolist(), topn=kw_topn)\n",
    "        # 예시 토픽(가장 count 큰 것 우선)\n",
    "        count_col = \"Count\" if \"Count\" in g.columns else None\n",
    "        g2 = g.sort_values(count_col, ascending=False) if count_col else g\n",
    "        example = g2.head(3)[[\"review_topic\", \"Name\"]].to_dict(\"records\") if \"Name\" in g2.columns else g2.head(3)[[\"review_topic\"]].to_dict(\"records\")\n",
    "\n",
    "        rows.append({\n",
    "            \"cluster_id\": int(cid),\n",
    "            \"n_topics\": int(len(g)),\n",
    "            \"cluster_keywords\": \", \".join(rep_kw),\n",
    "            \"example_topics\": example\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(rows).sort_values(\"n_topics\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # (6) 정리: 임시 컬럼 제거\n",
    "    clustered_df = df.drop(columns=[c for c in [\"__topic_text\", \"__rep_list\"] if c in df.columns])\n",
    "\n",
    "    return clustered_df, summary_df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) 실행: 영화/드라마 각각\n",
    "# =========================\n",
    "movie_info = pd.read_csv(\"review_topic_info_movie_50k.csv\")\n",
    "drama_info = pd.read_csv(\"review_topic_info_drama_50k.csv\")\n",
    "\n",
    "# 추천: 8~12 사이로 시작\n",
    "movie_clustered, movie_summary = run_topic_clustering(movie_info, k=10)\n",
    "drama_clustered, drama_summary = run_topic_clustering(drama_info, k=10)\n",
    "\n",
    "# =========================\n",
    "# 2) 저장\n",
    "# =========================\n",
    "movie_clustered.to_csv(\"review_topic_info_movie_50k_clustered.csv\", index=False)\n",
    "movie_summary.to_csv(\"review_topic_cluster_summary_movie.csv\", index=False)\n",
    "\n",
    "drama_clustered.to_csv(\"review_topic_info_drama_50k_clustered.csv\", index=False)\n",
    "drama_summary.to_csv(\"review_topic_cluster_summary_drama.csv\", index=False)\n",
    "\n",
    "print(\"✅ done\")\n",
    "print(\"movie clusters:\", movie_summary.shape, \"drama clusters:\", drama_summary.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c12b417",
   "metadata": {},
   "source": [
    "## Step 6 한글 클러스터링 네이밍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf047554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# 1) 한글 클러스터명 매핑 (최종)\n",
    "# =========================\n",
    "movie_cluster_ko = {\n",
    "    1: \"전반적 완성도·호불호 평가\",\n",
    "    8: \"디즈니·애니메이션 감정 반응\",\n",
    "    0: \"전쟁·역사 사실성 평가\",\n",
    "    3: \"프랜차이즈·액션 콘텐츠 평가\",\n",
    "    7: \"음악·퍼포먼스 완성도 평가\",\n",
    "    2: \"시즌·홀리데이 감성 반응\",\n",
    "    6: \"원작·각색 비교 평가\",\n",
    "    4: \"가족·인물 감정 반응\",\n",
    "    5: \"사회·인종 이슈 인식 반응\",\n",
    "    9: \"종교·신앙 메시지 해석\",\n",
    "}\n",
    "\n",
    "\n",
    "drama_cluster_ko = {\n",
    "    0: \"전반적 감상·전개 평가\",\n",
    "    2: \"세계관·원작 충실도 평가\",\n",
    "    3: \"사실성·논쟁 이슈 반응\",\n",
    "    1: \"젠더·정체성 표현 반응\",\n",
    "    5: \"청소년·교육적 메시지 인식\",\n",
    "    4: \"종교·신앙 메시지 해석\",\n",
    "    6: \"레이싱·모터스포츠 시청 몰입 반응\",\n",
    "    7: \"실험적 연출·예술성 평가\",\n",
    "    8: \"요리·경연 리얼리티 반응\",\n",
    "    9: \"가족·코미디 정서 반응\",\n",
    "}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) 파일 로드\n",
    "# =========================\n",
    "movie_info = pd.read_csv(\"review_topic_info_movie_50k_clustered.csv\")\n",
    "drama_info = pd.read_csv(\"review_topic_info_drama_50k_clustered.csv\")\n",
    "\n",
    "movie_summary = pd.read_csv(\"review_topic_cluster_summary_movie.csv\")\n",
    "drama_summary = pd.read_csv(\"review_topic_cluster_summary_drama.csv\")\n",
    "\n",
    "# =========================\n",
    "# 3) 한글 컬럼 추가\n",
    "# =========================\n",
    "movie_info[\"cluster_name_ko\"] = movie_info[\"cluster_id\"].map(movie_cluster_ko)\n",
    "drama_info[\"cluster_name_ko\"] = drama_info[\"cluster_id\"].map(drama_cluster_ko)\n",
    "\n",
    "movie_summary[\"cluster_name_ko\"] = movie_summary[\"cluster_id\"].map(movie_cluster_ko)\n",
    "drama_summary[\"cluster_name_ko\"] = drama_summary[\"cluster_id\"].map(drama_cluster_ko)\n",
    "\n",
    "# =========================\n",
    "# 4) 누락 체크(필수)\n",
    "# =========================\n",
    "def _check_missing(df, label):\n",
    "    miss = df[df[\"cluster_name_ko\"].isna()][\"cluster_id\"].unique().tolist()\n",
    "    if miss:\n",
    "        print(f\"⚠️ {label} 매핑 누락 cluster_id:\", miss)\n",
    "    else:\n",
    "        print(f\"✅ {label} 매핑 OK\")\n",
    "\n",
    "_check_missing(movie_info, \"movie_info\")\n",
    "_check_missing(drama_info, \"drama_info\")\n",
    "_check_missing(movie_summary, \"movie_summary\")\n",
    "_check_missing(drama_summary, \"drama_summary\")\n",
    "\n",
    "# =========================\n",
    "# 5) 저장 (한글 반영본)\n",
    "# =========================\n",
    "movie_info.to_csv(\"review_topic_info_movie_50k_clustered_ko.csv\", index=False)\n",
    "drama_info.to_csv(\"review_topic_info_drama_50k_clustered_ko.csv\", index=False)\n",
    "\n",
    "movie_summary.to_csv(\"review_topic_cluster_summary_movie_ko.csv\", index=False)\n",
    "drama_summary.to_csv(\"review_topic_cluster_summary_drama_ko.csv\", index=False)\n",
    "\n",
    "print(\"✅ 저장 완료: *_clustered_ko.csv, *_summary_ko.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
