{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 리뷰 임베딩\n",
    "\n",
    "줄거리 임베딩을 제거하고 추출하여 사용자 반응만을 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설정 및 공통 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, gc, random, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GPU 환경 안정화\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# ==================== 전역 설정 ====================\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# 파일 경로\n",
    "PATHS = {\n",
    "    'drama_main': \"files/drama/00_drama_main.parquet\",\n",
    "    'movie_main': \"files/movie/00_movie_main.parquet\",\n",
    "    'drama_review': \"drama_review_final.parquet\",\n",
    "    'movie_review': \"movie_review_final.parquet\",\n",
    "}\n",
    "\n",
    "# 공통 파라미터\n",
    "CONFIG = {\n",
    "    'embedding_model': \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    'min_overview_len': 30,\n",
    "    'top_n_words': 30,\n",
    "    'review_sample_n': 50_000,\n",
    "    'review_len_min': 50,\n",
    "    'review_len_max': 2000,\n",
    "}\n",
    "\n",
    "# ==================== 공통 함수 ====================\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"텍스트 클리닝\"\"\"\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def create_embedding_model(device=\"cuda\"):\n",
    "    \"\"\"임베딩 모델 생성\"\"\"\n",
    "    return SentenceTransformer(CONFIG['embedding_model'], device=device)\n",
    "\n",
    "\n",
    "def create_bertopic_components(vectorizer_params, umap_params, hdbscan_params, embedding_model=None):\n",
    "    \"\"\"BERTopic 컴포넌트 생성\"\"\"\n",
    "    vectorizer = CountVectorizer(**vectorizer_params)\n",
    "    umap_model = UMAP(**umap_params, random_state=SEED)\n",
    "    hdbscan_model = HDBSCAN(**hdbscan_params)\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer,\n",
    "        top_n_words=CONFIG['top_n_words'],\n",
    "        calculate_probabilities=False,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return topic_model\n",
    "\n",
    "\n",
    "def extract_keywords_from_topics(topic_model):\n",
    "    \"\"\"토픽에서 키워드 추출\"\"\"\n",
    "    keywords_dict = {}\n",
    "    for tid, tuples in topic_model.get_topics().items():\n",
    "        if tuples is None:\n",
    "            continue\n",
    "        keywords_dict[tid] = [w for w, _ in tuples if w]\n",
    "    return keywords_dict\n",
    "\n",
    "\n",
    "def print_topic_stats(df, topic_col, label):\n",
    "    \"\"\"토픽 통계 출력\"\"\"\n",
    "    outlier_ratio = (df[topic_col] == -1).mean()\n",
    "    n_topics_incl = df[topic_col].nunique()\n",
    "    n_topics_excl = df.loc[df[topic_col] != -1, topic_col].nunique()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{label} - Topic Statistics\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Outlier(-1) ratio: {outlier_ratio:.4f}\")\n",
    "    print(f\"N topics (incl -1): {n_topics_incl}\")\n",
    "    print(f\"N topics (excl -1): {n_topics_excl}\")\n",
    "    print(f\"\\nTop 10 topic sizes:\")\n",
    "    print(df[topic_col].value_counts().head(10))\n",
    "\n",
    "\n",
    "print(\"설정 및 공통 함수 로드 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: 줄거리 토픽 모델링 (드라마 + 영화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_plot_topics(data_path, is_drama=True):\n",
    "    \"\"\"\n",
    "    줄거리(overview) 기반 토픽 모델링\n",
    "    \n",
    "    Returns:\n",
    "        df: plot_topic 컬럼이 추가된 데이터프레임\n",
    "        keywords_dict: {topic_id: [keywords]} 딕셔너리\n",
    "        topic_info: 토픽 정보 데이터프레임\n",
    "    \"\"\"\n",
    "    # 데이터 로드\n",
    "    df = pd.read_parquet(data_path).copy()\n",
    "    \n",
    "    # 전처리\n",
    "    df[\"overview_clean\"] = df[\"overview\"].apply(clean_text)\n",
    "    df = df[df[\"overview_clean\"].str.len() >= CONFIG['min_overview_len']].copy()\n",
    "    docs = df[\"overview_clean\"].tolist()\n",
    "    \n",
    "    n_docs = len(docs)\n",
    "    label = \"DRAMA\" if is_drama else \"MOVIE\"\n",
    "    print(f\"\\n{label} - Filtered docs: {n_docs}\")\n",
    "    \n",
    "    # 임베딩 모델\n",
    "    embedding_model = create_embedding_model()\n",
    "    \n",
    "    # 파라미터 설정 (드라마/영화 구분)\n",
    "    if is_drama:\n",
    "        # 드라마: 문서 수가 적으므로 느슨한 설정\n",
    "        vectorizer_params = {\n",
    "            'stop_words': 'english',\n",
    "            'ngram_range': (1, 2),\n",
    "            'token_pattern': r'\\b[a-zA-Z]{3,}\\b',\n",
    "            'min_df': 1,\n",
    "            'max_df': 1.0\n",
    "        }\n",
    "        umap_params = {\n",
    "            'n_neighbors': 15,\n",
    "            'n_components': 5,\n",
    "            'min_dist': 0.0,\n",
    "            'metric': 'cosine'\n",
    "        }\n",
    "        hdbscan_params = {\n",
    "            'min_cluster_size': 20,\n",
    "            'min_samples': 5,\n",
    "            'prediction_data': True\n",
    "        }\n",
    "        min_topic_size = 20\n",
    "    else:\n",
    "        # 영화: 문서 수가 많으므로 엄격한 설정\n",
    "        min_df = max(5, int(n_docs * 0.002))\n",
    "        vectorizer_params = {\n",
    "            'stop_words': 'english',\n",
    "            'ngram_range': (1, 2),\n",
    "            'token_pattern': r'\\b[a-zA-Z]{3,}\\b',\n",
    "            'min_df': min_df,\n",
    "            'max_df': 0.90\n",
    "        }\n",
    "        umap_params = {\n",
    "            'n_neighbors': 20,\n",
    "            'n_components': 5,\n",
    "            'min_dist': 0.0,\n",
    "            'metric': 'cosine'\n",
    "        }\n",
    "        hdbscan_params = {\n",
    "            'min_cluster_size': 50,\n",
    "            'min_samples': 10,\n",
    "            'prediction_data': True\n",
    "        }\n",
    "        min_topic_size = 50\n",
    "    \n",
    "    # BERTopic 생성\n",
    "    topic_model = create_bertopic_components(\n",
    "        vectorizer_params, umap_params, hdbscan_params, embedding_model\n",
    "    )\n",
    "    topic_model.min_topic_size = min_topic_size\n",
    "    \n",
    "    # 학습\n",
    "    topics, _ = topic_model.fit_transform(docs)\n",
    "    df[\"plot_topic\"] = topics\n",
    "    \n",
    "    # 결과 추출\n",
    "    keywords_dict = extract_keywords_from_topics(topic_model)\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    \n",
    "    # 통계 출력\n",
    "    print_topic_stats(df, \"plot_topic\", label)\n",
    "    \n",
    "    return df, keywords_dict, topic_info\n",
    "\n",
    "\n",
    "# 실행\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"줄거리 토픽 모델링 시작\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "drama_plot_df, drama_plot_keywords, drama_plot_info = build_plot_topics(\n",
    "    PATHS['drama_main'], is_drama=True\n",
    ")\n",
    "\n",
    "movie_plot_df, movie_plot_keywords, movie_plot_info = build_plot_topics(\n",
    "    PATHS['movie_main'], is_drama=False\n",
    ")\n",
    "\n",
    "print(\"\\n줄거리 토픽 모델링 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: 리뷰 토픽 모델링 (드라마 + 영화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stopwords_from_plot(plot_keywords_dict, is_drama=True):\n",
    "    \"\"\"\n",
    "    줄거리 키워드를 stopwords로 변환\n",
    "    \n",
    "    Args:\n",
    "        plot_keywords_dict: {topic_id: [keywords]} 딕셔너리\n",
    "        is_drama: 드라마 여부\n",
    "    \n",
    "    Returns:\n",
    "        combined_stopwords: 통합 stopwords 리스트\n",
    "    \"\"\"\n",
    "    # 줄거리 키워드 수집\n",
    "    plot_words = set()\n",
    "    for keywords in plot_keywords_dict.values():\n",
    "        for w in keywords:\n",
    "            w = str(w).strip().lower()\n",
    "            if w:\n",
    "                plot_words.add(w)\n",
    "    \n",
    "    # 범용 서사 단어 제거\n",
    "    ban_words = {\n",
    "        \"film\", \"movie\", \"movies\", \"story\", \"stories\", \"plot\",\n",
    "        \"series\", \"show\", \"shows\", \"season\", \"seasons\",\n",
    "        \"episode\", \"episodes\", \"character\", \"characters\"\n",
    "    }\n",
    "    \n",
    "    if is_drama:\n",
    "        ban_words.update({\"drama\", \"dramas\", \"tv\", \"television\"})\n",
    "    \n",
    "    plot_words = plot_words - ban_words\n",
    "    \n",
    "    # 리뷰 전용 stopwords\n",
    "    review_generic_sw = {\n",
    "        \"like\", \"just\", \"good\", \"really\", \"time\", \"way\",\n",
    "        \"watch\", \"watched\", \"watching\", \"people\",\n",
    "        \"dont\", \"didnt\", \"doesnt\", \"isnt\", \"wasnt\", \"werent\",\n",
    "        \"cant\", \"couldnt\", \"wouldnt\",\n",
    "        \"im\", \"ive\", \"youre\", \"theyre\", \"thats\", \"theres\",\n",
    "        \"hes\", \"shes\", \"weve\", \"id\",\n",
    "        \"film\", \"films\", \"movie\", \"movies\", \"show\", \"shows\",\n",
    "        \"series\", \"season\", \"seasons\", \"episode\", \"episodes\",\n",
    "        \"story\", \"plot\", \"character\", \"characters\"\n",
    "    }\n",
    "    \n",
    "    if is_drama:\n",
    "        review_generic_sw.update({\"drama\", \"dramas\", \"tv\", \"television\"})\n",
    "    \n",
    "    # 통합\n",
    "    base_sw = set(ENGLISH_STOP_WORDS)\n",
    "    combined = sorted(list(base_sw | plot_words | review_generic_sw))\n",
    "    \n",
    "    print(f\"Stopwords - ENGLISH: {len(base_sw)}, Plot: {len(plot_words)}, Total: {len(combined)}\")\n",
    "    \n",
    "    return combined\n",
    "\n",
    "\n",
    "def build_review_topics(review_path, stopwords, is_drama=True):\n",
    "    \"\"\"\n",
    "    리뷰 기반 토픽 모델링\n",
    "    \n",
    "    Args:\n",
    "        review_path: 리뷰 데이터 경로\n",
    "        stopwords: stopwords 리스트\n",
    "        is_drama: 드라마 여부\n",
    "    \n",
    "    Returns:\n",
    "        df_sample: review_topic 컬럼이 추가된 샘플 데이터\n",
    "        topic_info: 토픽 정보 데이터프레임\n",
    "    \"\"\"\n",
    "    # 데이터 로드\n",
    "    df = pd.read_parquet(review_path)\n",
    "    df = df[df['review_text'].notna()].copy()\n",
    "    df['review_text'] = df['review_text'].astype(str)\n",
    "    \n",
    "    # 길이 필터링\n",
    "    df['__len'] = df['review_text'].str.len()\n",
    "    df = df[\n",
    "        (df['__len'] >= CONFIG['review_len_min']) & \n",
    "        (df['__len'] <= CONFIG['review_len_max'])\n",
    "    ].copy()\n",
    "    \n",
    "    label = \"DRAMA\" if is_drama else \"MOVIE\"\n",
    "    print(f\"\\n{label} Review - After filter: {len(df)}\")\n",
    "    \n",
    "    # 샘플링\n",
    "    df_sample = df.sample(\n",
    "        n=min(CONFIG['review_sample_n'], len(df)), \n",
    "        random_state=SEED\n",
    "    ).copy()\n",
    "    docs = df_sample['review_text'].tolist()\n",
    "    print(f\"{label} Review - Sample docs: {len(docs)}\")\n",
    "    \n",
    "    # 메모리 정리\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    # BERTopic 파라미터\n",
    "    vectorizer_params = {\n",
    "        'stop_words': stopwords,\n",
    "        'ngram_range': (1, 2),\n",
    "        'token_pattern': r'\\b[a-zA-Z]{3,}\\b',\n",
    "        'min_df': 20,\n",
    "        'max_df': 0.95\n",
    "    }\n",
    "    \n",
    "    umap_params = {\n",
    "        'n_neighbors': 15,\n",
    "        'n_components': 5,\n",
    "        'min_dist': 0.0,\n",
    "        'metric': 'cosine'\n",
    "    }\n",
    "    \n",
    "    hdbscan_params = {\n",
    "        'min_cluster_size': 50,\n",
    "        'min_samples': 10,\n",
    "        'prediction_data': True\n",
    "    }\n",
    "    \n",
    "    # 임베딩 및 학습 (GPU 시도 -> CPU fallback)\n",
    "    def run_bertopic(device=\"cuda\", batch_size=16):\n",
    "        embedder = create_embedding_model(device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            embeddings = embedder.encode(\n",
    "                docs,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_numpy=True,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "        \n",
    "        topic_model = create_bertopic_components(\n",
    "            vectorizer_params, umap_params, hdbscan_params, embedding_model=None\n",
    "        )\n",
    "        \n",
    "        topics, _ = topic_model.fit_transform(docs, embeddings)\n",
    "        return topic_model, topics\n",
    "    \n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "        topic_model, topics = run_bertopic(device=\"cuda\", batch_size=16)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nGPU 실패 -> CPU fallback: {repr(e)}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        topic_model, topics = run_bertopic(device=\"cpu\", batch_size=32)\n",
    "    \n",
    "    # 결과 저장\n",
    "    df_sample['review_topic'] = topics\n",
    "    df_sample.drop(columns=['__len'], inplace=True, errors='ignore')\n",
    "    \n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    \n",
    "    # 통계 출력\n",
    "    print_topic_stats(df_sample, 'review_topic', label + ' Review')\n",
    "    \n",
    "    return df_sample, topic_info\n",
    "\n",
    "\n",
    "# 실행\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"리뷰 토픽 모델링 시작\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Stopwords 생성\n",
    "drama_stopwords = create_stopwords_from_plot(drama_plot_keywords, is_drama=True)\n",
    "movie_stopwords = create_stopwords_from_plot(movie_plot_keywords, is_drama=False)\n",
    "\n",
    "# 리뷰 토픽 모델링\n",
    "drama_review_df, drama_review_info = build_review_topics(\n",
    "    PATHS['drama_review'], drama_stopwords, is_drama=True\n",
    ")\n",
    "\n",
    "movie_review_df, movie_review_info = build_review_topics(\n",
    "    PATHS['movie_review'], movie_stopwords, is_drama=False\n",
    ")\n",
    "\n",
    "print(\"\\n리뷰 토픽 모델링 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: 토픽 클러스터링 (영화 + 드라마)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_representation(x):\n",
    "    \"\"\"Representation 컬럼을 list로 변환\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return [str(w) for w in x]\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return [str(w) for w in v]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return [w for w in s.replace(\",\", \" \").split() if w]\n",
    "\n",
    "\n",
    "def make_topic_text(row, topk=20):\n",
    "    \"\"\"토픽 대표 텍스트 생성\"\"\"\n",
    "    reps = parse_representation(row.get(\"Representation\", []))\n",
    "    name = str(row.get(\"Name\", \"\")).replace(\"_\", \" \")\n",
    "    reps = reps[:topk]\n",
    "    if reps:\n",
    "        return \" \".join(reps) + \" || \" + name\n",
    "    return name\n",
    "\n",
    "\n",
    "def cluster_keywords(rep_lists, topn=12):\n",
    "    \"\"\"클러스터 대표 키워드 추출\"\"\"\n",
    "    c = Counter()\n",
    "    for reps in rep_lists:\n",
    "        for w in reps:\n",
    "            w = str(w).strip().lower()\n",
    "            if w:\n",
    "                c[w] += 1\n",
    "    return [w for w, _ in c.most_common(topn)]\n",
    "\n",
    "\n",
    "def cluster_topics(info_df, n_clusters=10):\n",
    "    \"\"\"\n",
    "    토픽을 클러스터링\n",
    "    \n",
    "    Args:\n",
    "        info_df: 토픽 정보 데이터프레임\n",
    "        n_clusters: 클러스터 수\n",
    "    \n",
    "    Returns:\n",
    "        clustered_df: cluster_id가 추가된 데이터프레임\n",
    "        summary_df: 클러스터 요약 데이터프레임\n",
    "    \"\"\"\n",
    "    df = info_df.copy()\n",
    "    \n",
    "    # 컬럼명 통일\n",
    "    if \"review_topic\" not in df.columns and \"Topic\" in df.columns:\n",
    "        df = df.rename(columns={\"Topic\": \"review_topic\"})\n",
    "    \n",
    "    # 토픽 텍스트 생성\n",
    "    df[\"__topic_text\"] = df.apply(make_topic_text, axis=1)\n",
    "    \n",
    "    # 임베딩\n",
    "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = embedder.encode(\n",
    "        df[\"__topic_text\"].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    \n",
    "    # 클러스터링\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"average\")\n",
    "    labels = clustering.fit_predict(embeddings)\n",
    "    df[\"cluster_id\"] = labels\n",
    "    \n",
    "    # Representation 파싱\n",
    "    df[\"__rep_list\"] = df[\"Representation\"].apply(parse_representation) \\\n",
    "        if \"Representation\" in df.columns else [[]]*len(df)\n",
    "    \n",
    "    # 클러스터 요약\n",
    "    summary_rows = []\n",
    "    for cid, g in df.groupby(\"cluster_id\"):\n",
    "        rep_kw = cluster_keywords(g[\"__rep_list\"].tolist(), topn=12)\n",
    "        \n",
    "        # 예시 토픽 추출\n",
    "        if \"Count\" in g.columns:\n",
    "            g_sorted = g.sort_values(\"Count\", ascending=False)\n",
    "        else:\n",
    "            g_sorted = g\n",
    "        \n",
    "        example_cols = [\"review_topic\", \"Name\"] if \"Name\" in g_sorted.columns else [\"review_topic\"]\n",
    "        examples = g_sorted.head(3)[example_cols].to_dict(\"records\")\n",
    "        \n",
    "        summary_rows.append({\n",
    "            \"cluster_id\": int(cid),\n",
    "            \"n_topics\": int(len(g)),\n",
    "            \"cluster_keywords\": \", \".join(rep_kw),\n",
    "            \"example_topics\": examples\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_rows).sort_values(\n",
    "        \"n_topics\", ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # 임시 컬럼 제거\n",
    "    clustered_df = df.drop(columns=[\"__topic_text\", \"__rep_list\"])\n",
    "    \n",
    "    return clustered_df, summary_df\n",
    "\n",
    "\n",
    "# 실행\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"토픽 클러스터링 시작\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "movie_clustered, movie_summary = cluster_topics(movie_review_info, n_clusters=10)\n",
    "drama_clustered, drama_summary = cluster_topics(drama_review_info, n_clusters=10)\n",
    "\n",
    "print(f\"\\n영화 클러스터: {len(movie_summary)}개\")\n",
    "print(f\"드라마 클러스터: {len(drama_summary)}개\")\n",
    "print(\"\\n토픽 클러스터링 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: 한글 클러스터명 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 클러스터명 매핑\n",
    "movie_cluster_ko = {\n",
    "    1: \"전반적 완성도·호불호 평가\",\n",
    "    8: \"디즈니·애니메이션 감정 반응\",\n",
    "    0: \"전쟁·역사 사실성 평가\",\n",
    "    3: \"프랜차이즈·액션 콘텐츠 평가\",\n",
    "    7: \"음악·퍼포먼스 완성도 평가\",\n",
    "    2: \"시즌·홀리데이 감성 반응\",\n",
    "    6: \"원작·각색 비교 평가\",\n",
    "    4: \"가족·인물 감정 반응\",\n",
    "    5: \"사회·인종 이슈 인식 반응\",\n",
    "    9: \"종교·신앙 메시지 해석\",\n",
    "}\n",
    "\n",
    "drama_cluster_ko = {\n",
    "    0: \"전반적 감상·전개 평가\",\n",
    "    2: \"세계관·원작 충실도 평가\",\n",
    "    3: \"사실성·논쟁 이슈 반응\",\n",
    "    1: \"젠더·정체성 표현 반응\",\n",
    "    5: \"청소년·교육적 메시지 인식\",\n",
    "    4: \"종교·신앙 메시지 해석\",\n",
    "    6: \"레이싱·모터스포츠 시청 몰입 반응\",\n",
    "    7: \"실험적 연출·예술성 평가\",\n",
    "    8: \"요리·경연 리얼리티 반응\",\n",
    "    9: \"가족·코미디 정서 반응\",\n",
    "}\n",
    "\n",
    "# 매핑 적용\n",
    "movie_clustered[\"cluster_name_ko\"] = movie_clustered[\"cluster_id\"].map(movie_cluster_ko)\n",
    "drama_clustered[\"cluster_name_ko\"] = drama_clustered[\"cluster_id\"].map(drama_cluster_ko)\n",
    "\n",
    "movie_summary[\"cluster_name_ko\"] = movie_summary[\"cluster_id\"].map(movie_cluster_ko)\n",
    "drama_summary[\"cluster_name_ko\"] = drama_summary[\"cluster_id\"].map(drama_cluster_ko)\n",
    "\n",
    "# 누락 체크\n",
    "def check_missing_mapping(df, label):\n",
    "    missing = df[df[\"cluster_name_ko\"].isna()][\"cluster_id\"].unique().tolist()\n",
    "    if missing:\n",
    "        print(f\"{label} 매핑 누락 cluster_id: {missing}\")\n",
    "    else:\n",
    "        print(f\"{label} 매핑 완료\")\n",
    "\n",
    "check_missing_mapping(movie_clustered, \"영화 info\")\n",
    "check_missing_mapping(drama_clustered, \"드라마 info\")\n",
    "check_missing_mapping(movie_summary, \"영화 summary\")\n",
    "check_missing_mapping(drama_summary, \"드라마 summary\")\n",
    "\n",
    "print(\"\\n한글 클러스터명 매핑 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 결과 저장 (선택사항)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 결과만 저장 (중간 파일 없음)\n",
    "OUTPUT_DIR = \"results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 줄거리 토픽\n",
    "drama_plot_df.to_parquet(f\"{OUTPUT_DIR}/drama_with_plot_topic.parquet\", index=False)\n",
    "movie_plot_df.to_parquet(f\"{OUTPUT_DIR}/movie_with_plot_topic.parquet\", index=False)\n",
    "\n",
    "# 리뷰 토픽\n",
    "drama_review_df.to_parquet(f\"{OUTPUT_DIR}/drama_review_with_topic.parquet\", index=False)\n",
    "movie_review_df.to_parquet(f\"{OUTPUT_DIR}/movie_review_with_topic.parquet\", index=False)\n",
    "\n",
    "# 클러스터 정보 (한글명 포함)\n",
    "movie_clustered.to_csv(f\"{OUTPUT_DIR}/movie_topic_info_clustered.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "drama_clustered.to_csv(f\"{OUTPUT_DIR}/drama_topic_info_clustered.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "movie_summary.to_csv(f\"{OUTPUT_DIR}/movie_cluster_summary.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "drama_summary.to_csv(f\"{OUTPUT_DIR}/drama_cluster_summary.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n모든 결과가 '{OUTPUT_DIR}' 디렉토리에 저장되었습니다.\")\n",
    "print(\"\\n저장된 파일:\")\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 변수 참조 가이드\n",
    "\n",
    "### 주요 변수 설명:\n",
    "\n",
    "**줄거리 관련:**\n",
    "- `drama_plot_df` / `movie_plot_df`: plot_topic 컬럼이 추가된 원본 데이터\n",
    "- `drama_plot_keywords` / `movie_plot_keywords`: {topic_id: [keywords]} 딕셔너리\n",
    "- `drama_plot_info` / `movie_plot_info`: BERTopic의 get_topic_info() 결과\n",
    "\n",
    "**리뷰 관련:**\n",
    "- `drama_review_df` / `movie_review_df`: review_topic 컬럼이 추가된 샘플 데이터\n",
    "- `drama_review_info` / `movie_review_info`: BERTopic의 get_topic_info() 결과\n",
    "\n",
    "**클러스터 관련:**\n",
    "- `drama_clustered` / `movie_clustered`: cluster_id와 cluster_name_ko가 추가된 토픽 정보\n",
    "- `drama_summary` / `movie_summary`: 클러스터별 요약 정보"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
