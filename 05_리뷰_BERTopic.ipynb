{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "drama = pd.read_parquet('00_drama_main.parquet')\n",
    "movie = pd.read_parquet('00_movie_main.parquet')\n",
    "# =========================================================\n",
    "# RESET RUN (DRAMA ONLY) - Plot Topic (overview) / GPU SAFE\n",
    "# - 드라마 데이터셋만 따로: 임베딩/토픽 모델 분리\n",
    "# - max_df < min_df 에러 원천 차단 (토픽 내부 문서수까지 고려)\n",
    "# - outlier(-1) 과도하면 파라미터만 조금씩 완화하는 방식으로 튜닝\n",
    "# =========================================================\n",
    "\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# -------------------------\n",
    "# 0) 경로/설정\n",
    "# -------------------------\n",
    "PATH_DRAMA = \"00_drama_main.parquet\"   # 파일명 그대로\n",
    "OUT_DRAMA  = \"drama_with_plot_topic.parquet\"\n",
    "OUT_INFO   = \"plot_topic_info_drama.csv\"\n",
    "OUT_KW     = \"plot_keywords_drama.parquet\"\n",
    "\n",
    "SEED = 42\n",
    "MIN_OVERVIEW_LEN = 30\n",
    "TOP_N_WORDS = 30\n",
    "\n",
    "# -------------------------\n",
    "# 1) 전처리\n",
    "# -------------------------\n",
    "def clean_text(s: str) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# -------------------------\n",
    "# 2) 로드 + 필터\n",
    "# -------------------------\n",
    "drama = pd.read_parquet(PATH_DRAMA).copy()\n",
    "\n",
    "need_cols = [\"imdb_id\", \"overview\"]\n",
    "missing = [c for c in need_cols if c not in drama.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"드라마 데이터에 필요한 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "drama[\"overview_clean\"] = drama[\"overview\"].apply(clean_text)\n",
    "drama = drama[drama[\"overview_clean\"].str.len() >= MIN_OVERVIEW_LEN].copy()\n",
    "docs = drama[\"overview_clean\"].tolist()\n",
    "\n",
    "print(\"✅ drama docs(after filter):\", len(docs))\n",
    "if len(docs) < 200:\n",
    "    print(\"⚠️ 드라마 문서 수가 너무 적으면 토픽 품질이 낮아질 수 있어요.\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) GPU 임베딩 모델\n",
    "# -------------------------\n",
    "embedding_model = SentenceTransformer(\n",
    "    \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    device=\"cuda\"  # GPU 사용\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4) 드라마 전용 파라미터(안정 + 토픽 뭉개짐 방지)\n",
    "# -------------------------\n",
    "# ✅ 핵심: 여기서는 '토픽 내부 문서수'까지 고려해서 에러가 절대 안 나게 설정\n",
    "# - min_df=1, max_df=1.0  (토픽이 1개 문서여도 CountVectorizer가 안 터짐)\n",
    "# - 대신 너무 흔한 단어는 stop_words=\"english\" + token_pattern으로 1차 걸러짐\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r\"\\b[a-zA-Z]{3,}\\b\",\n",
    "    min_df=1,\n",
    "    max_df=1.0\n",
    ")\n",
    "\n",
    "# 드라마는 문서 수가 작으니 클러스터를 조금 더 잘게 쪼개도록 완화\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,     # 작을수록 더 로컬(토픽 더 잘게)\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=20,  # 작게: 토픽 수 늘리는 방향\n",
    "    min_samples=5,\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    top_n_words=TOP_N_WORDS,\n",
    "    min_topic_size=20,        # 토픽 최소 문서수\n",
    "    nr_topics=None,\n",
    "    calculate_probabilities=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 5) 학습\n",
    "# -------------------------\n",
    "topics, _ = topic_model.fit_transform(docs)\n",
    "drama[\"plot_topic\"] = topics\n",
    "\n",
    "# -------------------------\n",
    "# 6) 저장(요약표/키워드/데이터)\n",
    "# -------------------------\n",
    "info = topic_model.get_topic_info()\n",
    "info.to_csv(OUT_INFO, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "kw_rows = []\n",
    "for tid, tuples in topic_model.get_topics().items():\n",
    "    words = [w for w, _ in tuples if w]\n",
    "    kw_rows.append({\"plot_topic\": tid, \"keywords\": words})\n",
    "pd.DataFrame(kw_rows).to_parquet(OUT_KW, index=False)\n",
    "\n",
    "drama.to_parquet(OUT_DRAMA, index=False)\n",
    "\n",
    "# -------------------------\n",
    "# 7) 바로 검증(너가 봐야 할 3개만)\n",
    "# -------------------------\n",
    "outlier_ratio = (drama[\"plot_topic\"] == -1).mean()\n",
    "n_topics_incl = drama[\"plot_topic\"].nunique()\n",
    "n_topics_excl = drama.loc[drama[\"plot_topic\"] != -1, \"plot_topic\"].nunique()\n",
    "\n",
    "print(\"\\n================ CHECK ================\")\n",
    "print(\"outlier(-1) ratio:\", float(outlier_ratio))\n",
    "print(\"n_topics (incl -1):\", int(n_topics_incl))\n",
    "print(\"n_topics (excl -1):\", int(n_topics_excl))\n",
    "print(\"top topic sizes:\\n\", pd.Series(topics).value_counts().head(10))\n",
    "print(\"saved:\", OUT_DRAMA, OUT_INFO, OUT_KW)\n",
    "print(\"✅ DONE (DRAMA)\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =========================================================\n",
    "# MOVIE RUN (GPU) - Plot Topic (overview) [분리 임베딩]\n",
    "# - input : 00_movie_main.parquet  (or 네 영화 parquet 경로)\n",
    "# - output:\n",
    "#   1) movie_with_plot_topic.parquet\n",
    "#   2) plot_topic_info_movie.csv\n",
    "#   3) plot_keywords_movie.parquet\n",
    "# =========================================================\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# -------------------------\n",
    "# 0) 경로/설정\n",
    "# -------------------------\n",
    "MOVIE_PATH = \"00_movie_main.parquet\"\n",
    "OUT_MOVIE  = \"movie_with_plot_topic.parquet\"\n",
    "OUT_INFO   = \"plot_topic_info_movie.csv\"\n",
    "OUT_KW     = \"plot_keywords_movie.parquet\"\n",
    "\n",
    "SEED = 42\n",
    "MIN_OVERVIEW_LEN = 30\n",
    "TOP_N_WORDS = 30\n",
    "\n",
    "# -------------------------\n",
    "# 1) GPU 임베딩 모델\n",
    "# -------------------------\n",
    "embedding_model = SentenceTransformer(\n",
    "    \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 2) 텍스트 클린\n",
    "# -------------------------\n",
    "def clean_text(s: str) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# -------------------------\n",
    "# 3) 로드 + 필터\n",
    "# -------------------------\n",
    "movie = pd.read_parquet(MOVIE_PATH).copy()\n",
    "\n",
    "need_cols = [\"imdb_id\", \"overview\"]\n",
    "missing = [c for c in need_cols if c not in movie.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"movie에 필요한 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "movie[\"overview_clean\"] = movie[\"overview\"].apply(clean_text)\n",
    "movie = movie[movie[\"overview_clean\"].str.len() >= MIN_OVERVIEW_LEN].copy()\n",
    "\n",
    "docs = movie[\"overview_clean\"].tolist()\n",
    "n_docs = len(docs)\n",
    "print(\"movie docs(after filter):\", n_docs)\n",
    "\n",
    "# -------------------------\n",
    "# 4) 영화용 파라미터 (드라마보다 보수적으로)\n",
    "# -------------------------\n",
    "# vectorizer: 문서 수 비례 min_df (너무 빡세면 토픽이 말라죽음)\n",
    "MIN_DF = max(5, int(n_docs * 0.002))   # 0.2% ≈ 27k면 ~54\n",
    "MAX_DF = 0.90\n",
    "\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r\"\\b[a-zA-Z]{3,}\\b\",\n",
    "    min_df=MIN_DF,\n",
    "    max_df=MAX_DF\n",
    ")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=20,\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50,\n",
    "    min_samples=10,\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    top_n_words=TOP_N_WORDS,\n",
    "    min_topic_size=50,\n",
    "    nr_topics=None,\n",
    "    calculate_probabilities=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n[MOVIE] vectorizer params:\", {\"min_df\": MIN_DF, \"max_df\": MAX_DF})\n",
    "print(\"[MOVIE] cluster/topic params:\", {\"min_cluster_size\": 50, \"min_samples\": 10, \"min_topic_size\": 50, \"n_neighbors\": 20, \"n_components\": 5})\n",
    "\n",
    "# -------------------------\n",
    "# 5) 학습\n",
    "# -------------------------\n",
    "topics, _ = topic_model.fit_transform(docs)\n",
    "movie[\"plot_topic\"] = topics\n",
    "\n",
    "# -------------------------\n",
    "# 6) 저장 (분리)\n",
    "# -------------------------\n",
    "movie.to_parquet(OUT_MOVIE, index=False)\n",
    "topic_model.get_topic_info().to_csv(OUT_INFO, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "kw_rows = []\n",
    "for tid, tuples in topic_model.get_topics().items():\n",
    "    if tuples is None:\n",
    "        continue\n",
    "    words = [w for w, _ in tuples if w]\n",
    "    kw_rows.append({\"plot_topic\": tid, \"keywords\": words})\n",
    "pd.DataFrame(kw_rows).to_parquet(OUT_KW, index=False)\n",
    "\n",
    "# -------------------------\n",
    "# 7) 간단 체크\n",
    "# -------------------------\n",
    "outlier_ratio = (movie[\"plot_topic\"] == -1).mean()\n",
    "n_topics_incl = movie[\"plot_topic\"].nunique()\n",
    "n_topics_excl = movie.loc[movie[\"plot_topic\"] != -1, \"plot_topic\"].nunique()\n",
    "\n",
    "print(\"\\n============== CHECK ==============\")\n",
    "print(\"outlier(-1) ratio:\", float(outlier_ratio))\n",
    "print(\"n_topics (incl -1):\", int(n_topics_incl))\n",
    "print(\"n_topics (excl -1):\", int(n_topics_excl))\n",
    "print(\"top topic sizes:\\n\", movie[\"plot_topic\"].value_counts().head(10))\n",
    "print(\"saved:\", OUT_MOVIE, OUT_INFO, OUT_KW)\n",
    "print(\"✅ DONE (MOVIE)\")\n"
   ],
   "id": "bdff26a70b2c315b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =========================================\n",
    "# Drama Review BERTopic (50k) + Plot Stopwords 적용\n",
    "# - plot_keywords_drama.parquet 에서 \"모든\" 줄거리 키워드 모아서 stopwords로 사용\n",
    "# - 8GB(4070 laptop) 기준: batch 작게 + 오류시 CPU fallback\n",
    "# - output:\n",
    "#   1) review_topic_info_drama_50k.csv\n",
    "#   2) review_topic_map_drama_50k.parquet\n",
    "# =========================================\n",
    "\n",
    "import os, gc, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# -------------------------\n",
    "# 0) 설정\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "REVIEW_PATH  = \"drama_review_final.parquet\"        # ✅ 드라마 리뷰 파일명\n",
    "PLOT_KW_PATH = \"plot_keywords_drama.parquet\"       # ✅ 드라마 줄거리 키워드 파일\n",
    "TEXT_COL = \"review_text\"\n",
    "IMDB_COL = \"imdb_id\"\n",
    "\n",
    "SAMPLE_N = 50_000\n",
    "LEN_MIN, LEN_MAX = 50, 2000\n",
    "\n",
    "OUT_INFO = \"review_topic_info_drama_50k.csv\"\n",
    "OUT_MAP  = \"review_topic_map_drama_50k.parquet\"\n",
    "\n",
    "# GPU 환경 안정화(가끔 \"CUDA unknown error\" 완화용)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# -------------------------\n",
    "# 1) Plot stopwords 만들기 (핵심!)\n",
    "# -------------------------\n",
    "pk = pd.read_parquet(PLOT_KW_PATH)\n",
    "\n",
    "# keywords 컬럼이 ndarray/list 형태여도 flatten 되게 처리\n",
    "plot_words = set()\n",
    "for kws in pk[\"keywords\"].tolist():\n",
    "    if kws is None:\n",
    "        continue\n",
    "    for w in list(kws):\n",
    "        if not w:\n",
    "            continue\n",
    "        w = str(w).strip().lower()\n",
    "        if w:\n",
    "            plot_words.add(w)\n",
    "\n",
    "# 줄거리에서 너무 흔해서 빼는 게 오히려 더 좋은 \"범용 서사/형식 단어\"는 추가로 제거(선택)\n",
    "# ✅ 드라마는 시즌/에피소드 언급이 많아서 여기 ban_words를 좀 더 강하게 둠\n",
    "ban_words = {\n",
    "    \"film\",\"movie\",\"movies\",\n",
    "    \"story\",\"stories\",\"plot\",\n",
    "    \"series\",\"show\",\"shows\",\n",
    "    \"season\",\"seasons\",\"episode\",\"episodes\",\n",
    "    \"character\",\"characters\",\n",
    "    # 드라마 리뷰에서 특히 흔한 단어들(필요 없으면 빼도 됨)\n",
    "    \"drama\",\"dramas\",\"tv\",\"television\",\n",
    "}\n",
    "plot_words = plot_words - ban_words\n",
    "\n",
    "base_sw = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# 리뷰에서만 제거하고 싶은 \"평가/관용어\"\n",
    "review_generic_sw = {\n",
    "    \"like\",\"just\",\"good\",\"really\",\"time\",\"way\",\"watch\",\"watched\",\"watching\",\"people\",\n",
    "    \"dont\",\"didnt\",\"doesnt\",\"isnt\",\"wasnt\",\"werent\",\"cant\",\"couldnt\",\"wouldnt\",\n",
    "    \"im\",\"ive\",\"youre\",\"theyre\",\"thats\",\"theres\",\"hes\",\"shes\",\"weve\",\"id\",\n",
    "    # 형식/대상 단어(드라마 쪽 강화)\n",
    "    \"film\",\"films\",\"movie\",\"movies\",\"show\",\"shows\",\"series\",\"season\",\"seasons\",\"episode\",\"episodes\",\n",
    "    \"drama\",\"dramas\",\"tv\",\"television\",\n",
    "    \"story\",\"plot\",\"character\",\"characters\",\n",
    "}\n",
    "\n",
    "combined_sw = sorted(list(base_sw | plot_words | review_generic_sw))\n",
    "\n",
    "print(f\"[Stopwords] ENGLISH={len(base_sw)}, plot_words={len(plot_words)}, total={len(combined_sw)}\")\n",
    "print(\"plot_words sample:\", list(sorted(list(plot_words)))[:30])\n",
    "\n",
    "# -------------------------\n",
    "# 2) 리뷰 로드 + 필터 + 샘플\n",
    "# -------------------------\n",
    "df = pd.read_parquet(REVIEW_PATH)\n",
    "\n",
    "# 텍스트 유효\n",
    "df = df[df[TEXT_COL].notna()].copy()\n",
    "df[TEXT_COL] = df[TEXT_COL].astype(str)\n",
    "\n",
    "# 길이 컷\n",
    "df[\"__len\"] = df[TEXT_COL].str.len()\n",
    "df = df[(df[\"__len\"] >= LEN_MIN) & (df[\"__len\"] <= LEN_MAX)].copy()\n",
    "\n",
    "print(\"after filter:\", len(df))\n",
    "\n",
    "# 샘플\n",
    "df_s = df.sample(n=min(SAMPLE_N, len(df)), random_state=SEED).copy()\n",
    "docs = df_s[TEXT_COL].tolist()\n",
    "\n",
    "print(\"sample docs:\", len(docs))\n",
    "\n",
    "# 메모리 정리\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "# -------------------------\n",
    "# 3) BERTopic 구성\n",
    "# -------------------------\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=combined_sw,\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r\"\\b[a-zA-Z]{3,}\\b\",\n",
    "    min_df=20,       # 50k면 10~30 사이 추천\n",
    "    max_df=0.95,\n",
    ")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,     # 안정\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50,\n",
    "    min_samples=10,\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4) 임베딩 + fit_transform (GPU 우선, 실패하면 CPU fallback)\n",
    "# -------------------------\n",
    "def run_bertopic(docs, device=\"cuda\", enc_batch=16):\n",
    "    # ✅ Qwen 임베딩 모델\n",
    "    embedder = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\", device=device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        embeddings = embedder.encode(\n",
    "            docs,\n",
    "            batch_size=enc_batch,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=None,      # embeddings를 직접 넣을 거라 None\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        calculate_probabilities=False,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "    return topic_model, topics\n",
    "\n",
    "# 1차: GPU 시도\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "    model, topics = run_bertopic(docs, device=\"cuda\", enc_batch=16)\n",
    "except Exception as e:\n",
    "    print(\"\\n⚠️ GPU 실패 -> CPU로 fallback\")\n",
    "    print(\"GPU error:\", repr(e))\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model, topics = run_bertopic(docs, device=\"cpu\", enc_batch=32)\n",
    "\n",
    "# -------------------------\n",
    "# 5) 결과 저장\n",
    "# -------------------------\n",
    "info = model.get_topic_info()\n",
    "info.to_csv(OUT_INFO, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "df_s[\"review_topic\"] = topics\n",
    "df_s.drop(columns=[\"__len\"], inplace=True, errors=\"ignore\")\n",
    "df_s.to_parquet(OUT_MAP, index=False)\n",
    "\n",
    "outlier_ratio = (df_s[\"review_topic\"] == -1).mean()\n",
    "print(\"\\n===== DONE =====\")\n",
    "print(\"outlier(-1) ratio:\", float(outlier_ratio))\n",
    "print(\"n_topics(excl -1):\", int((info[\"Topic\"] != -1).sum()))\n",
    "print(\"saved:\", OUT_INFO, OUT_MAP)\n",
    "\n",
    "# -------------------------\n",
    "# 6) 간단 검증(토픽 분포 Top15)\n",
    "# -------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tmp = df_s[df_s[\"review_topic\"] != -1][\"review_topic\"].value_counts().head(15).sort_index()\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(tmp.index.astype(str), tmp.values)\n",
    "plt.title(\"Drama Review Topic Distribution (Top 15, excl -1)\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Review Count\")\n",
    "plt.show()\n"
   ],
   "id": "3c8af003fb5d216f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =========================================\n",
    "# Movie Review BERTopic (50k) + Plot Stopwords 적용\n",
    "# - plot_keywords_movie.parquet 에서 \"모든\" 줄거리 키워드 모아서 stopwords로 사용\n",
    "# - 8GB(4070 laptop) 기준: batch 작게 + fp16 + 오류시 CPU fallback\n",
    "# - output:\n",
    "#   1) review_topic_info_movie_50k.csv\n",
    "#   2) review_topic_map_movie_50k.parquet\n",
    "# =========================================\n",
    "\n",
    "import os, gc, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# -------------------------\n",
    "# 0) 설정\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "REVIEW_PATH = \"movie_review_final.parquet\"        # ✅ 네 영화 리뷰 파일명에 맞게 수정\n",
    "PLOT_KW_PATH = \"plot_keywords_movie.parquet\"  # ✅ 이미 만든 줄거리 키워드 파일\n",
    "TEXT_COL = \"review_text\"\n",
    "IMDB_COL = \"imdb_id\"\n",
    "\n",
    "SAMPLE_N = 50_000\n",
    "LEN_MIN, LEN_MAX = 50, 2000\n",
    "\n",
    "OUT_INFO = \"review_topic_info_movie_50k.csv\"\n",
    "OUT_MAP  = \"review_topic_map_movie_50k.parquet\"\n",
    "\n",
    "# GPU 환경 안정화(가끔 \"CUDA unknown error\" 완화용)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# -------------------------\n",
    "# 1) Plot stopwords 만들기 (핵심!)\n",
    "# -------------------------\n",
    "pk = pd.read_parquet(PLOT_KW_PATH)\n",
    "\n",
    "# keywords 컬럼이 ndarray/list 형태여도 flatten 되게 처리\n",
    "plot_words = set()\n",
    "for kws in pk[\"keywords\"].tolist():\n",
    "    if kws is None:\n",
    "        continue\n",
    "    for w in list(kws):\n",
    "        if not w:\n",
    "            continue\n",
    "        w = str(w).strip().lower()\n",
    "        if w:\n",
    "            plot_words.add(w)\n",
    "\n",
    "# 줄거리에서 너무 흔해서 빼는 게 오히려 더 좋은 \"범용 서사/형식 단어\"는 추가로 제거(선택)\n",
    "ban_words = {\n",
    "    \"film\",\"movie\",\"movies\",\"story\",\"stories\",\"series\",\"season\",\"seasons\",\"episode\",\"episodes\",\n",
    "    \"character\",\"characters\",\"plot\",\"show\",\"shows\"\n",
    "}\n",
    "plot_words = plot_words - ban_words\n",
    "\n",
    "base_sw = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# 리뷰에서만 제거하고 싶은 \"평가/관용어\" (너가 쓰던 것 있으면 여기 더 붙여도 됨)\n",
    "review_generic_sw = {\n",
    "    \"like\",\"just\",\"good\",\"really\",\"time\",\"way\",\"watch\",\"watched\",\"watching\",\"people\",\n",
    "    \"dont\",\"didnt\",\"doesnt\",\"isnt\",\"wasnt\",\"werent\",\"cant\",\"couldnt\",\"wouldnt\",\n",
    "    \"im\",\"ive\",\"youre\",\"theyre\",\"thats\",\"theres\",\"hes\",\"shes\",\"weve\",\"id\",\n",
    "    \"film\",\"films\",\"movie\",\"movies\",\"show\",\"shows\",\"series\",\"season\",\"seasons\",\"episode\",\"episodes\",\n",
    "    \"story\",\"plot\",\"character\",\"characters\",\n",
    "}\n",
    "\n",
    "combined_sw = sorted(list(base_sw | plot_words | review_generic_sw))\n",
    "\n",
    "print(f\"[Stopwords] ENGLISH={len(base_sw)}, plot_words={len(plot_words)}, total={len(combined_sw)}\")\n",
    "print(\"plot_words sample:\", list(sorted(list(plot_words)))[:30])\n",
    "\n",
    "# -------------------------\n",
    "# 2) 리뷰 로드 + 필터 + 샘플\n",
    "# -------------------------\n",
    "df = pd.read_parquet(REVIEW_PATH)\n",
    "\n",
    "# 텍스트 유효\n",
    "df = df[df[TEXT_COL].notna()].copy()\n",
    "df[TEXT_COL] = df[TEXT_COL].astype(str)\n",
    "\n",
    "# 길이 컷\n",
    "df[\"__len\"] = df[TEXT_COL].str.len()\n",
    "df = df[(df[\"__len\"] >= LEN_MIN) & (df[\"__len\"] <= LEN_MAX)].copy()\n",
    "\n",
    "print(\"after filter:\", len(df))\n",
    "\n",
    "# 샘플\n",
    "df_s = df.sample(n=min(SAMPLE_N, len(df)), random_state=SEED).copy()\n",
    "docs = df_s[TEXT_COL].tolist()\n",
    "\n",
    "print(\"sample docs:\", len(docs))\n",
    "\n",
    "# 메모리 정리\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "# -------------------------\n",
    "# 3) BERTopic 구성\n",
    "# -------------------------\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=combined_sw,\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r\"\\b[a-zA-Z]{3,}\\b\",\n",
    "    min_df=20,       # 50k면 10~30 사이 추천\n",
    "    max_df=0.95,\n",
    ")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,     # 안정\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50,\n",
    "    min_samples=10,\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4) 임베딩 + fit_transform (GPU 우선, 실패하면 CPU fallback)\n",
    "# -------------------------\n",
    "def run_bertopic(docs, device=\"cuda\", enc_batch=16):\n",
    "    # ✅ Qwen 임베딩 모델 (너가 쓰던 그대로)\n",
    "    embedder = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\", device=device)\n",
    "\n",
    "    # 안정: fp16 + 작은 배치\n",
    "    with torch.inference_mode():\n",
    "        embeddings = embedder.encode(\n",
    "            docs,\n",
    "            batch_size=enc_batch,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=None,      # embeddings를 직접 넣을 거라 None\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        calculate_probabilities=False,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "    return topic_model, topics\n",
    "\n",
    "# 1차: GPU 시도\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "    model, topics = run_bertopic(docs, device=\"cuda\", enc_batch=16)\n",
    "except Exception as e:\n",
    "    print(\"\\n⚠️ GPU 실패 -> CPU로 fallback\")\n",
    "    print(\"GPU error:\", repr(e))\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model, topics = run_bertopic(docs, device=\"cpu\", enc_batch=32)\n",
    "\n",
    "# -------------------------\n",
    "# 5) 결과 저장\n",
    "# -------------------------\n",
    "info = model.get_topic_info()\n",
    "info.to_csv(OUT_INFO, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "df_s[\"review_topic\"] = topics\n",
    "df_s.drop(columns=[\"__len\"], inplace=True, errors=\"ignore\")\n",
    "df_s.to_parquet(OUT_MAP, index=False)\n",
    "\n",
    "outlier_ratio = (df_s[\"review_topic\"] == -1).mean()\n",
    "print(\"\\n===== DONE =====\")\n",
    "print(\"outlier(-1) ratio:\", float(outlier_ratio))\n",
    "print(\"n_topics(excl -1):\", int((info[\"Topic\"] != -1).sum()))\n",
    "print(\"saved:\", OUT_INFO, OUT_MAP)\n",
    "\n",
    "# -------------------------\n",
    "# 6) 간단 검증(토픽 분포 Top15)\n",
    "# -------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tmp = df_s[df_s[\"review_topic\"] != -1][\"review_topic\"].value_counts().head(15).sort_index()\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(tmp.index.astype(str), tmp.values)\n",
    "plt.title(\"Review Topic Distribution (Top 15, excl -1)\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Review Count\")\n",
    "plt.show()"
   ],
   "id": "239ac8903b9dd603"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
