# ==========================================================
# IMDB ë°ì´í„° í¬ë¡¤ëŸ¬ (ë¦¬ë·°, í‰ì , ë©”íƒ€ìŠ¤ì½”ì–´)
# TV ì‹œë¦¬ì¦ˆ ì „ìš© - ë¹„ë™ê¸° + Rate Limiting
# ==========================================================

import asyncio
import aiohttp
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import json
import re
import time
from pathlib import Path
import random

# ==========================================================
# ì„¤ì •
# ==========================================================

# Rate Limiting (IMDBëŠ” ì—„ê²©í•˜ë¯€ë¡œ ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •)
MAX_CALLS_PER_SECOND = 2  # ì´ˆë‹¹ 2íšŒ (ì•ˆì „í•œ ì†ë„)
TIMEOUT = aiohttp.ClientTimeout(total=30, connect=10)
MAX_RETRIES = 3
RETRY_DELAY = [2, 5, 10]  # ì¬ì‹œë„ ê°„ê²© (ì´ˆ)

# User-Agent (ì‹¤ì œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ë„ë¡)
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
]

# ì¶œë ¥ íŒŒì¼
OUTPUT_CSV = "imdb_data_collected.csv"
OUTPUT_PARQUET = "imdb_data_collected.parquet"
CHECKPOINT_FILE = "imdb_checkpoint.json"

# í†µê³„
stats = {
    "total": 0,
    "success": 0,
    "failed": 0,
    "requests": 0,
    "start_time": None
}

# ==========================================================
# Rate Limiter
# ==========================================================
class RateLimiter:
    def __init__(self, rate):
        self.rate = rate
        self.tokens = rate
        self.updated_at = time.monotonic()
        self.lock = asyncio.Lock()
    
    async def acquire(self):
        async with self.lock:
            now = time.monotonic()
            elapsed = now - self.updated_at
            
            # í† í° ë³´ì¶©
            self.tokens = min(self.rate, self.tokens + elapsed * self.rate)
            self.updated_at = now
            
            # í† í° ë¶€ì¡±ì‹œ ëŒ€ê¸°
            if self.tokens < 1:
                sleep_time = (1 - self.tokens) / self.rate
                await asyncio.sleep(sleep_time)
                self.tokens = 1
            
            self.tokens -= 1

rate_limiter = RateLimiter(MAX_CALLS_PER_SECOND)

# ==========================================================
# HTML ê°€ì ¸ì˜¤ê¸°
# ==========================================================
async def fetch_html(session, url, retry=0):
    """HTML í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    if retry >= MAX_RETRIES:
        stats["failed"] += 1
        return None
    
    await rate_limiter.acquire()
    stats["requests"] += 1
    
    headers = {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    try:
        async with session.get(url, headers=headers, timeout=TIMEOUT) as resp:
            # Rate limit ê°ì§€
            if resp.status == 429 or resp.status == 503:
                wait_time = RETRY_DELAY[min(retry, len(RETRY_DELAY)-1)]
                print(f"âš ï¸  Rate limited, waiting {wait_time}s...")
                await asyncio.sleep(wait_time)
                return await fetch_html(session, url, retry + 1)
            
            # 404ëŠ” ì •ìƒ ì¼€ì´ìŠ¤ (ë°ì´í„° ì—†ìŒ)
            if resp.status == 404:
                return None
            
            # ê¸°íƒ€ ì—ëŸ¬
            if resp.status != 200:
                if retry < MAX_RETRIES - 1:
                    await asyncio.sleep(RETRY_DELAY[retry])
                    return await fetch_html(session, url, retry + 1)
                return None
            
            html = await resp.text()
            return html
    
    except asyncio.TimeoutError:
        print(f"âš ï¸  Timeout: {url}")
        if retry < MAX_RETRIES - 1:
            await asyncio.sleep(RETRY_DELAY[retry])
            return await fetch_html(session, url, retry + 1)
        return None
    
    except Exception as e:
        print(f"âŒ Error fetching {url}: {e}")
        if retry < MAX_RETRIES - 1:
            await asyncio.sleep(RETRY_DELAY[retry])
            return await fetch_html(session, url, retry + 1)
        return None

# ==========================================================
# IMDB í‰ì  & ë©”íƒ€ìŠ¤ì½”ì–´ ì¶”ì¶œ
# ==========================================================
def extract_rating_and_metascore(soup, imdb_id):
    """ë©”ì¸ í˜ì´ì§€ì—ì„œ í‰ì ê³¼ ë©”íƒ€ìŠ¤ì½”ì–´ ì¶”ì¶œ"""
    result = {
        'imdb_id': imdb_id,
        'imdb_rating': None,
        'imdb_rating_count': None,
        'meta_score': None
    }
    
    try:
        # IMDB Rating - JSON-LDì—ì„œ ì¶”ì¶œ (ê°€ì¥ ì •í™•)
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict) and 'aggregateRating' in data:
                    rating_data = data['aggregateRating']
                    result['imdb_rating'] = float(rating_data.get('ratingValue', 0))
                    result['imdb_rating_count'] = int(rating_data.get('ratingCount', 0))
                    break
            except:
                continue
        
        # ëŒ€ì²´ ë°©ë²•: div[data-testid="hero-rating-bar__aggregate-rating__score"]
        if result['imdb_rating'] is None:
            rating_elem = soup.find('div', {'data-testid': 'hero-rating-bar__aggregate-rating__score'})
            if rating_elem:
                rating_text = rating_elem.get_text(strip=True)
                match = re.search(r'([\d.]+)', rating_text)
                if match:
                    result['imdb_rating'] = float(match.group(1))
        
        # Meta Score - ì—¬ëŸ¬ ì„ íƒì ì‹œë„
        metascore_selectors = [
            {'class': 'metacritic-score-box'},
            {'data-testid': 'metacritic-score'},
            {'class': 'score-meta'}
        ]
        
        for selector in metascore_selectors:
            meta_elem = soup.find('span', selector) or soup.find('div', selector)
            if meta_elem:
                meta_text = meta_elem.get_text(strip=True)
                match = re.search(r'(\d+)', meta_text)
                if match:
                    result['meta_score'] = int(match.group(1))
                    break
    
    except Exception as e:
        print(f"âš ï¸  Error parsing rating/metascore for {imdb_id}: {e}")
    
    return result

# ==========================================================
# IMDB ë¦¬ë·° ì¶”ì¶œ
# ==========================================================
def extract_reviews(soup, imdb_id, max_reviews=10):
    """ë¦¬ë·° í˜ì´ì§€ì—ì„œ ë¦¬ë·° ì¶”ì¶œ"""
    reviews = []
    
    try:
        # ë¦¬ë·° ì»¨í…Œì´ë„ˆ ì°¾ê¸°
        review_containers = soup.find_all('div', {'class': 'review-container'})
        
        for container in review_containers[:max_reviews]:
            review = {}
            
            # ë¦¬ë·° ì œëª©
            title_elem = container.find('a', {'class': 'title'})
            if title_elem:
                review['title'] = title_elem.get_text(strip=True)
            
            # í‰ì 
            rating_elem = container.find('span', {'class': 'rating-other-user-rating'})
            if rating_elem:
                rating_span = rating_elem.find('span')
                if rating_span:
                    try:
                        review['user_rating'] = int(rating_span.get_text(strip=True))
                    except:
                        pass
            
            # ë¦¬ë·° ë‚´ìš©
            content_elem = container.find('div', {'class': 'text'})
            if content_elem:
                review['content'] = content_elem.get_text(strip=True)
            
            # ì‘ì„±ì
            author_elem = container.find('span', {'class': 'display-name-link'})
            if author_elem:
                review['author'] = author_elem.get_text(strip=True)
            
            # ë‚ ì§œ
            date_elem = container.find('span', {'class': 'review-date'})
            if date_elem:
                review['date'] = date_elem.get_text(strip=True)
            
            # Helpful íˆ¬í‘œ
            helpful_elem = container.find('div', {'class': 'actions'})
            if helpful_elem:
                helpful_text = helpful_elem.get_text()
                match = re.search(r'(\d+)\s+out of\s+(\d+)', helpful_text)
                if match:
                    review['helpful'] = f"{match.group(1)}/{match.group(2)}"
            
            if review:  # ìµœì†Œí•œ í•˜ë‚˜ì˜ í•„ë“œë¼ë„ ìˆìœ¼ë©´ ì¶”ê°€
                reviews.append(review)
        
    except Exception as e:
        print(f"âš ï¸  Error parsing reviews for {imdb_id}: {e}")
    
    return reviews

# ==========================================================
# ë‹¨ì¼ IMDB ID ì²˜ë¦¬
# ==========================================================
async def scrape_imdb_data(session, imdb_id, series_title=""):
    """í•˜ë‚˜ì˜ IMDB IDì— ëŒ€í•œ ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘"""
    stats["total"] += 1
    
    result = {
        'imdb_id': imdb_id,
        'series_title': series_title,
        'imdb_rating': None,
        'imdb_rating_count': None,
        'meta_score': None,
        'reviews_json': None,
        'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    try:
        # 1. ë©”ì¸ í˜ì´ì§€ì—ì„œ í‰ì  & ë©”íƒ€ìŠ¤ì½”ì–´
        main_url = f"https://www.imdb.com/title/{imdb_id}/"
        main_html = await fetch_html(session, main_url)
        
        if main_html:
            soup = BeautifulSoup(main_html, 'html.parser')
            rating_data = extract_rating_and_metascore(soup, imdb_id)
            result.update(rating_data)
        
        # 2. ë¦¬ë·° í˜ì´ì§€ì—ì„œ ë¦¬ë·° ìˆ˜ì§‘
        reviews_url = f"https://www.imdb.com/title/{imdb_id}/reviews/"
        reviews_html = await fetch_html(session, reviews_url)
        
        if reviews_html:
            soup_reviews = BeautifulSoup(reviews_html, 'html.parser')
            reviews = extract_reviews(soup_reviews, imdb_id)
            if reviews:
                result['reviews_json'] = json.dumps(reviews, ensure_ascii=False)
        
        stats["success"] += 1
        return result
    
    except Exception as e:
        print(f"âŒ Error processing {imdb_id}: {e}")
        stats["failed"] += 1
        return result

# ==========================================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
# ==========================================================
def save_checkpoint(processed_ids):
    """ì§„í–‰ ìƒí™© ì €ì¥"""
    with open(CHECKPOINT_FILE, 'w') as f:
        json.dump({'processed_ids': list(processed_ids)}, f)

def load_checkpoint():
    """ì´ì „ ì§„í–‰ ìƒí™© ë¡œë“œ"""
    if Path(CHECKPOINT_FILE).exists():
        with open(CHECKPOINT_FILE, 'r') as f:
            return set(json.load(f)['processed_ids'])
    return set()

# ==========================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ==========================================================
async def main(input_csv_path):
    """
    TMDB ë°ì´í„°ì—ì„œ ì¡°ê±´ì— ë§ëŠ” ì‹œë¦¬ì¦ˆì˜ IMDB ë°ì´í„° ìˆ˜ì§‘
    
    Args:
        input_csv_path: TMDB TV ì‹œë¦¬ì¦ˆ CSV íŒŒì¼ ê²½ë¡œ
    """
    print("=" * 90)
    print("ğŸ¬ IMDB ë°ì´í„° í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("=" * 90)
    
    stats["start_time"] = datetime.now()
    t0 = datetime.now()
    
    # 1. ë°ì´í„° ë¡œë“œ ë° í•„í„°ë§
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df = pd.read_csv(input_csv_path)
    print(f"âœ… ì „ì²´ ì‹œë¦¬ì¦ˆ: {len(df):,}ê°œ")
    
    # ì¡°ê±´ í•„í„°ë§: vote_count >= 30 AND imdb_idê°€ ì¡´ì¬
    df_filtered = df[(df['vote_count'] >= 30) & (df['imdb_id'].notna())]
    print(f"âœ… í•„í„°ë§ëœ ì‹œë¦¬ì¦ˆ (vote_count>=30 & imdb_id ì¡´ì¬): {len(df_filtered):,}ê°œ")
    
    if len(df_filtered) == 0:
        print("âš ï¸  ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # IMDB ID ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    imdb_data = df_filtered[['id', 'title', 'imdb_id']].to_dict('records')
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    processed_ids = load_checkpoint()
    if processed_ids:
        print(f"ğŸ“Œ ì´ì „ ì§„í–‰ ìƒí™© ë¡œë“œ: {len(processed_ids):,}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
        imdb_data = [x for x in imdb_data if x['imdb_id'] not in processed_ids]
        print(f"ğŸ“Œ ë‚¨ì€ ì‘ì—…: {len(imdb_data):,}ê°œ")
    
    if len(imdb_data) == 0:
        print("âœ… ëª¨ë“  ë°ì´í„°ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    # 2. í¬ë¡¤ë§ ì‹œì‘
    print(f"\nğŸš€ í¬ë¡¤ë§ ì‹œì‘: {len(imdb_data):,}ê°œ ì‹œë¦¬ì¦ˆ")
    print(f"âš™ï¸  ì†ë„ ì œí•œ: {MAX_CALLS_PER_SECOND}íšŒ/ì´ˆ")
    print(f"â±ï¸  ì˜ˆìƒ ì†Œìš” ì‹œê°„: {len(imdb_data)*2/MAX_CALLS_PER_SECOND/60:.1f}ë¶„")
    
    # ì„¸ì…˜ ì„¤ì •
    connector = aiohttp.TCPConnector(
        limit=10,  # ë™ì‹œ ì—°ê²° ìˆ˜ ì œí•œ
        force_close=True,
        enable_cleanup_closed=True
    )
    
    results = []
    batch_size = 50  # ë°°ì¹˜ í¬ê¸°
    
    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as session:
        for i in range(0, len(imdb_data), batch_size):
            batch = imdb_data[i:i+batch_size]
            
            # ë°°ì¹˜ ì²˜ë¦¬
            batch_results = await asyncio.gather(
                *[scrape_imdb_data(session, item['imdb_id'], item['title']) for item in batch],
                return_exceptions=True
            )
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for r in batch_results:
                if isinstance(r, dict):
                    results.append(r)
                    processed_ids.add(r['imdb_id'])
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ì €ì¥
            if len(results) % 20 == 0:
                df_results = pd.DataFrame(results)
                df_results.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
                save_checkpoint(processed_ids)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            elapsed = (datetime.now() - t0).total_seconds()
            progress = (i + len(batch)) / len(imdb_data) * 100
            rate = stats["total"] / elapsed if elapsed > 0 else 0
            eta = (len(imdb_data) - stats["total"]) / rate / 60 if rate > 0 else 0
            
            print(f"ğŸ“Š ì§„í–‰: {stats['total']:,}/{len(imdb_data):,} ({progress:.1f}%) | "
                  f"ì„±ê³µ: {stats['success']:,} | ì‹¤íŒ¨: {stats['failed']} | "
                  f"ì†ë„: {rate:.2f}/s | ETA: {eta:.1f}ë¶„")
    
    # 3. ìµœì¢… ì €ì¥
    print("\nğŸ’¾ ìµœì¢… ì €ì¥ ì¤‘...")
    df_results = pd.DataFrame(results)
    df_results.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
    
    try:
        df_results.to_parquet(OUTPUT_PARQUET, index=False)
    except:
        print("âš ï¸  Parquet ì €ì¥ ì‹¤íŒ¨ (CSVë§Œ ì €ì¥ë¨)")
    
    # ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
    if Path(CHECKPOINT_FILE).exists():
        Path(CHECKPOINT_FILE).unlink()
    
    # 4. í†µê³„ ì¶œë ¥
    elapsed = (datetime.now() - t0).total_seconds() / 60
    
    print("\n" + "=" * 90)
    print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 90)
    print(f"ğŸ“Œ ì´ ì²˜ë¦¬: {stats['total']:,}ê°œ")
    print(f"ğŸ“Œ ì„±ê³µ: {stats['success']:,}ê°œ ({stats['success']/stats['total']*100:.1f}%)")
    print(f"ğŸ“Œ ì‹¤íŒ¨: {stats['failed']}ê°œ")
    print(f"ğŸ“Œ ì´ ìš”ì²­: {stats['requests']:,}íšŒ")
    print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {elapsed:.1f}ë¶„ ({elapsed/60:.2f}ì‹œê°„)")
    print(f"ğŸ“Š í‰ê·  ì†ë„: {stats['success']/elapsed:.1f}ê°œ/ë¶„")
    print("=" * 90)
    
    # ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ
    print("\nğŸ“Š ìƒ˜í”Œ ë°ì´í„°:")
    sample = df_results[df_results['imdb_rating'].notna()].head(3)
    for idx, row in sample.iterrows():
        print(f"\nì œëª©: {row['series_title']}")
        print(f"  IMDB ID: {row['imdb_id']}")
        print(f"  IMDB í‰ì : {row['imdb_rating']}/10 ({row['imdb_rating_count']:,}í‘œ)")
        print(f"  Meta Score: {row['meta_score']}")
        if row['reviews_json']:
            reviews = json.loads(row['reviews_json'])
            print(f"  ë¦¬ë·° ìˆ˜: {len(reviews)}ê°œ")
    
    print(f"\nâœ… ê²°ê³¼ íŒŒì¼: {OUTPUT_CSV}")

# ==========================================================
# ì‹¤í–‰
# ==========================================================
if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    input_file = "tv_series_2013_0101_0215_FULL.csv"
    
    if not Path(input_file).exists():
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        print("ğŸ“ TMDB ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
    else:
        asyncio.run(main(input_file))
