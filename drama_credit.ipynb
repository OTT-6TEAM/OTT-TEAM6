{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMDB APIë¡œ ë°ì´í„° ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TMDB ë“œë¼ë§ˆ í¬ë ˆë”§ ë°ì´í„° ì¶”ê°€ ìˆ˜ì§‘\n",
    "\n",
    "1. ë²”ìœ„:Â TMDB ë“œë¼ë§ˆ í¬ë ˆë”§(í”„ë¡œë“€ì„œ/ë°°ìš°/ì‘ê°€) ë°ì´í„°\n",
    "2. ë°©ì‹: ê¸°ì¡´ì— ìˆ˜ì§‘í•œ ë“œë¼ë§ˆ íŒŒì¼ ì† ê° í”„ë¡œë“€ì„œ/ë°°ìš°/ì‘ê°€ì— ëŒ€í•œ ì„¸ë¶€ ë°ì´í„° ìˆ˜ì§‘\n",
    "3. ëª©ì : í˜„ì¬ ë°ì´í„°ëŠ” íŒŒì‹±ì´ í•„ìš”í•œë°, ê° ì»¬ëŸ¼ë³„ ê°’ë¼ë¦¬ ë§¤ì¹­ì„ í™•ì‹¤íˆ í•˜ê¸° ìœ„í•´ ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œë¶€í„° íŒŒì‹±í•˜ì—¬ ì»¬ëŸ¼ì„ ë¶„ë¦¬í•˜ê³ ì í•¨.\n",
    "4. ì¡°ê±´:Â ëˆ„ë½ëœ í˜ì´ì§€ê°€ 0%ì—¬ì•¼ í•¨, TMDB API ì´ˆë‹¹ ìš”ì²­ ìˆ˜(40 req/s) ì œí•œ ëŒ€ì‘, 500í˜ì´ì§€ ì œí•œ ëŒ€ì‘, ì¤‘ë‹¨/ì¬ê°œ ì§€ì›, ìˆ˜ì§‘ íŒŒì¼ì€ ê° ë©€í‹°ê°’ì„ íŒŒì‹±í•˜ì—¬ ì»¬ëŸ¼ ë¶„ë¦¬í•˜ê³ , í”„ë¡œë“€ì„œ/ë°°ìš°/ì‘ê°€ë³„ í…Œì´ë¸” ë¶„ë¦¬í•˜ì—¬ 3ê°œì˜ Parquetë¡œ ì €ì¥, Jupyterì—ì„œ ê·¸ëŒ€ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ\n",
    "5. í•„ìˆ˜ ì»¬ëŸ¼ (23ê°œ):\n",
    "- executive_producer: crew_member_id, name, gender, popularity, department, job, series_id, profile_path\n",
    "- writer: crew_member_id, name, gender, popularity, department, job, series_id, profile_path\n",
    "- cast: cast_id, name, gender, popularity, character, series_id, profile_path\n",
    "\n",
    "6. ìˆ˜ì§‘ ì‹œ ì£¼ì˜ì‚¬í•­\n",
    "- crewì—ëŠ” í”„ë¡œë“€ì„œ/ì‘ê°€ ì™¸ ë‹¤ì–‘í•œ ì§êµ°(department/job)ì´ í¬í•¨ë˜ë¯€ë¡œ, í•„í„° ì •í™•íˆ í•´ì•¼ í•¨\n",
    "- ê°ë…: crew ì•ˆì—ì„œ get(\"job\") == \"Executive Producer\", ê³µë™ ê°ë… ìˆì„ ìˆ˜ ìˆìŒ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„)\n",
    "- ì‘ê°€: ì›ì•ˆ, ê°ë³¸, ì§‘í•„ì ëª¨ë‘ ì»¤ë²„í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì¡°ê±´ í•„ìš”, ê³µë™ ì‘ê°€ ìˆì„ ìˆ˜ ìˆìŒ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„)\n",
    "writers = [c for c in crew_list if c.get(\"job\") in (\"Writer\", \"Screenplay\", \"Story\", \"Creator\", \"Novel\", \"Comic Book\") or c.get(\"department\") == \"Writing\"]\n",
    "- ì£¼ì—° ë°°ìš° top 5: cast ë°°ì—´ì„ order í•„ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ (ë°˜ë“œì‹œ ì£¼ì—° ë°°ìš°ë¶€í„° 5ëª… ì´ë¦„ìœ¼ë¡œ ìˆ˜ì§‘, ì ˆëŒ€ popularity ê¸°ì¤€ ì“°ë©´ ì•ˆ ë¨)\n",
    "filtered = [c for c in sorted_cast if c.get(\"character\")]\n",
    "top_5 = filtered[:5]\n",
    "\n",
    "7. ê³µì‹ë¬¸ì„œ:Â https://developer.themoviedb.org/reference/tv-series-credits\n",
    "8. ìˆ˜ì§‘ ë°©ì‹: ë©€í‹° ìŠ¤ë ˆë“œ ë°©ì‹\n",
    "9. ìˆ˜ì§‘ ê²°ê³¼: ì´ 3,581ê°œ ìˆ˜ì§‘ ì™„ë£Œ, ì•½ 1.5ë¶„ ì†Œìš”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì˜í™” í¬ë ˆë”§ ìˆ˜ì§‘ ì½”ë“œ (íŒŒì‹± í›„ ì •ê·œí™”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸš€ TMDB DRAMA CREDITS COLLECTOR V3 (TV Series)\n",
      "================================================================================\n",
      "ğŸ“‚ ë“œë¼ë§ˆ ì‹œë¦¬ì¦ˆ ìˆ˜: 3,581ê°œ (unique TMDB TV id)\n",
      "âœ… ì´ë¯¸ ì™„ë£Œëœ ì‹œë¦¬ì¦ˆ: 0ê°œ\n",
      "âš ï¸ ì´ì „ ì‹¤íŒ¨ ì‹œë¦¬ì¦ˆ: 0ê°œ\n",
      "â³ ì´ë²ˆì— ì²˜ë¦¬í•  ì‹œë¦¬ì¦ˆ: 3,581ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë“œë¼ë§ˆ í¬ë ˆë”§ ìˆ˜ì§‘:  14%|â–ˆâ–        | 486/3581 [00:12<01:38, 31.39series/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (processed=500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë“œë¼ë§ˆ í¬ë ˆë”§ ìˆ˜ì§‘:  28%|â–ˆâ–ˆâ–Š       | 999/3581 [00:25<01:00, 42.34series/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (processed=1,000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë“œë¼ë§ˆ í¬ë ˆë”§ ìˆ˜ì§‘:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1484/3581 [00:37<01:10, 29.69series/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (processed=1,500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë“œë¼ë§ˆ í¬ë ˆë”§ ìˆ˜ì§‘:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1999/3581 [00:50<00:37, 41.88series/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (processed=2,000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë“œë¼ë§ˆ í¬ë ˆë”§ ìˆ˜ì§‘:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2510/3581 [01:03<00:20, 53.39series/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (processed=2,500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë“œë¼ë§ˆ í¬ë ˆë”§ ìˆ˜ì§‘:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2998/3581 [01:15<00:13, 44.74series/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (processed=3,000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë“œë¼ë§ˆ í¬ë ˆë”§ ìˆ˜ì§‘:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3510/3581 [01:28<00:01, 49.87series/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (processed=3,500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë“œë¼ë§ˆ í¬ë ˆë”§ ìˆ˜ì§‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3581/3581 [01:30<00:00, 39.49series/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… í¬ë ˆë”§ ìˆ˜ì§‘ ë‹¨ê³„ ì™„ë£Œ\n",
      "\n",
      "ğŸ“¦ ì„ì‹œ CSV â†’ Parquet ë³€í™˜\n",
      "  â–¶ Producer Parquet ì €ì¥: files/tv_dramas_producer.parquet (7,871 rows)\n",
      "  â–¶ Writer Parquet ì €ì¥: files/tv_dramas_writer.parquet (3,676 rows)\n",
      "  â–¶ Cast Parquet ì €ì¥: files/tv_dramas_cast.parquet (16,692 rows)\n",
      "\n",
      "================================================================================\n",
      "ğŸ‰ TMDB DRAMA CREDITS ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (V3)\n",
      "â± ì´ ì†Œìš”ì‹œê°„: 1.5ë¶„ (0.03ì‹œê°„)\n",
      "âœ… ì™„ë£Œ ì‹œë¦¬ì¦ˆ: 3,581ê°œ\n",
      "âš ï¸ ì‹¤íŒ¨ ì‹œë¦¬ì¦ˆ: 0ê°œ (íŒŒì¼: files/tv_dramas_credits_failed_ids.json)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# TMDB DRAMA CREDITS COLLECTOR V3 (TV Series, Producer/Writer/Cast)\n",
    "# - ê¸°ì¤€: ê¸°ì¡´ ì˜í™” í¬ë ˆë”§ ìˆ˜ì§‘ ì½”ë“œ ìŠ¤íƒ€ì¼ (ë©€í‹°ìŠ¤ë ˆë“œ + requests)\n",
    "# - ëŒ€ìƒ: ë¯¸ë¦¬ í•„í„°ëœ \"ë“œë¼ë§ˆ ì‹œë¦¬ì¦ˆ\" íŒŒì¼ ì† TMDB TV id ëª©ë¡\n",
    "# - ê¸°ëŠ¥:\n",
    "#   * /tv/{series_id}/credits ê¸°ë°˜ producer / writer / top 5 cast \"í–‰ ë¶„ë¦¬\" ìˆ˜ì§‘\n",
    "#   * ì´ˆë‹¹ 40req/s ì´í•˜ Rate Limiter (thread-safe)\n",
    "#   * ì¤‘ë‹¨/ì¬ê°œ ì§€ì› (checkpoint.json + ì„ì‹œ CSV ëˆ„ì )\n",
    "#   * ìµœì¢… ì¶œë ¥: 3ê°œì˜ Parquet\n",
    "#       - tv_dramas_producer.parquet\n",
    "#       - tv_dramas_writer.parquet\n",
    "#       - tv_dramas_cast.parquet\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================================\n",
    "# ì„¤ì •\n",
    "# ==========================================================\n",
    "\n",
    "# (1) ì…ë ¥: ë“œë¼ë§ˆ ì‹œë¦¬ì¦ˆ íŒŒì¼ & TMDB id ì»¬ëŸ¼ëª…\n",
    "INPUT_PARQUET = \"dramas_merged_final.parquet\"\n",
    "ID_COLUMN = \"id\"   # TMDB TV series id ì»¬ëŸ¼ëª…\n",
    "\n",
    "# (2) ì¶œë ¥ ë””ë ‰í† ë¦¬\n",
    "OUTPUT_DIR = Path(\"files\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# (3) ì„ì‹œ CSV & ìµœì¢… Parquet ê²½ë¡œ\n",
    "PRODUCER_TEMP_CSV = OUTPUT_DIR / \"tv_dramas_producer_temp.csv\"\n",
    "WRITER_TEMP_CSV   = OUTPUT_DIR / \"tv_dramas_writer_temp.csv\"\n",
    "CAST_TEMP_CSV     = OUTPUT_DIR / \"tv_dramas_cast_temp.csv\"\n",
    "\n",
    "PRODUCER_PARQUET  = OUTPUT_DIR / \"tv_dramas_producer.parquet\"\n",
    "WRITER_PARQUET    = OUTPUT_DIR / \"tv_dramas_writer.parquet\"\n",
    "CAST_PARQUET      = OUTPUT_DIR / \"tv_dramas_cast.parquet\"\n",
    "\n",
    "# (4) ì²´í¬í¬ì¸íŠ¸ & ì‹¤íŒ¨ ID ê¸°ë¡\n",
    "CHECKPOINT_PATH   = OUTPUT_DIR / \"tv_dramas_credits_checkpoint.json\"\n",
    "FAILED_IDS_PATH   = OUTPUT_DIR / \"tv_dramas_credits_failed_ids.json\"\n",
    "\n",
    "# (5) TMDB API\n",
    "HEADERS = {\"accept\": \"application/json\"}\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"TMDB_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"TMDB_API_KEY ê°€ .env ì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# (6) Rate Limit & Thread ì„¤ì •\n",
    "MAX_CALLS_PER_SECOND = 40   # TMDB ë¬¸ì„œ ê¸°ì¤€ ì—¬ìœ  ìˆê²Œ ì‚¬ìš©\n",
    "MAX_WORKERS = 30            # ThreadPoolExecutor worker ìˆ˜\n",
    "FLUSH_EVERY = 500           # Nê°œ ì‹œë¦¬ì¦ˆ ì²˜ë¦¬ë§ˆë‹¤ ì„ì‹œ CSV + ì²´í¬í¬ì¸íŠ¸ flush\n",
    "MAX_RETRIES = 3             # ê°œë³„ ìš”ì²­ ì¬ì‹œë„ íšŸìˆ˜\n",
    "\n",
    "WRITER_JOBS = {\n",
    "    \"Writer\", \"Screenplay\", \"Story\", \"Creator\",\n",
    "    \"Novel\", \"Comic Book\", \"Adaptation\", \"Screenstory\"\n",
    "}\n",
    "WRITER_JOBS_LOWER = {j.lower() for j in WRITER_JOBS}\n",
    "\n",
    "# ==========================================================\n",
    "# Rate Limiter (thread-safe)\n",
    "# ==========================================================\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"ì´ˆë‹¹ max_calls ì´í•˜ë¡œ ìš”ì²­ì„ ì œí•œí•˜ëŠ” Rate Limiter (thread-safe)\"\"\"\n",
    "    def __init__(self, max_calls: int, period: float = 1.0):\n",
    "        self.max_calls = max_calls\n",
    "        self.period = period\n",
    "        self.calls = deque()\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def acquire(self):\n",
    "        with self.lock:\n",
    "            now = time.time()\n",
    "\n",
    "            # ê¸°ê°„ ì§€ë‚œ í˜¸ì¶œ ì œê±°\n",
    "            while self.calls and now - self.calls[0] > self.period:\n",
    "                self.calls.popleft()\n",
    "\n",
    "            # ì´ˆê³¼ ì‹œ ëŒ€ê¸°\n",
    "            if len(self.calls) >= self.max_calls:\n",
    "                sleep_for = self.period - (now - self.calls[0]) + 0.01\n",
    "                if sleep_for > 0:\n",
    "                    time.sleep(sleep_for)\n",
    "\n",
    "                now = time.time()\n",
    "                while self.calls and now - self.calls[0] > self.period:\n",
    "                    self.calls.popleft()\n",
    "\n",
    "            self.calls.append(now)\n",
    "\n",
    "rate_limiter = RateLimiter(MAX_CALLS_PER_SECOND)\n",
    "\n",
    "# ==========================================================\n",
    "# HTTP Session (thread-local)\n",
    "# ==========================================================\n",
    "\n",
    "thread_local = threading.local()\n",
    "\n",
    "def create_session() -> requests.Session:\n",
    "    \"\"\"ì¬ì‹œë„ ë¡œì§ê³¼ ì—°ê²° í’€ì´ ìˆëŠ” ì„¸ì…˜ ìƒì„±\"\"\"\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.3,\n",
    "        status_forcelist=[500, 502, 503, 504],\n",
    "    )\n",
    "    adapter = HTTPAdapter(\n",
    "        max_retries=retry,\n",
    "        pool_connections=50,\n",
    "        pool_maxsize=50,\n",
    "    )\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def get_session() -> requests.Session:\n",
    "    \"\"\"ê° ì“°ë ˆë“œë³„ë¡œ ë…ë¦½ì ì¸ Session ì‚¬ìš©\"\"\"\n",
    "    if not hasattr(thread_local, \"session\"):\n",
    "        thread_local.session = create_session()\n",
    "    return thread_local.session\n",
    "\n",
    "# ==========================================================\n",
    "# ì²´í¬í¬ì¸íŠ¸ ìœ í‹¸\n",
    "# ==========================================================\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"ì´ë¯¸ ì²˜ë¦¬í•œ ì‹œë¦¬ì¦ˆ ID ì •ë³´ ë¡œë“œ\"\"\"\n",
    "    if CHECKPOINT_PATH.exists():\n",
    "        try:\n",
    "            data = json.loads(CHECKPOINT_PATH.read_text(encoding=\"utf-8\"))\n",
    "            done_ids = set(data.get(\"done_ids\", []))\n",
    "            failed_ids = set(data.get(\"failed_ids\", []))\n",
    "            return done_ids, failed_ids\n",
    "        except Exception:\n",
    "            pass\n",
    "    return set(), set()\n",
    "\n",
    "def save_checkpoint(done_ids, failed_ids):\n",
    "    \"\"\"ì²´í¬í¬ì¸íŠ¸ ì €ì¥\"\"\"\n",
    "    data = {\n",
    "        \"done_ids\": sorted(list(done_ids)),\n",
    "        \"failed_ids\": sorted(list(failed_ids)),\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    tmp = CHECKPOINT_PATH.with_suffix(\".tmp\")\n",
    "    tmp.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    tmp.replace(CHECKPOINT_PATH)\n",
    "\n",
    "# ==========================================================\n",
    "# ì„ì‹œ CSV append ìœ í‹¸ (ë‹¨ì¼ ìŠ¤ë ˆë“œì—ì„œë§Œ í˜¸ì¶œ)\n",
    "# ==========================================================\n",
    "\n",
    "def append_temp_csv(df: pd.DataFrame, path: Path):\n",
    "    \"\"\"\n",
    "    ì„ì‹œ CSVì— DataFrameì„ append (headerëŠ” ì²˜ìŒ í•œ ë²ˆë§Œ)\n",
    "    - main threadì—ì„œë§Œ í˜¸ì¶œ â†’ race condition ì—†ìŒ\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    header = not path.exists()\n",
    "    df.to_csv(path, mode=\"a\", header=header, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# ==========================================================\n",
    "# TV Series Credits Fetcher (í•µì‹¬)\n",
    "# ==========================================================\n",
    "\n",
    "def get_tv_credit(series_id: int):\n",
    "    \"\"\"\n",
    "    TV ì‹œë¦¬ì¦ˆì˜ cast / writer / producer í¬ë ˆë”§ ìˆ˜ì§‘ (í–‰ ë‹¨ìœ„)\n",
    "    - Producer: job == \"Executive Producer\" (ì†Œë¬¸ì ë¹„êµ)\n",
    "    - Writer: job âˆˆ WRITER_JOBS (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ) ë˜ëŠ” department == \"Writing\"\n",
    "    - Cast: order ê¸°ì¤€ ì •ë ¬ í›„, characterê°€ ì¡´ì¬í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì€ ë°°ìš°ë§Œ í•„í„° â†’ ê·¸ ì¤‘ Top 5\n",
    "    \"\"\"\n",
    "    session = get_session()\n",
    "    url = f\"https://api.themoviedb.org/3/tv/{series_id}/credits\"\n",
    "    params = {\"api_key\": API_KEY, \"language\": \"en-US\"}\n",
    "\n",
    "    data = None\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            rate_limiter.acquire()\n",
    "            resp = session.get(url, params=params, headers=HEADERS, timeout=10)\n",
    "\n",
    "            # 404: í¬ë ˆë”§ ì—†ìŒ â†’ ê·¸ëƒ¥ None\n",
    "            if resp.status_code == 404:\n",
    "                return None\n",
    "\n",
    "            # 429: rate limit â†’ Retry-After ë˜ëŠ” ì§€ìˆ˜ backoff\n",
    "            if resp.status_code == 429:\n",
    "                retry_after = resp.headers.get(\"Retry-After\", None)\n",
    "                try:\n",
    "                    sleep_for = float(retry_after) if retry_after is not None else 2 ** attempt\n",
    "                except (ValueError, TypeError):\n",
    "                    sleep_for = 2 ** attempt\n",
    "                print(f\"[429] series_id={series_id}, attempt={attempt+1}, sleep={sleep_for}s\")\n",
    "                time.sleep(sleep_for)\n",
    "                continue\n",
    "\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            break  # ì„±ê³µ â†’ ë£¨í”„ íƒˆì¶œ\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # ë§ˆì§€ë§‰ ì¬ì‹œë„ì—ì„œë„ ì‹¤íŒ¨í•˜ë©´ í¬ê¸°\n",
    "            if attempt == MAX_RETRIES - 1:\n",
    "                print(f\"[ERROR] series_id={series_id} ìµœì¢… ì‹¤íŒ¨: {e}\")\n",
    "                return None\n",
    "            sleep_for = 2 ** attempt\n",
    "            print(f\"[WARN] series_id={series_id}, attempt={attempt+1}, error={e}, retry in {sleep_for}s\")\n",
    "            time.sleep(sleep_for)\n",
    "\n",
    "    # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    cast = data.get(\"cast\", []) or []\n",
    "    crew = data.get(\"crew\", []) or []\n",
    "\n",
    "    # =============================\n",
    "    # 1) CAST ì²˜ë¦¬ (Top 5 by order, character í•„í„°)\n",
    "    # =============================\n",
    "    if cast:\n",
    "        cast_df = pd.DataFrame(cast)\n",
    "\n",
    "        # order ê¸°ì¤€ ì •ë ¬\n",
    "        if \"order\" in cast_df.columns:\n",
    "            cast_df = cast_df.sort_values(by=\"order\")\n",
    "\n",
    "        # ğŸ”´ ìš”êµ¬ì‚¬í•­: characterê°€ ì¡´ì¬í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì€ ë°°ìš°ë§Œ\n",
    "        if \"character\" in cast_df.columns:\n",
    "            cast_df = cast_df[\n",
    "                cast_df[\"character\"].notna()\n",
    "                & (cast_df[\"character\"].astype(str).str.strip() != \"\")\n",
    "            ]\n",
    "        else:\n",
    "            # character ì»¬ëŸ¼ ìì²´ê°€ ì—†ìœ¼ë©´ ì£¼ì—° ì •ì˜ê°€ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ ë¹„ì›Œë‘ \n",
    "            cast_df = cast_df.iloc[0:0]\n",
    "\n",
    "        # í•„í„°ë§ í›„ ìƒìœ„ 5ëª…\n",
    "        cast_df = cast_df.head(5)\n",
    "\n",
    "        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "        cast_cols_select = [\"id\", \"name\", \"gender\", \"popularity\",\n",
    "                            \"profile_path\", \"character\"]\n",
    "        cast_existing = [c for c in cast_cols_select if c in cast_df.columns]\n",
    "        cast_df = cast_df[cast_existing].copy()\n",
    "        cast_df = cast_df.rename(columns={\"id\": \"cast_id\"})\n",
    "\n",
    "        # character ì»¬ëŸ¼ ë³´ì •\n",
    "        if \"character\" not in cast_df.columns:\n",
    "            cast_df[\"character\"] = \"\"\n",
    "        else:\n",
    "            cast_df[\"character\"] = cast_df[\"character\"].fillna(\"\").astype(str)\n",
    "\n",
    "        cast_df[\"series_id\"] = int(series_id)\n",
    "    else:\n",
    "        cast_df = pd.DataFrame()\n",
    "\n",
    "    # =============================\n",
    "    # 2) CREW â†’ WRITER / PRODUCER\n",
    "    # =============================\n",
    "    if crew:\n",
    "        crew_df = pd.DataFrame(crew)\n",
    "\n",
    "        crew_cols_select = [\n",
    "            \"id\", \"name\", \"gender\", \"popularity\",\n",
    "            \"profile_path\", \"department\", \"job\"\n",
    "        ]\n",
    "        existing = [c for c in crew_cols_select if c in crew_df.columns]\n",
    "        crew_df = crew_df[existing].copy()\n",
    "        crew_df = crew_df.rename(columns={\"id\": \"crew_member_id\"})\n",
    "        crew_df[\"series_id\"] = int(series_id)\n",
    "\n",
    "        # 2-1) PRODUCER (Executive Producerë§Œ, ì†Œë¬¸ì ë¹„êµ)\n",
    "        if \"job\" in crew_df.columns:\n",
    "            producer_df = crew_df[\n",
    "                crew_df[\"job\"].fillna(\"\").str.lower() == \"executive producer\"\n",
    "            ].copy()\n",
    "        else:\n",
    "            producer_df = pd.DataFrame()\n",
    "\n",
    "        # 2-2) WRITER (job: WRITER_JOBS_LOWER, department: writing)\n",
    "        if \"job\" in crew_df.columns and \"department\" in crew_df.columns:\n",
    "            job_lower = crew_df[\"job\"].fillna(\"\").str.lower()\n",
    "            dept_lower = crew_df[\"department\"].fillna(\"\").str.lower()\n",
    "            writer_mask = job_lower.isin(WRITER_JOBS_LOWER) | (dept_lower == \"writing\")\n",
    "            writer_df = crew_df[writer_mask].copy()\n",
    "        elif \"job\" in crew_df.columns:\n",
    "            job_lower = crew_df[\"job\"].fillna(\"\").str.lower()\n",
    "            writer_mask = job_lower.isin(WRITER_JOBS_LOWER)\n",
    "            writer_df = crew_df[writer_mask].copy()\n",
    "        else:\n",
    "            writer_df = pd.DataFrame()\n",
    "    else:\n",
    "        producer_df = pd.DataFrame()\n",
    "        writer_df = pd.DataFrame()\n",
    "\n",
    "    # =============================\n",
    "    # 3) ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬ + ëˆ„ë½ ì»¬ëŸ¼ ì±„ìš°ê¸°\n",
    "    # =============================\n",
    "    producer_cols = [\n",
    "        \"crew_member_id\", \"name\", \"gender\", \"popularity\",\n",
    "        \"department\", \"job\", \"series_id\", \"profile_path\"\n",
    "    ]\n",
    "    writer_cols   = [\n",
    "        \"crew_member_id\", \"name\", \"gender\", \"popularity\",\n",
    "        \"department\", \"job\", \"series_id\", \"profile_path\"\n",
    "    ]\n",
    "    cast_cols     = [\n",
    "        \"cast_id\", \"name\", \"gender\", \"popularity\",\n",
    "        \"character\", \"series_id\", \"profile_path\"\n",
    "    ]\n",
    "\n",
    "    # Producer\n",
    "    for col in producer_cols:\n",
    "        if col not in producer_df.columns:\n",
    "            producer_df[col] = pd.NA\n",
    "    producer_df = producer_df[producer_cols]\n",
    "\n",
    "    # Writer\n",
    "    for col in writer_cols:\n",
    "        if col not in writer_df.columns:\n",
    "            writer_df[col] = pd.NA\n",
    "    writer_df = writer_df[writer_cols]\n",
    "\n",
    "    # Cast\n",
    "    for col in cast_cols:\n",
    "        if col not in cast_df.columns:\n",
    "            cast_df[col] = pd.NA\n",
    "    cast_df = cast_df[cast_cols]\n",
    "\n",
    "    return {\n",
    "        \"producer\": producer_df,\n",
    "        \"writer\":   writer_df,\n",
    "        \"cast\":     cast_df,\n",
    "    }\n",
    "\n",
    "# ==========================================================\n",
    "# ë©”ì¸ íŒŒì´í”„ë¼ì¸\n",
    "# ==========================================================\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸš€ TMDB DRAMA CREDITS COLLECTOR V3 (TV Series)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # ----- ì…ë ¥ íŒŒì¼ ë¡œë“œ -----\n",
    "    ipath = Path(INPUT_PARQUET)\n",
    "    if not ipath.exists():\n",
    "        raise FileNotFoundError(f\"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ipath.resolve()}\")\n",
    "\n",
    "    dramas = pd.read_parquet(ipath)\n",
    "    if ID_COLUMN not in dramas.columns:\n",
    "        raise KeyError(f\"ì…ë ¥ íŒŒì¼ì— '{ID_COLUMN}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    ids = (\n",
    "        dramas[ID_COLUMN]\n",
    "        .dropna()\n",
    "        .astype(int)\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "    ids = sorted(ids)\n",
    "    print(f\"ğŸ“‚ ë“œë¼ë§ˆ ì‹œë¦¬ì¦ˆ ìˆ˜: {len(ids):,}ê°œ (unique TMDB TV id)\")\n",
    "\n",
    "    # ----- ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ -----\n",
    "    done_ids, failed_ids = load_checkpoint()\n",
    "    print(f\"âœ… ì´ë¯¸ ì™„ë£Œëœ ì‹œë¦¬ì¦ˆ: {len(done_ids):,}ê°œ\")\n",
    "    print(f\"âš ï¸ ì´ì „ ì‹¤íŒ¨ ì‹œë¦¬ì¦ˆ: {len(failed_ids):,}ê°œ\")\n",
    "\n",
    "    remaining_ids = [i for i in ids if i not in done_ids and i not in failed_ids]\n",
    "    print(f\"â³ ì´ë²ˆì— ì²˜ë¦¬í•  ì‹œë¦¬ì¦ˆ: {len(remaining_ids):,}ê°œ\")\n",
    "\n",
    "    # ----- í¬ë ˆë”§ ìˆ˜ì§‘ -----\n",
    "    producer_buf = []\n",
    "    writer_buf = []\n",
    "    cast_buf = []\n",
    "    processed = 0\n",
    "\n",
    "    if remaining_ids:\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            futures = {\n",
    "                executor.submit(get_tv_credit, series_id): series_id\n",
    "                for series_id in remaining_ids\n",
    "            }\n",
    "\n",
    "            for future in tqdm(\n",
    "                as_completed(futures),\n",
    "                total=len(futures),\n",
    "                desc=\"ë“œë¼ë§ˆ í¬ë ˆë”§ ìˆ˜ì§‘\",\n",
    "                unit=\"series\",\n",
    "            ):\n",
    "                series_id = futures[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"[EXCEPTION] ì‹œë¦¬ì¦ˆ ID {series_id} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {e}\")\n",
    "                    failed_ids.add(series_id)\n",
    "                    continue\n",
    "\n",
    "                if result is None:\n",
    "                    failed_ids.add(series_id)\n",
    "                else:\n",
    "                    if not result[\"producer\"].empty:\n",
    "                        producer_buf.append(result[\"producer\"])\n",
    "                    if not result[\"writer\"].empty:\n",
    "                        writer_buf.append(result[\"writer\"])\n",
    "                    if not result[\"cast\"].empty:\n",
    "                        cast_buf.append(result[\"cast\"])\n",
    "                    done_ids.add(series_id)\n",
    "\n",
    "                processed += 1\n",
    "\n",
    "                # ì£¼ê¸°ì ìœ¼ë¡œ flush + checkpoint\n",
    "                if processed % FLUSH_EVERY == 0:\n",
    "                    if producer_buf:\n",
    "                        append_temp_csv(\n",
    "                            pd.concat(producer_buf, ignore_index=True),\n",
    "                            PRODUCER_TEMP_CSV,\n",
    "                        )\n",
    "                        producer_buf = []\n",
    "                    if writer_buf:\n",
    "                        append_temp_csv(\n",
    "                            pd.concat(writer_buf, ignore_index=True),\n",
    "                            WRITER_TEMP_CSV,\n",
    "                        )\n",
    "                        writer_buf = []\n",
    "                    if cast_buf:\n",
    "                        append_temp_csv(\n",
    "                            pd.concat(cast_buf, ignore_index=True),\n",
    "                            CAST_TEMP_CSV,\n",
    "                        )\n",
    "                        cast_buf = []\n",
    "\n",
    "                    save_checkpoint(done_ids, failed_ids)\n",
    "                    print(f\"\\nğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (processed={processed:,})\")\n",
    "\n",
    "        # ë§ˆì§€ë§‰ ë‚¨ì€ ë²„í¼ flush\n",
    "        if producer_buf:\n",
    "            append_temp_csv(pd.concat(producer_buf, ignore_index=True), PRODUCER_TEMP_CSV)\n",
    "        if writer_buf:\n",
    "            append_temp_csv(pd.concat(writer_buf, ignore_index=True), WRITER_TEMP_CSV)\n",
    "        if cast_buf:\n",
    "            append_temp_csv(pd.concat(cast_buf, ignore_index=True), CAST_TEMP_CSV)\n",
    "\n",
    "        save_checkpoint(done_ids, failed_ids)\n",
    "        FAILED_IDS_PATH.write_text(\n",
    "            json.dumps(sorted(list(failed_ids)), ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        print(\"\\nâœ… í¬ë ˆë”§ ìˆ˜ì§‘ ë‹¨ê³„ ì™„ë£Œ\")\n",
    "    else:\n",
    "        print(\"\\nëª¨ë“  ì‹œë¦¬ì¦ˆê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë°”ë¡œ Parquet ë³€í™˜ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.\")\n",
    "\n",
    "    # ======================================================\n",
    "    # ì„ì‹œ CSV â†’ Parquet ë³€í™˜\n",
    "    # ======================================================\n",
    "    print(\"\\nğŸ“¦ ì„ì‹œ CSV â†’ Parquet ë³€í™˜\")\n",
    "\n",
    "    # Producer\n",
    "    if PRODUCER_TEMP_CSV.exists():\n",
    "        df_prod = pd.read_csv(PRODUCER_TEMP_CSV)\n",
    "        prod_cols = [\n",
    "            \"crew_member_id\", \"name\", \"gender\", \"popularity\",\n",
    "            \"department\", \"job\", \"series_id\", \"profile_path\",\n",
    "        ]\n",
    "        for c in prod_cols:\n",
    "            if c not in df_prod.columns:\n",
    "                df_prod[c] = pd.NA\n",
    "        df_prod = df_prod[prod_cols]\n",
    "        df_prod.to_parquet(PRODUCER_PARQUET, index=False)\n",
    "        print(f\"  â–¶ Producer Parquet ì €ì¥: {PRODUCER_PARQUET} ({len(df_prod):,} rows)\")\n",
    "    else:\n",
    "        print(\"  âš ï¸ Producer ì„ì‹œ CSVê°€ ì—†ìŠµë‹ˆë‹¤ (ë°ì´í„° ì—†ìŒ).\")\n",
    "\n",
    "    # Writer\n",
    "    if WRITER_TEMP_CSV.exists():\n",
    "        df_writer = pd.read_csv(WRITER_TEMP_CSV)\n",
    "        writer_cols = [\n",
    "            \"crew_member_id\", \"name\", \"gender\", \"popularity\",\n",
    "            \"department\", \"job\", \"series_id\", \"profile_path\",\n",
    "        ]\n",
    "        for c in writer_cols:\n",
    "            if c not in df_writer.columns:\n",
    "                df_writer[c] = pd.NA\n",
    "        df_writer = df_writer[writer_cols]\n",
    "        df_writer.to_parquet(WRITER_PARQUET, index=False)\n",
    "        print(f\"  â–¶ Writer Parquet ì €ì¥: {WRITER_PARQUET} ({len(df_writer):,} rows)\")\n",
    "    else:\n",
    "        print(\"  âš ï¸ Writer ì„ì‹œ CSVê°€ ì—†ìŠµë‹ˆë‹¤ (ë°ì´í„° ì—†ìŒ).\")\n",
    "\n",
    "    # Cast\n",
    "    if CAST_TEMP_CSV.exists():\n",
    "        df_cast = pd.read_csv(CAST_TEMP_CSV)\n",
    "        cast_cols = [\n",
    "            \"cast_id\", \"name\", \"gender\", \"popularity\",\n",
    "            \"character\", \"series_id\", \"profile_path\",\n",
    "        ]\n",
    "        for c in cast_cols:\n",
    "            if c not in df_cast.columns:\n",
    "                df_cast[c] = pd.NA\n",
    "        df_cast = df_cast[cast_cols]\n",
    "        df_cast.to_parquet(CAST_PARQUET, index=False)\n",
    "        print(f\"  â–¶ Cast Parquet ì €ì¥: {CAST_PARQUET} ({len(df_cast):,} rows)\")\n",
    "    else:\n",
    "        print(\"  âš ï¸ Cast ì„ì‹œ CSVê°€ ì—†ìŠµë‹ˆë‹¤ (ë°ì´í„° ì—†ìŒ).\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ‰ TMDB DRAMA CREDITS ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (V3)\")\n",
    "    print(f\"â± ì´ ì†Œìš”ì‹œê°„: {elapsed/60:.1f}ë¶„ ({elapsed/3600:.2f}ì‹œê°„)\")\n",
    "    print(f\"âœ… ì™„ë£Œ ì‹œë¦¬ì¦ˆ: {len(done_ids):,}ê°œ\")\n",
    "    print(f\"âš ï¸ ì‹¤íŒ¨ ì‹œë¦¬ì¦ˆ: {len(failed_ids):,}ê°œ (íŒŒì¼: {FAILED_IDS_PATH})\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_dramas_cast = pd.read_parquet('tv_dramas_cast.parquet')\n",
    "tv_dramas_writer = pd.read_parquet('tv_dramas_writer.parquet')\n",
    "tv_dramas_producer = pd.read_parquet('tv_dramas_producer.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cast_id</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>popularity</th>\n",
       "      <th>character</th>\n",
       "      <th>series_id</th>\n",
       "      <th>profile_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12795</td>\n",
       "      <td>Nikolaj Coster-Waldau</td>\n",
       "      <td>2</td>\n",
       "      <td>2.9643</td>\n",
       "      <td>John Amsterdam</td>\n",
       "      <td>1144</td>\n",
       "      <td>/6w2SgB20qzs2R5MQIAckINLhfoP.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20803</td>\n",
       "      <td>Zuleikha Robinson</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4830</td>\n",
       "      <td>Eva Marquez</td>\n",
       "      <td>1144</td>\n",
       "      <td>/838l1DBT6LqxI9Lw5qXrDpIqEzW.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>196179</td>\n",
       "      <td>Stephen McKinley Henderson</td>\n",
       "      <td>2</td>\n",
       "      <td>1.1218</td>\n",
       "      <td>Omar</td>\n",
       "      <td>1144</td>\n",
       "      <td>/z2weSPo4sdMNj47tP5o0me41r2z.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87669</td>\n",
       "      <td>Alexie Gilmore</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7575</td>\n",
       "      <td>Dr. Sara Dillane</td>\n",
       "      <td>1144</td>\n",
       "      <td>/1Ev9eTuu8bL6t8xzfqchZm4wSfI.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005045</td>\n",
       "      <td>Jian Leonardo</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3031</td>\n",
       "      <td>Detective</td>\n",
       "      <td>1144</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16687</th>\n",
       "      <td>1299479</td>\n",
       "      <td>Lee Jin-uk</td>\n",
       "      <td>2</td>\n",
       "      <td>3.3383</td>\n",
       "      <td>Yoon Suk-hoon</td>\n",
       "      <td>281004</td>\n",
       "      <td>/fODHfrMuRac7xkeFErJOjawggM1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16688</th>\n",
       "      <td>1813385</td>\n",
       "      <td>Jung Chae-yeon</td>\n",
       "      <td>1</td>\n",
       "      <td>2.2840</td>\n",
       "      <td>Kang Hyo-min / Kang Hyo-Ju</td>\n",
       "      <td>281004</td>\n",
       "      <td>/xdE2sxSVRf0JMLoyqTpwY3oU5Bl.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16689</th>\n",
       "      <td>1115730</td>\n",
       "      <td>Jeon Hye-bin</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4287</td>\n",
       "      <td>Hyo Min-jung</td>\n",
       "      <td>281004</td>\n",
       "      <td>/Ar9Ol7gk1YQdQqdNaYEgzcIEstY.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16690</th>\n",
       "      <td>1615316</td>\n",
       "      <td>Lee Hak-ju</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0675</td>\n",
       "      <td>Lee Jin-woo</td>\n",
       "      <td>281004</td>\n",
       "      <td>/gajjcFdyWx49Mlca4JwTVe3zr87.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16691</th>\n",
       "      <td>3271690</td>\n",
       "      <td>Kang Sang-jun</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0486</td>\n",
       "      <td>Ji Kook-hyun</td>\n",
       "      <td>281004</td>\n",
       "      <td>/ntVm8JzE6eXvfDsvDOY9hQQbEmP.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16692 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cast_id                        name  gender  popularity  \\\n",
       "0        12795       Nikolaj Coster-Waldau       2      2.9643   \n",
       "1        20803           Zuleikha Robinson       1      1.4830   \n",
       "2       196179  Stephen McKinley Henderson       2      1.1218   \n",
       "3        87669              Alexie Gilmore       1      0.7575   \n",
       "4      2005045               Jian Leonardo       2      0.3031   \n",
       "...        ...                         ...     ...         ...   \n",
       "16687  1299479                  Lee Jin-uk       2      3.3383   \n",
       "16688  1813385              Jung Chae-yeon       1      2.2840   \n",
       "16689  1115730                Jeon Hye-bin       1      1.4287   \n",
       "16690  1615316                  Lee Hak-ju       2      1.0675   \n",
       "16691  3271690               Kang Sang-jun       2      1.0486   \n",
       "\n",
       "                        character  series_id                      profile_path  \n",
       "0                  John Amsterdam       1144  /6w2SgB20qzs2R5MQIAckINLhfoP.jpg  \n",
       "1                     Eva Marquez       1144  /838l1DBT6LqxI9Lw5qXrDpIqEzW.jpg  \n",
       "2                            Omar       1144  /z2weSPo4sdMNj47tP5o0me41r2z.jpg  \n",
       "3                Dr. Sara Dillane       1144  /1Ev9eTuu8bL6t8xzfqchZm4wSfI.jpg  \n",
       "4                       Detective       1144                              None  \n",
       "...                           ...        ...                               ...  \n",
       "16687               Yoon Suk-hoon     281004  /fODHfrMuRac7xkeFErJOjawggM1.jpg  \n",
       "16688  Kang Hyo-min / Kang Hyo-Ju     281004  /xdE2sxSVRf0JMLoyqTpwY3oU5Bl.jpg  \n",
       "16689                Hyo Min-jung     281004  /Ar9Ol7gk1YQdQqdNaYEgzcIEstY.jpg  \n",
       "16690                 Lee Jin-woo     281004  /gajjcFdyWx49Mlca4JwTVe3zr87.jpg  \n",
       "16691                Ji Kook-hyun     281004  /ntVm8JzE6eXvfDsvDOY9hQQbEmP.jpg  \n",
       "\n",
       "[16692 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_dramas_cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "profile_path    0.030374\n",
       "cast_id         0.000000\n",
       "name            0.000000\n",
       "gender          0.000000\n",
       "popularity      0.000000\n",
       "character       0.000000\n",
       "series_id       0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_dramas_cast.isna().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crew_member_id</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>popularity</th>\n",
       "      <th>department</th>\n",
       "      <th>job</th>\n",
       "      <th>series_id</th>\n",
       "      <th>profile_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1318326</td>\n",
       "      <td>Jacob Cooney</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3665</td>\n",
       "      <td>Writing</td>\n",
       "      <td>Writers' Assistant</td>\n",
       "      <td>36</td>\n",
       "      <td>/4hEMgTX1BBXXNPGNFga8xY3e7MS.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1223164</td>\n",
       "      <td>Cecily von Ziegesar</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0654</td>\n",
       "      <td>Writing</td>\n",
       "      <td>Novel</td>\n",
       "      <td>1395</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7624</td>\n",
       "      <td>Stan Lee</td>\n",
       "      <td>2</td>\n",
       "      <td>2.7493</td>\n",
       "      <td>Writing</td>\n",
       "      <td>Comic Book</td>\n",
       "      <td>1403</td>\n",
       "      <td>/kKeyWoFtTqOPsbmwylNHmuB3En9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18866</td>\n",
       "      <td>Jack Kirby</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0420</td>\n",
       "      <td>Writing</td>\n",
       "      <td>Comic Book</td>\n",
       "      <td>1403</td>\n",
       "      <td>/nSD0Q3gt7SQpsqyScRoUp1Hjh60.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1855651</td>\n",
       "      <td>IvÃ¡n Cuevas</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>Writing</td>\n",
       "      <td>Writer</td>\n",
       "      <td>1341</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>1302761</td>\n",
       "      <td>Toni Carrizosa</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>Writing</td>\n",
       "      <td>Idea</td>\n",
       "      <td>280888</td>\n",
       "      <td>/gfcWPejP672bt0ODKmWbgZQ8kXJ.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3672</th>\n",
       "      <td>4102600</td>\n",
       "      <td>VerÃ³nica Vila San Juan</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0966</td>\n",
       "      <td>Writing</td>\n",
       "      <td>Idea</td>\n",
       "      <td>280888</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3673</th>\n",
       "      <td>5638231</td>\n",
       "      <td>Dario Venegas</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>Writing</td>\n",
       "      <td>Writer</td>\n",
       "      <td>297464</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3674</th>\n",
       "      <td>1563037</td>\n",
       "      <td>Lina Uribe</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1445</td>\n",
       "      <td>Writing</td>\n",
       "      <td>Writer</td>\n",
       "      <td>297464</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>5694558</td>\n",
       "      <td>Park Guk-jae</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>Writing</td>\n",
       "      <td>Original Story</td>\n",
       "      <td>280945</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3676 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      crew_member_id                    name  gender  popularity department  \\\n",
       "0            1318326            Jacob Cooney       2      0.3665    Writing   \n",
       "1            1223164     Cecily von Ziegesar       0      0.0654    Writing   \n",
       "2               7624                Stan Lee       2      2.7493    Writing   \n",
       "3              18866              Jack Kirby       2      1.0420    Writing   \n",
       "4            1855651             IvÃ¡n Cuevas       2      0.0963    Writing   \n",
       "...              ...                     ...     ...         ...        ...   \n",
       "3671         1302761          Toni Carrizosa       0      0.2087    Writing   \n",
       "3672         4102600  VerÃ³nica Vila San Juan       0      0.0966    Writing   \n",
       "3673         5638231           Dario Venegas       0      0.0071    Writing   \n",
       "3674         1563037              Lina Uribe       1      0.1445    Writing   \n",
       "3675         5694558            Park Guk-jae       0      0.0322    Writing   \n",
       "\n",
       "                     job  series_id                      profile_path  \n",
       "0     Writers' Assistant         36  /4hEMgTX1BBXXNPGNFga8xY3e7MS.jpg  \n",
       "1                  Novel       1395                              None  \n",
       "2             Comic Book       1403  /kKeyWoFtTqOPsbmwylNHmuB3En9.jpg  \n",
       "3             Comic Book       1403  /nSD0Q3gt7SQpsqyScRoUp1Hjh60.jpg  \n",
       "4                 Writer       1341                              None  \n",
       "...                  ...        ...                               ...  \n",
       "3671                Idea     280888  /gfcWPejP672bt0ODKmWbgZQ8kXJ.jpg  \n",
       "3672                Idea     280888                              None  \n",
       "3673              Writer     297464                              None  \n",
       "3674              Writer     297464                              None  \n",
       "3675      Original Story     280945                              None  \n",
       "\n",
       "[3676 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_dramas_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "profile_path      0.597116\n",
       "crew_member_id    0.000000\n",
       "name              0.000000\n",
       "gender            0.000000\n",
       "popularity        0.000000\n",
       "department        0.000000\n",
       "job               0.000000\n",
       "series_id         0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_dramas_writer.isna().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crew_member_id</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>popularity</th>\n",
       "      <th>department</th>\n",
       "      <th>job</th>\n",
       "      <th>series_id</th>\n",
       "      <th>profile_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54773</td>\n",
       "      <td>Steven Pearl</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2786</td>\n",
       "      <td>Production</td>\n",
       "      <td>Executive Producer</td>\n",
       "      <td>1144</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68901</td>\n",
       "      <td>David Manson</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3145</td>\n",
       "      <td>Production</td>\n",
       "      <td>Executive Producer</td>\n",
       "      <td>1144</td>\n",
       "      <td>/4EYuvfQ9Z5tAyQ7lT6xfKzUietR.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34050</td>\n",
       "      <td>Allan Loeb</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4953</td>\n",
       "      <td>Production</td>\n",
       "      <td>Executive Producer</td>\n",
       "      <td>1144</td>\n",
       "      <td>/xtDHFBb0QQ3QGvdPSextyouOBQr.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168812</td>\n",
       "      <td>Stephanie Savage</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2975</td>\n",
       "      <td>Production</td>\n",
       "      <td>Executive Producer</td>\n",
       "      <td>1395</td>\n",
       "      <td>/bWCBO6nDtYsUrPXZiFsqrMvtZgN.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2609135</td>\n",
       "      <td>Mark A. Burley</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0967</td>\n",
       "      <td>Production</td>\n",
       "      <td>Executive Producer</td>\n",
       "      <td>186</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7866</th>\n",
       "      <td>3723056</td>\n",
       "      <td>Ana Eiras</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1719</td>\n",
       "      <td>Production</td>\n",
       "      <td>Executive Producer</td>\n",
       "      <td>280888</td>\n",
       "      <td>/jCw8MIggxVdXNNshz4U6NhPLVlx.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7867</th>\n",
       "      <td>1311136</td>\n",
       "      <td>Jang Kyung-ik</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8182</td>\n",
       "      <td>Production</td>\n",
       "      <td>Executive Producer</td>\n",
       "      <td>280945</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7868</th>\n",
       "      <td>3984952</td>\n",
       "      <td>Walter Iuzzolino</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0867</td>\n",
       "      <td>Production</td>\n",
       "      <td>Executive Producer</td>\n",
       "      <td>279603</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7869</th>\n",
       "      <td>3358016</td>\n",
       "      <td>Jo McGrath</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1432</td>\n",
       "      <td>Production</td>\n",
       "      <td>Executive Producer</td>\n",
       "      <td>279603</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7870</th>\n",
       "      <td>1721255</td>\n",
       "      <td>Dries Vos</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2853</td>\n",
       "      <td>Production</td>\n",
       "      <td>Executive Producer</td>\n",
       "      <td>279603</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7871 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      crew_member_id              name  gender  popularity  department  \\\n",
       "0              54773      Steven Pearl       2      0.2786  Production   \n",
       "1              68901      David Manson       2      0.3145  Production   \n",
       "2              34050        Allan Loeb       2      0.4953  Production   \n",
       "3             168812  Stephanie Savage       1      0.2975  Production   \n",
       "4            2609135    Mark A. Burley       2      0.0967  Production   \n",
       "...              ...               ...     ...         ...         ...   \n",
       "7866         3723056         Ana Eiras       1      0.1719  Production   \n",
       "7867         1311136     Jang Kyung-ik       2      0.8182  Production   \n",
       "7868         3984952  Walter Iuzzolino       2      0.0867  Production   \n",
       "7869         3358016        Jo McGrath       0      0.1432  Production   \n",
       "7870         1721255         Dries Vos       0      0.2853  Production   \n",
       "\n",
       "                     job  series_id                      profile_path  \n",
       "0     Executive Producer       1144                              None  \n",
       "1     Executive Producer       1144  /4EYuvfQ9Z5tAyQ7lT6xfKzUietR.jpg  \n",
       "2     Executive Producer       1144  /xtDHFBb0QQ3QGvdPSextyouOBQr.jpg  \n",
       "3     Executive Producer       1395  /bWCBO6nDtYsUrPXZiFsqrMvtZgN.jpg  \n",
       "4     Executive Producer        186                              None  \n",
       "...                  ...        ...                               ...  \n",
       "7866  Executive Producer     280888  /jCw8MIggxVdXNNshz4U6NhPLVlx.jpg  \n",
       "7867  Executive Producer     280945                              None  \n",
       "7868  Executive Producer     279603                              None  \n",
       "7869  Executive Producer     279603                              None  \n",
       "7870  Executive Producer     279603                              None  \n",
       "\n",
       "[7871 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_dramas_producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "profile_path      0.440097\n",
       "crew_member_id    0.000000\n",
       "name              0.000000\n",
       "gender            0.000000\n",
       "popularity        0.000000\n",
       "department        0.000000\n",
       "job               0.000000\n",
       "series_id         0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_dramas_producer.isna().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê¸°ì¡´ ì˜í™” í¬ë ˆë”§ ìˆ˜ì§‘ ì½”ë“œ (íŒŒì‹± ì „)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Discover ë‹¨ê³„ ì‹œì‘ (movie_id ìˆ˜ì§‘)\n",
      "ğŸ“… ë¶„í• ëœ ë‚ ì§œ êµ¬ê°„: 4\n",
      "  â–· [1/4] 2005-01-01 ~ 2010-03-25 (pages=266)\n",
      "  â–· [2/4] 2010-03-26 ~ 2015-06-17 (pages=364)\n",
      "  â–· [3/4] 2015-06-18 ~ 2020-09-08 (pages=460)\n",
      "  â–· [4/4] 2020-09-09 ~ 2025-11-30 (pages=331)\n",
      "ğŸ‰ Discover ì™„ë£Œ: ê³ ìœ  movie_id 28,384ê°œ í™•ë³´\n",
      "â³ ì²˜ë¦¬ ëŒ€ìƒ: 23,984\n",
      "   ì™„ë£Œ: 4,399, ì‹¤íŒ¨: 1\n",
      "  â–· Batch 1/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =400, 11.4s\n",
      "  â–· Batch 2/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =799, 11.4s\n",
      "  â–· Batch 3/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =1199, 12.1s\n",
      "  â–· Batch 4/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =1599, 11.8s\n",
      "  â–· Batch 5/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =1999, 12.1s\n",
      "  â–· Batch 6/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =2399, 12.0s\n",
      "  â–· Batch 7/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =2798, 12.0s\n",
      "  â–· Batch 8/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =3197, 12.0s\n",
      "  â–· Batch 9/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =3595, 12.0s\n",
      "  â–· Batch 10/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =3993, 12.1s\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (batch=10)\n",
      "  â–· Batch 11/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =4393, 12.1s\n",
      "  â–· Batch 12/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =4793, 11.9s\n",
      "  â–· Batch 13/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =5193, 11.9s\n",
      "  â–· Batch 14/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =5593, 11.9s\n",
      "  â–· Batch 15/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =5993, 11.8s\n",
      "  â–· Batch 16/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =6392, 11.8s\n",
      "  â–· Batch 17/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =6792, 11.8s\n",
      "  â–· Batch 18/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =7191, 12.0s\n",
      "  â–· Batch 19/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =7590, 11.8s\n",
      "  â–· Batch 20/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =7989, 12.0s\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (batch=20)\n",
      "  â–· Batch 21/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =8389, 11.9s\n",
      "  â–· Batch 22/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =8788, 11.9s\n",
      "  â–· Batch 23/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =9188, 12.0s\n",
      "  â–· Batch 24/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =9588, 12.0s\n",
      "  â–· Batch 25/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =9987, 12.0s\n",
      "  â–· Batch 26/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =10386, 12.0s\n",
      "  â–· Batch 27/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =10786, 11.9s\n",
      "  â–· Batch 28/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =11186, 11.9s\n",
      "  â–· Batch 29/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =11586, 11.8s\n",
      "  â–· Batch 30/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =11986, 11.8s\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (batch=30)\n",
      "  â–· Batch 31/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =12386, 11.9s\n",
      "  â–· Batch 32/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =12785, 12.1s\n",
      "  â–· Batch 33/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =13185, 12.7s\n",
      "  â–· Batch 34/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =13583, 11.9s\n",
      "  â–· Batch 35/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =13981, 12.0s\n",
      "  â–· Batch 36/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =14380, 11.8s\n",
      "  â–· Batch 37/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =14780, 11.9s\n",
      "  â–· Batch 38/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =15179, 11.9s\n",
      "  â–· Batch 39/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =15577, 12.0s\n",
      "  â–· Batch 40/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =15976, 11.8s\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (batch=40)\n",
      "  â–· Batch 41/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =16374, 11.8s\n",
      "  â–· Batch 42/60: 397ê°œ ì²˜ë¦¬, ëˆ„ì =16771, 12.2s\n",
      "  â–· Batch 43/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =17170, 11.9s\n",
      "  â–· Batch 44/60: 397ê°œ ì²˜ë¦¬, ëˆ„ì =17567, 11.8s\n",
      "  â–· Batch 45/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =17966, 11.8s\n",
      "  â–· Batch 46/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =18364, 11.9s\n",
      "  â–· Batch 47/60: 395ê°œ ì²˜ë¦¬, ëˆ„ì =18759, 11.8s\n",
      "  â–· Batch 48/60: 397ê°œ ì²˜ë¦¬, ëˆ„ì =19156, 12.0s\n",
      "  â–· Batch 49/60: 395ê°œ ì²˜ë¦¬, ëˆ„ì =19551, 11.9s\n",
      "  â–· Batch 50/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =19950, 11.9s\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (batch=50)\n",
      "  â–· Batch 51/60: 391ê°œ ì²˜ë¦¬, ëˆ„ì =20341, 11.8s\n",
      "  â–· Batch 52/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =20740, 12.0s\n",
      "  â–· Batch 53/60: 396ê°œ ì²˜ë¦¬, ëˆ„ì =21136, 11.9s\n",
      "  â–· Batch 54/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =21534, 12.0s\n",
      "  â–· Batch 55/60: 397ê°œ ì²˜ë¦¬, ëˆ„ì =21931, 12.0s\n",
      "  â–· Batch 56/60: 396ê°œ ì²˜ë¦¬, ëˆ„ì =22327, 11.9s\n",
      "  â–· Batch 57/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =22726, 11.9s\n",
      "  â–· Batch 58/60: 391ê°œ ì²˜ë¦¬, ëˆ„ì =23117, 12.1s\n",
      "  â–· Batch 59/60: 397ê°œ ì²˜ë¦¬, ëˆ„ì =23514, 11.8s\n",
      "  â–· Batch 60/60: 377ê°œ ì²˜ë¦¬, ëˆ„ì =23891, 11.4s\n",
      "âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: [Errno 2] No such file or directory: 'tmdb_movie_full/checkpoint.tmp' -> 'tmdb_movie_full/checkpoint.json'\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (batch=60)\n",
      "\n",
      "ğŸ“¦ JSONL â†’ Parquet ë³€í™˜ ì¤‘...\n",
      "\n",
      "ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! (15.9ë¶„)\n",
      "ğŸ“ ì €ì¥ íŒŒì¼: tmdb_movie_full/tmdb_movies_2005_2025_FULL.parquet\n",
      "   ì´ ì˜í™” ìˆ˜: 28,290í¸\n",
      "   ì‹¤íŒ¨ ì˜í™” ìˆ˜: 94í¸\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# TMDB MOVIE FULL COLLECTOR â€” PRODUCTION V9 (Extreme-Day FULL)\n",
    "# (2005-01-01 ~ 2025-11-30)\n",
    "#\n",
    "# - ëŒ€ìƒ: TMDB ì˜í™” (vote_count >= 30 + imdb_id í•„ìˆ˜)\n",
    "# - íŠ¹ì§•:\n",
    "#   * discover/movie ê¸°ê°„ ìë™ ë¶„í•  (500í˜ì´ì§€ ì œí•œ ëŒ€ì‘)\n",
    "#   * âœ… ì–´ë–¤ êµ¬ê°„ë„ 500í˜ì´ì§€ ì´ˆê³¼ ìƒíƒœë¡œ ìˆ˜ì§‘í•˜ì§€ ì•ŠìŒ\n",
    "#   * âœ… ë§Œì•½ \"ë‹¨ì¼ ë‚ ì§œ\"ì—ì„œ total_pages > 500 ë°œìƒ ì‹œ â†’ RuntimeError ë°œìƒ\n",
    "#       â†’ TMDB API í•œê³„ë¡œ ê·¸ ë‚ ì§œëŠ” ì „ìˆ˜ ìˆ˜ì§‘ ë¶ˆê°€í•˜ë‹¤ëŠ” ê²ƒì„ ëª…ì‹œì ìœ¼ë¡œ ì•Œë ¤ì¤Œ\n",
    "#       â†’ ì¦‰, \"ì¡°ìš©í•œ ëˆ„ë½\"ì€ ì ˆëŒ€ ì—†ìŒ (0% ëˆ„ë½ ë³´ì¥ or ì‹¤íŒ¨)\n",
    "#   * ê°ë… / ì‘ê°€ / ì£¼ì—°ë°°ìš°(Top 5) í¬ë ˆë”§ ì „ìˆ˜ ìˆ˜ì§‘\n",
    "#   * ì´ˆë‹¹ ìš”ì²­ ì œí•œ(35 req/s) + ì¬ì‹œë„(backoff)\n",
    "#   * JSONL ìŠ¤íŠ¸ë¦¼ ì €ì¥ + Parquet ìµœì¢… ì €ì¥\n",
    "#   * ì¤‘ë‹¨/ì¬ê°œ: checkpoint + JSONL + failed_ids\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIG\n",
    "# ==========================================================\n",
    "DATA_DIR = Path(\"./tmdb_movie_full\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MOVIE_IDS_PATH     = DATA_DIR / \"movie_ids_2005_2025.json\"\n",
    "DETAILS_JSONL_PATH = DATA_DIR / \"movie_details_2005_2025.jsonl\"\n",
    "CHECKPOINT_PATH    = DATA_DIR / \"checkpoint.json\"\n",
    "FAILED_IDS_PATH    = DATA_DIR / \"failed_ids.json\"\n",
    "FINAL_PARQUET_PATH = DATA_DIR / \"tmdb_movies_2005_2025_FULL.parquet\"\n",
    "\n",
    "API_KEY = os.getenv(\"TMDB_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"í™˜ê²½ë³€ìˆ˜ TMDB_API_KEY ì„¤ì • í•„ìš”!\")\n",
    "\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "HEADERS  = {\"accept\": \"application/json\"}\n",
    "\n",
    "TIMEOUT   = 10\n",
    "MAX_RETRIES = 5\n",
    "MAX_CALLS_PER_SECOND   = 35\n",
    "MAX_CONCURRENT_REQUESTS = 150  # íš¨ìœ¨ì„±\n",
    "\n",
    "START_DATE = datetime(2005, 1, 1)\n",
    "END_DATE   = datetime(2025, 11, 30)\n",
    "\n",
    "WRITER_JOBS = {\n",
    "    \"Writer\", \"Screenplay\", \"Story\", \"Creator\",\n",
    "    \"Novel\", \"Comic Book\", \"Adaptation\", \"Screenstory\"\n",
    "}\n",
    "\n",
    "jsonl_lock      = asyncio.Lock()\n",
    "checkpoint_lock = asyncio.Lock()\n",
    "\n",
    "# ==========================================================\n",
    "# Rate Limiter\n",
    "# ==========================================================\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_calls=35, period=1.0):\n",
    "        self.max_calls = max_calls\n",
    "        self.period = period\n",
    "        self.calls = deque()\n",
    "        self.lock  = asyncio.Lock()\n",
    "\n",
    "    async def acquire(self):\n",
    "        async with self.lock:\n",
    "            loop = asyncio.get_running_loop()\n",
    "            now = loop.time()\n",
    "\n",
    "            # ì˜¤ë˜ëœ í˜¸ì¶œ ì œê±°\n",
    "            while self.calls and now - self.calls[0] > self.period:\n",
    "                self.calls.popleft()\n",
    "\n",
    "            if len(self.calls) >= self.max_calls:\n",
    "                wait = self.period - (now - self.calls[0]) + 0.02\n",
    "                if wait > 0:\n",
    "                    await asyncio.sleep(wait)\n",
    "\n",
    "                now = loop.time()\n",
    "                while self.calls and now - self.calls[0] > self.period:\n",
    "                    self.calls.popleft()\n",
    "\n",
    "            self.calls.append(loop.time())\n",
    "\n",
    "\n",
    "rate_limiter = RateLimiter(MAX_CALLS_PER_SECOND)\n",
    "\n",
    "# ==========================================================\n",
    "# fetch_json\n",
    "# ==========================================================\n",
    "async def fetch_json(session, url, params, desc=\"\", retries=MAX_RETRIES):\n",
    "    for attempt in range(1, retries + 1):\n",
    "        await rate_limiter.acquire()\n",
    "\n",
    "        try:\n",
    "            async with session.get(url, params=params, timeout=TIMEOUT) as resp:\n",
    "                if resp.status == 200:\n",
    "                    return await resp.json()\n",
    "\n",
    "                if resp.status == 404:\n",
    "                    return None\n",
    "\n",
    "                if resp.status in {429, 500, 502, 503, 504}:\n",
    "                    if resp.status == 429:\n",
    "                        retry_after = resp.headers.get(\"Retry-After\", \"2\")\n",
    "                        try:\n",
    "                            retry_after = int(retry_after)\n",
    "                        except Exception:\n",
    "                            retry_after = 2\n",
    "                        await asyncio.sleep(retry_after)\n",
    "                    else:\n",
    "                        await asyncio.sleep(attempt * 2)\n",
    "                    continue\n",
    "\n",
    "                # ê¸°íƒ€ ì—ëŸ¬ëŠ” ì‹¤íŒ¨ë¡œ ì²˜ë¦¬\n",
    "                return None\n",
    "\n",
    "        except asyncio.TimeoutError:\n",
    "            await asyncio.sleep(attempt * 2)\n",
    "        except Exception:\n",
    "            await asyncio.sleep(attempt * 2)\n",
    "\n",
    "    return None\n",
    "\n",
    "# ==========================================================\n",
    "# ë¬¸ìì—´ ì •ì œ ìœ í‹¸\n",
    "# ==========================================================\n",
    "def sanitize_text_mild(text):\n",
    "    \"\"\"\n",
    "    ê²½ë¯¸í•œ ì •ì œ (ì´ë¦„, ì œëª©ìš©)\n",
    "    - ì¤„ë°”ê¿ˆ, HTML íƒœê·¸, ì—°ì† ê³µë°±ë§Œ ì œê±°\n",
    "    - ìœ ë‹ˆì½”ë“œ ë³´ì¡´ (í•œê¸€, ì¼ë³¸ì–´, ìœ ëŸ½ì–´ ë“±)\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # HTML íƒœê·¸ ì œê±°\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    # ì¤„ë°”ê¿ˆ, íƒ­ ì œê±°\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    # ì—°ì† ê³µë°± ì œê±°\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def sanitize_text_strong(text):\n",
    "    \"\"\"\n",
    "    ê°•ë ¥í•œ ì •ì œ (characterìš© - Parquet ì•ˆì •ì„± ìš°ì„ )\n",
    "    - ì œì–´ ë¬¸ì, íŠ¹ìˆ˜ ìœ ë‹ˆì½”ë“œ ì œê±°\n",
    "    - ì¼ë°˜ ìœ ë‹ˆì½”ë“œëŠ” ë³´ì¡´\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # HTML íƒœê·¸ ì œê±°\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    # HTML ì—”í‹°í‹° ì œê±°\n",
    "    text = re.sub(r\"&[a-zA-Z]+;\", \" \", text)\n",
    "    # ì œì–´ ë¬¸ì ì œê±°\n",
    "    text = re.sub(r\"[\\x00-\\x1F\\x7F-\\x9F]\", \" \", text)\n",
    "    # ì—°ì† ê³µë°± ì œê±°\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def join_values(arr, use_strong_sanitize=False):\n",
    "    \"\"\"\n",
    "    ë¦¬ìŠ¤íŠ¸ë¥¼ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ join\n",
    "    - use_strong_sanitize=True: characterìš© (ê°•ë ¥í•œ ì •ì œ)\n",
    "    - use_strong_sanitize=False: ì´ë¦„/ì œëª©ìš© (ê²½ë¯¸í•œ ì •ì œ)\n",
    "    \"\"\"\n",
    "    if not arr:\n",
    "        return \"\"\n",
    "    \n",
    "    sanitize_func = sanitize_text_strong if use_strong_sanitize else sanitize_text_mild\n",
    "    cleaned = [sanitize_func(x) if x is not None else \"\" for x in arr]\n",
    "    return \"; \".join(cleaned)\n",
    "\n",
    "# ==========================================================\n",
    "# DISCOVER\n",
    "# ==========================================================\n",
    "async def discover_total_pages(session, start_date: datetime, end_date: datetime):\n",
    "    params = {\n",
    "        \"api_key\": API_KEY,\n",
    "        \"language\": \"en-US\",\n",
    "        \"sort_by\": \"primary_release_date.asc\",\n",
    "        \"primary_release_date.gte\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"primary_release_date.lte\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"vote_count.gte\": 30,\n",
    "        \"include_adult\": \"false\",\n",
    "        \"page\": 1,\n",
    "    }\n",
    "    url = f\"{BASE_URL}/discover/movie\"\n",
    "    data = await fetch_json(session, url, params)\n",
    "    if not data:\n",
    "        return 0\n",
    "    return int(data.get(\"total_pages\", 0))\n",
    "\n",
    "\n",
    "async def split_ranges(session, start_dt: datetime, end_dt: datetime, limit: int = 500):\n",
    "    \"\"\"\n",
    "    ë‚ ì§œ êµ¬ê°„ì„ ì—¬ëŸ¬ ê°œë¡œ ë‚˜ëˆ„ì–´\n",
    "    ê° êµ¬ê°„ì˜ discover total_pagesê°€ limit ì´í•˜ê°€ ë˜ë„ë¡ ë¶„í• .\n",
    "\n",
    "    - ë§Œì•½ \"ë‹¨ì¼ ë‚ ì§œ(s == e)\"ì—ì„œì¡°ì°¨ total_pages > limitê°€ ë‚˜ì˜¤ë©´\n",
    "      TMDB API í•œê³„ë¡œ ì¸í•´ ê·¸ ë‚ ì§œëŠ” ì „ìˆ˜ ìˆ˜ì§‘ ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ\n",
    "      RuntimeErrorë¥¼ ë°œìƒì‹œì¼œ ì¡°ìš©í•œ ëˆ„ë½ì„ ë§‰ìŒ.\n",
    "    \"\"\"\n",
    "    queue = deque([(start_dt, end_dt)])\n",
    "    out = []\n",
    "\n",
    "    while queue:\n",
    "        s, e = queue.popleft()\n",
    "        if s > e:\n",
    "            continue\n",
    "\n",
    "        total_pages = await discover_total_pages(session, s, e)\n",
    "        if total_pages == 0:\n",
    "            continue\n",
    "\n",
    "        if total_pages <= limit:\n",
    "            out.append((s, e, total_pages))\n",
    "            continue\n",
    "\n",
    "        days = (e - s).days\n",
    "\n",
    "        # ë‹¨ì¼ ë‚ ì§œì¸ë°ë„ 500í˜ì´ì§€ ì´ˆê³¼ â†’ TMDB discover í•œê³„ë¡œ ì „ìˆ˜ ìˆ˜ì§‘ ë¶ˆê°€\n",
    "        if days <= 0:\n",
    "            msg = (\n",
    "                f\"âŒ TMDB discover í•œê³„ ì´ˆê³¼: {s.date()} ì˜ total_pages={total_pages} > {limit}\\n\"\n",
    "                f\"   ì´ ë‚ ì§œì— ëŒ€í•´ discover/movie APIë¡œëŠ” 500í˜ì´ì§€(10,000í¸) ì´ìƒì„ ì „ìˆ˜ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\\n\"\n",
    "                f\"   (primary_release_dateëŠ” ë‚ ì§œ ë‹¨ìœ„ë§Œ ì§€ì›, ì‹œê°„ ë‹¨ìœ„ ë¶„í•  ë¶ˆê°€)\\n\"\n",
    "                f\"   â†’ í•„í„° ì¡°ê±´ì„ ì™„í™”/ë³€ê²½í•˜ê±°ë‚˜, ì´ ë‚ ì§œë¥¼ ë¶„ì„ ë²”ìœ„ì—ì„œ ì œì™¸í•´ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "            )\n",
    "            raise RuntimeError(msg)\n",
    "\n",
    "        # ë‚ ì§œ êµ¬ê°„ì„ ë°˜ì”© ë‚˜ëˆ„ì–´ total_pagesê°€ 500 ì´í•˜ê°€ ë˜ë„ë¡ ì¬ê·€ ë¶„í• \n",
    "        mid_days = days // 2\n",
    "        mid = s + timedelta(days=mid_days)\n",
    "\n",
    "        # ì™¼ìª½: s ~ mid\n",
    "        queue.append((s, mid))\n",
    "        # ì˜¤ë¥¸ìª½: mid+1 ~ e\n",
    "        if mid < e:\n",
    "            queue.append((mid + timedelta(days=1), e))\n",
    "\n",
    "    return sorted(out, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "async def discover_movie_ids(session):\n",
    "    print(\"ğŸ“Œ Discover ë‹¨ê³„ ì‹œì‘ (movie_id ìˆ˜ì§‘)\")\n",
    "\n",
    "    ranges = await split_ranges(session, START_DATE, END_DATE, limit=500)\n",
    "    print(f\"ğŸ“… ë¶„í• ëœ ë‚ ì§œ êµ¬ê°„: {len(ranges)}\")\n",
    "\n",
    "    all_ids = set()\n",
    "    url = f\"{BASE_URL}/discover/movie\"\n",
    "\n",
    "    for idx, (s, e, pages) in enumerate(ranges, start=1):\n",
    "        print(f\"  â–· [{idx}/{len(ranges)}] {s.date()} ~ {e.date()} (pages={pages})\")\n",
    "\n",
    "        for p in range(1, pages + 1):\n",
    "            params = {\n",
    "                \"api_key\": API_KEY,\n",
    "                \"language\": \"en-US\",\n",
    "                \"sort_by\": \"primary_release_date.asc\",\n",
    "                \"primary_release_date.gte\": s.strftime(\"%Y-%m-%d\"),\n",
    "                \"primary_release_date.lte\": e.strftime(\"%Y-%m-%d\"),\n",
    "                \"vote_count.gte\": 30,\n",
    "                \"include_adult\": \"false\",\n",
    "                \"page\": p,\n",
    "            }\n",
    "            data = await fetch_json(session, url, params)\n",
    "            if not data:\n",
    "                continue\n",
    "\n",
    "            for item in data.get(\"results\", []):\n",
    "                if item.get(\"vote_count\", 0) >= 30:\n",
    "                    all_ids.add(item[\"id\"])\n",
    "\n",
    "    ids = sorted(all_ids)\n",
    "    MOVIE_IDS_PATH.write_text(json.dumps(ids))\n",
    "    print(f\"ğŸ‰ Discover ì™„ë£Œ: ê³ ìœ  movie_id {len(ids):,}ê°œ í™•ë³´\")\n",
    "\n",
    "    return ids\n",
    "\n",
    "# ==========================================================\n",
    "# EXTRACTORS\n",
    "# ==========================================================\n",
    "def extract_directors(crew):\n",
    "    if not crew:\n",
    "        return \"\", \"\", \"\", \"\"\n",
    "\n",
    "    seen = {}\n",
    "    for c in crew:\n",
    "        if c.get(\"job\") == \"Director\":\n",
    "            pid = c.get(\"id\")\n",
    "            if pid and pid not in seen:\n",
    "                seen[pid] = c\n",
    "\n",
    "    if not seen:\n",
    "        return \"\", \"\", \"\", \"\"\n",
    "\n",
    "    arr = sorted(seen.values(), key=lambda x: x[\"id\"])\n",
    "\n",
    "    names    = join_values([a.get(\"name\") for a in arr], use_strong_sanitize=False)\n",
    "    ids      = join_values([a.get(\"id\") for a in arr], use_strong_sanitize=False)\n",
    "    genders  = join_values([a.get(\"gender\") for a in arr], use_strong_sanitize=False)\n",
    "    profiles = join_values([a.get(\"profile_path\") for a in arr], use_strong_sanitize=False)\n",
    "\n",
    "    return names, ids, genders, profiles\n",
    "\n",
    "\n",
    "def extract_writers(crew):\n",
    "    \"\"\"\n",
    "    ì‘ê°€ ì¶”ì¶œ:\n",
    "    - jobì´ WRITER_JOBSì— ìˆëŠ” ê²½ìš°\n",
    "    - ë˜ëŠ” department == \"Writing\" ì´ë©´ì„œ jobì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°\n",
    "    \"\"\"\n",
    "    if not crew:\n",
    "        return \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "    seen = {}\n",
    "\n",
    "    for c in crew:\n",
    "        job  = c.get(\"job\")\n",
    "        dept = c.get(\"department\")\n",
    "        pid  = c.get(\"id\")\n",
    "\n",
    "        if not pid:\n",
    "            continue\n",
    "\n",
    "        if job in WRITER_JOBS:\n",
    "            if pid not in seen:\n",
    "                seen[pid] = {\"person\": c, \"jobs\": set()}\n",
    "            seen[pid][\"jobs\"].add(job)\n",
    "        elif dept == \"Writing\" and job:\n",
    "            if pid not in seen:\n",
    "                seen[pid] = {\"person\": c, \"jobs\": set()}\n",
    "            seen[pid][\"jobs\"].add(job)\n",
    "\n",
    "    if not seen:\n",
    "        return \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "    arr = sorted(seen.items(), key=lambda x: x[0])\n",
    "\n",
    "    names    = join_values([v[\"person\"].get(\"name\") for _, v in arr], use_strong_sanitize=False)\n",
    "    ids      = join_values([v[\"person\"].get(\"id\") for _, v in arr], use_strong_sanitize=False)\n",
    "    roles    = join_values([\n",
    "        \"/\".join(sorted(v[\"jobs\"])) if v[\"jobs\"] else \"Writer\"\n",
    "        for _, v in arr\n",
    "    ], use_strong_sanitize=False)\n",
    "    genders  = join_values([v[\"person\"].get(\"gender\") for _, v in arr], use_strong_sanitize=False)\n",
    "    profiles = join_values([v[\"person\"].get(\"profile_path\") for _, v in arr], use_strong_sanitize=False)\n",
    "\n",
    "    return names, ids, roles, genders, profiles\n",
    "\n",
    "\n",
    "def extract_top_cast(cast):\n",
    "    \"\"\"\n",
    "    ì£¼ì—° ë°°ìš° Top 5 ì¶”ì¶œ\n",
    "    - castë¥¼ order ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬\n",
    "    - characterê°€ ìˆëŠ” í•­ëª©ë§Œ ì‚¬ìš©\n",
    "    - ìƒìœ„ 5ëª…ê¹Œì§€\n",
    "    - ì´ë¦„ì€ ê²½ë¯¸ ì •ì œ, characterëŠ” ê°•ë ¥ ì •ì œ\n",
    "    \"\"\"\n",
    "    if not cast:\n",
    "        return \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "    sorted_cast = sorted(cast, key=lambda x: x.get(\"order\", 999))\n",
    "    filtered = [c for c in sorted_cast if c.get(\"character\")]\n",
    "\n",
    "    top_5 = filtered[:5]\n",
    "    if not top_5:\n",
    "        return \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "    names      = join_values([c.get(\"name\") for c in top_5], use_strong_sanitize=False)\n",
    "    ids        = join_values([c.get(\"id\") for c in top_5], use_strong_sanitize=False)\n",
    "    orders     = join_values([c.get(\"order\") for c in top_5], use_strong_sanitize=False)\n",
    "    characters = join_values([c.get(\"character\") for c in top_5], use_strong_sanitize=True)\n",
    "    genders    = join_values([c.get(\"gender\") for c in top_5], use_strong_sanitize=False)\n",
    "    profiles   = join_values([c.get(\"profile_path\") for c in top_5], use_strong_sanitize=False)\n",
    "\n",
    "    return names, ids, orders, characters, genders, profiles\n",
    "\n",
    "# ==========================================================\n",
    "# MOVIE DETAILS FETCHER\n",
    "# ==========================================================\n",
    "async def fetch_movie_detail(session, movie_id: int):\n",
    "    url = f\"{BASE_URL}/movie/{movie_id}\"\n",
    "    params = {\n",
    "        \"api_key\": API_KEY,\n",
    "        \"language\": \"en-US\",\n",
    "        \"append_to_response\": \"credits\",\n",
    "    }\n",
    "\n",
    "    data = await fetch_json(session, url, params, desc=f\"movie/{movie_id}\")\n",
    "    if not data:\n",
    "        return None\n",
    "\n",
    "    imdb_id = data.get(\"imdb_id\")\n",
    "    if not imdb_id:\n",
    "        return None\n",
    "\n",
    "    vote_count = data.get(\"vote_count\", 0)\n",
    "    if vote_count < 30:\n",
    "        return None\n",
    "\n",
    "    title = sanitize_text_mild(data.get(\"title\"))\n",
    "\n",
    "    credits = data.get(\"credits\", {})\n",
    "    cast = credits.get(\"cast\", []) or []\n",
    "    crew = credits.get(\"crew\", []) or []\n",
    "\n",
    "    directors, director_ids, director_gender, director_profile = extract_directors(crew)\n",
    "    writers, writer_ids, writer_roles, writer_gender, writer_profile = extract_writers(crew)\n",
    "    top_cast, top_cast_ids, top_cast_order, characters, top_cast_gender, top_cast_profile = extract_top_cast(cast)\n",
    "\n",
    "    return {\n",
    "        \"imdb_id\": imdb_id,\n",
    "        \"movie_id\": movie_id,\n",
    "        \"vote_count\": vote_count,\n",
    "        \"title\": title,\n",
    "\n",
    "        \"director_ids\": director_ids,\n",
    "        \"directors\": directors,\n",
    "        \"director_gender\": director_gender,\n",
    "        \"director_profile_path\": director_profile,\n",
    "\n",
    "        \"writer_ids\": writer_ids,\n",
    "        \"writers\": writers,\n",
    "        \"writer_roles\": writer_roles,\n",
    "        \"writer_gender\": writer_gender,\n",
    "        \"writer_profile_path\": writer_profile,\n",
    "\n",
    "        \"top_cast_ids\": top_cast_ids,\n",
    "        \"top_cast_order\": top_cast_order,\n",
    "        \"top_cast\": top_cast,\n",
    "        \"character\": characters,\n",
    "        \"top_cast_gender\": top_cast_gender,\n",
    "        \"top_cast_profile_path\": top_cast_profile,\n",
    "    }\n",
    "\n",
    "# ==========================================================\n",
    "# CHECKPOINT\n",
    "# ==========================================================\n",
    "def load_checkpoint():\n",
    "    if CHECKPOINT_PATH.exists():\n",
    "        try:\n",
    "            data = json.loads(CHECKPOINT_PATH.read_text())\n",
    "            return {\n",
    "                \"done_ids\": set(data.get(\"done_ids\", [])),\n",
    "                \"failed_ids\": set(data.get(\"failed_ids\", [])),\n",
    "                \"last_index\": data.get(\"last_index\", 0),\n",
    "            }\n",
    "        except Exception:\n",
    "            return {\"done_ids\": set(), \"failed_ids\": set(), \"last_index\": 0}\n",
    "    return {\"done_ids\": set(), \"failed_ids\": set(), \"last_index\": 0}\n",
    "\n",
    "\n",
    "async def save_checkpoint(done_ids, failed_ids, last_index: int):\n",
    "    async with checkpoint_lock:\n",
    "        try:\n",
    "            data = {\n",
    "                \"done_ids\": list(done_ids),\n",
    "                \"failed_ids\": list(failed_ids),\n",
    "                \"last_index\": last_index,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "            tmp_path = CHECKPOINT_PATH.with_suffix(\".tmp\")\n",
    "            tmp_path.write_text(json.dumps(data, ensure_ascii=False, indent=2))\n",
    "            tmp_path.replace(CHECKPOINT_PATH)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "\n",
    "def load_done_from_jsonl():\n",
    "    if not DETAILS_JSONL_PATH.exists():\n",
    "        return set()\n",
    "\n",
    "    done = set()\n",
    "    with open(DETAILS_JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "                mid = rec.get(\"movie_id\")\n",
    "                if mid:\n",
    "                    done.add(mid)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return done\n",
    "\n",
    "\n",
    "def save_failed_ids(failed_ids):\n",
    "    FAILED_IDS_PATH.write_text(\n",
    "        json.dumps(sorted(list(failed_ids)), ensure_ascii=False, indent=2)\n",
    "    )\n",
    "\n",
    "# ==========================================================\n",
    "# BATCH PROCESSING\n",
    "# ==========================================================\n",
    "async def process_batch(session, batch, done_ids: set, failed_ids: set):\n",
    "    sem = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "    async def worker(mid: int):\n",
    "        if mid in done_ids or mid in failed_ids:\n",
    "            return None\n",
    "        async with sem:\n",
    "            result = await fetch_movie_detail(session, mid)\n",
    "            if result is None:\n",
    "                failed_ids.add(mid)\n",
    "                return None\n",
    "            return result\n",
    "\n",
    "    tasks = [asyncio.create_task(worker(mid)) for mid in batch]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    valid = [r for r in results if r is not None]\n",
    "\n",
    "    if valid:\n",
    "        async with jsonl_lock:\n",
    "            try:\n",
    "                lines = [json.dumps(r, ensure_ascii=False) + \"\\n\" for r in valid]\n",
    "                with open(DETAILS_JSONL_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.writelines(lines)\n",
    "\n",
    "                for r in valid:\n",
    "                    done_ids.add(r[\"movie_id\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ JSONL ì“°ê¸° ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    return len(valid)\n",
    "\n",
    "\n",
    "def chunked(arr, n):\n",
    "    for i in range(0, len(arr), n):\n",
    "        yield arr[i:i+n]\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN\n",
    "# ==========================================================\n",
    "async def main():\n",
    "    start = datetime.now()\n",
    "\n",
    "    async with aiohttp.ClientSession(headers=HEADERS) as session:\n",
    "        if MOVIE_IDS_PATH.exists():\n",
    "            movie_ids = json.loads(MOVIE_IDS_PATH.read_text())\n",
    "            print(f\"ğŸ“‚ ê¸°ì¡´ movie_ids ë¡œë“œ: {len(movie_ids):,}\")\n",
    "        else:\n",
    "            movie_ids = await discover_movie_ids(session)\n",
    "\n",
    "        checkpoint = load_checkpoint()\n",
    "        done_ids   = checkpoint[\"done_ids\"]\n",
    "        failed_ids = checkpoint[\"failed_ids\"]\n",
    "\n",
    "        done_ids |= load_done_from_jsonl()\n",
    "\n",
    "        remaining = [mid for mid in movie_ids if mid not in done_ids and mid not in failed_ids]\n",
    "        print(f\"â³ ì²˜ë¦¬ ëŒ€ìƒ: {len(remaining):,}\")\n",
    "        print(f\"   ì™„ë£Œ: {len(done_ids):,}, ì‹¤íŒ¨: {len(failed_ids):,}\")\n",
    "\n",
    "        if not DETAILS_JSONL_PATH.exists():\n",
    "            DETAILS_JSONL_PATH.touch()\n",
    "\n",
    "        BATCH_SIZE = 400\n",
    "        total_processed = 0\n",
    "\n",
    "        batches = list(chunked(remaining, BATCH_SIZE))\n",
    "\n",
    "        for idx, batch in enumerate(batches, start=1):\n",
    "            batch_start = datetime.now()\n",
    "            n = await process_batch(session, batch, done_ids, failed_ids)\n",
    "            total_processed += n\n",
    "\n",
    "            elapsed = (datetime.now() - batch_start).total_seconds()\n",
    "            print(f\"  â–· Batch {idx}/{len(batches)}: {n}ê°œ ì²˜ë¦¬, ëˆ„ì ={total_processed}, {elapsed:.1f}s\")\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                await save_checkpoint(done_ids, failed_ids, idx)\n",
    "                save_failed_ids(failed_ids)\n",
    "                print(f\"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (batch={idx})\")\n",
    "\n",
    "        await save_checkpoint(done_ids, failed_ids, len(batches))\n",
    "        save_failed_ids(failed_ids)\n",
    "\n",
    "    # JSONL â†’ Parquet\n",
    "    print(\"\\nğŸ“¦ JSONL â†’ Parquet ë³€í™˜ ì¤‘...\")\n",
    "\n",
    "    if not DETAILS_JSONL_PATH.exists():\n",
    "        print(\"âš ï¸ JSONL íŒŒì¼ì´ ì—†ì–´ Parquet ë³€í™˜ì„ ê±´ë„ˆëœ€\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_json(DETAILS_JSONL_PATH, lines=True)\n",
    "\n",
    "    df = df[df[\"imdb_id\"].notna()]\n",
    "    df = df[df[\"movie_id\"].notna()]\n",
    "    df = df.drop_duplicates(subset=[\"movie_id\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    cols = [\n",
    "        \"imdb_id\", \"movie_id\", \"vote_count\", \"title\",\n",
    "        \"director_ids\", \"directors\", \"director_gender\", \"director_profile_path\",\n",
    "        \"writer_ids\", \"writers\", \"writer_roles\", \"writer_gender\", \"writer_profile_path\",\n",
    "        \"top_cast_ids\", \"top_cast_order\", \"top_cast\", \"character\",\n",
    "        \"top_cast_gender\", \"top_cast_profile_path\",\n",
    "    ]\n",
    "\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"âš ï¸ ëˆ„ë½ ì»¬ëŸ¼ ìë™ ìƒì„±: {missing}\")\n",
    "        for c in missing:\n",
    "            df[c] = \"\"\n",
    "\n",
    "    df = df[cols]\n",
    "\n",
    "    df[\"movie_id\"]   = df[\"movie_id\"].astype(\"int32\")\n",
    "    df[\"vote_count\"] = df[\"vote_count\"].astype(\"int32\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col not in [\"movie_id\", \"vote_count\"]:\n",
    "            df[col] = df[col].astype(\"string\")\n",
    "\n",
    "    df.to_parquet(FINAL_PARQUET_PATH, index=False)\n",
    "\n",
    "    elapsed = (datetime.now() - start).total_seconds() / 60\n",
    "    print(f\"\\nğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ({elapsed:.1f}ë¶„)\")\n",
    "    print(f\"ğŸ“ ì €ì¥ íŒŒì¼: {FINAL_PARQUET_PATH}\")\n",
    "    print(f\"   ì´ ì˜í™” ìˆ˜: {len(df):,}í¸\")\n",
    "    print(f\"   ì‹¤íŒ¨ ì˜í™” ìˆ˜: {len(json.loads(FAILED_IDS_PATH.read_text())) if FAILED_IDS_PATH.exists() else 0:,}í¸\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì˜í™” í¬ë ˆë”§ ìˆ˜ì§‘ ì½”ë“œ (ì¸í‘œ, íŒŒì‹± í›„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\"accept\": \"application/json\"}\n",
    "\n",
    "def create_session():\n",
    "    \"\"\"ì¬ì‹œë„ ë¡œì§ê³¼ ì—°ê²° í’€ì´ ìˆëŠ” ì„¸ì…˜ ìƒì„±\"\"\"\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.3,\n",
    "        status_forcelist=[500, 502, 503, 504]\n",
    "    )\n",
    "    adapter = HTTPAdapter(\n",
    "        max_retries=retry,\n",
    "        pool_connections=50,\n",
    "        pool_maxsize=50\n",
    "    )\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "# ê¸°ë³¸ ì¤€ë¹„\n",
    "## í™˜ê²½ë³€ìˆ˜ì—ì„œ API_KEY ì½ì–´ì˜¤ê¸°\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"TMDB_API_KEY\")\n",
    "session = create_session()\n",
    "\n",
    "WRITER_JOBS = {\n",
    "    \"Writer\", \"Screenplay\", \"Story\", \"Creator\",\n",
    "    \"Novel\", \"Comic Book\", \"Adaptation\", \"Screenstory\"\n",
    "}\n",
    "\n",
    "def get_credit(movie_id):\n",
    "    url = f\"https://api.themoviedb.org/3/movie/{movie_id}/credits\"\n",
    "    params = {\n",
    "        \"api_key\": API_KEY,\n",
    "        \"language\": \"en-US\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, params=params, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching details for {movie_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Cast ë°ì´í„° ì²˜ë¦¬ (ì•ˆì „í•œ ë°©ì‹)\n",
    "    cast = data.get(\"cast\", [])\n",
    "    if cast:\n",
    "        cast_df = pd.DataFrame(cast)\n",
    "        # 'order' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "        if 'order' in cast_df.columns:\n",
    "            cast_df = cast_df.sort_values(by=[\"order\"])\n",
    "        cast_df = cast_df.head(5)\n",
    "\n",
    "        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ)\n",
    "        available_cols = ['id', 'gender', 'name', 'popularity', 'profile_path', 'character']\n",
    "        existing_cols = [col for col in available_cols if col in cast_df.columns]\n",
    "        cast_df = cast_df[existing_cols].rename(columns={'id': 'cast_id'})\n",
    "        cast_df['movie_id'] = movie_id\n",
    "    else:\n",
    "        # ë¹ˆ DataFrame ìƒì„± (ì»¬ëŸ¼ êµ¬ì¡° ìœ ì§€)\n",
    "        cast_df = pd.DataFrame(columns=['cast_id', 'gender', 'name', 'popularity',\n",
    "                                        'profile_path', 'character', 'movie_id'])\n",
    "\n",
    "    # Crew ë°ì´í„° ì²˜ë¦¬ (ì•ˆì „í•œ ë°©ì‹)\n",
    "    crew = data.get(\"crew\", [])\n",
    "    if crew:\n",
    "        crew_df = pd.DataFrame(crew)\n",
    "        available_cols = ['id', 'gender', 'name', 'popularity', 'profile_path', 'department', 'job']\n",
    "        existing_cols = [col for col in available_cols if col in crew_df.columns]\n",
    "        crew_df = crew_df[existing_cols].rename(columns={'id': 'crew_member_id'})\n",
    "        crew_df['movie_id'] = movie_id\n",
    "\n",
    "        # Writerì™€ Director í•„í„°ë§\n",
    "        if 'job' in crew_df.columns:\n",
    "            writer_df = crew_df[crew_df['job'].isin(WRITER_JOBS)]\n",
    "            director_df = crew_df[crew_df['job'] == 'Director']\n",
    "        else:\n",
    "            writer_df = pd.DataFrame(columns=crew_df.columns)\n",
    "            director_df = pd.DataFrame(columns=crew_df.columns)\n",
    "    else:\n",
    "        # ë¹ˆ DataFrame ìƒì„±\n",
    "        empty_cols = ['crew_member_id', 'gender', 'name', 'popularity',\n",
    "                     'profile_path', 'department', 'job', 'movie_id']\n",
    "        writer_df = pd.DataFrame(columns=empty_cols)\n",
    "        director_df = pd.DataFrame(columns=empty_cols)\n",
    "\n",
    "    return {\"cast\": cast_df, \"writer\": writer_df, \"director\": director_df}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    movies = pd.read_parquet('files/movies_filtered_f.parquet')\n",
    "    ids = movies['id'].tolist()  # Seriesë¥¼ listë¡œ ë³€í™˜í•˜ë©´ ë©”ëª¨ë¦¬ íš¨ìœ¨ì \n",
    "\n",
    "    all_cast_dfs = []\n",
    "    all_writer_dfs = []\n",
    "    all_director_dfs = []\n",
    "\n",
    "    # 3. ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ë‹¨ê³„\n",
    "    with ThreadPoolExecutor(max_workers=30) as executor:\n",
    "        futures = {executor.submit(get_credit, movie_id): movie_id for movie_id in ids}\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures),\n",
    "                          desc=\"ì˜í™” í¬ë ˆë”§ ìˆ˜ì§‘\", unit=\"movie\"):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    all_cast_dfs.append(result['cast'])\n",
    "                    all_writer_dfs.append(result['writer'])\n",
    "                    all_director_dfs.append(result['director'])\n",
    "            except Exception as e:\n",
    "                movie_id = futures[future]\n",
    "                print(f\"ì˜í™” ID {movie_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "    def safe_concat(df_list, name):\n",
    "        \"\"\"ë¹ˆ ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ë©´ì„œ DataFrameì„ ë³‘í•©\"\"\"\n",
    "        if df_list and not all(df.empty for df in df_list):\n",
    "            return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "        print(f\"ê²½ê³ : {name} ë°ì´í„°ê°€ ë¹„ì–´ ìˆê±°ë‚˜ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        # ì»¬ëŸ¼ êµ¬ì¡° ì •ì˜\n",
    "        column_schemas = {\n",
    "            'Cast': ['cast_id', 'gender', 'name', 'popularity', 'profile_path', 'character', 'movie_id'],\n",
    "            'Writer': ['crew_member_id', 'gender', 'name', 'popularity', 'profile_path', 'department', 'job', 'movie_id'],\n",
    "            'Director': ['crew_member_id', 'gender', 'name', 'popularity', 'profile_path', 'department', 'job', 'movie_id']\n",
    "        }\n",
    "\n",
    "        return pd.DataFrame(columns=column_schemas.get(name, []))\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    final_cast_df = safe_concat(all_cast_dfs, 'Cast')\n",
    "    final_cast_df.to_parquet('files/movies_cast.parquet', index=False)\n",
    "    print(f\"Cast ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(final_cast_df)} í–‰\")\n",
    "\n",
    "    final_writer_df = safe_concat(all_writer_dfs, 'Writer')\n",
    "    final_writer_df.to_parquet('files/movies_writer.parquet', index=False)\n",
    "    print(f\"Writer ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(final_writer_df)} í–‰\")\n",
    "\n",
    "    final_director_df = safe_concat(all_director_dfs, 'Director')\n",
    "    final_director_df.to_parquet('files/movies_director.parquet', index=False)\n",
    "    print(f\"Director ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(final_director_df)} í–‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë“œë¼ë§ˆ í¬ë ˆë”§ ìˆ˜ì§‘ ì½”ë“œ (ì˜ˆì›, íŒŒì‹± ì „)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TMDB TV Series Data Collector - Async Version with Environment Variables\n",
    "í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ API í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ëŠ” ê°œì„ ëœ ë²„ì „\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('tmdb_collector.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TMDBAsyncCollector:\n",
    "    def __init__(self, api_key: str = None, \n",
    "                 start_date: str = None, \n",
    "                 end_date: str = None,\n",
    "                 min_vote_count: int = None,\n",
    "                 max_workers: int = None):\n",
    "        # í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” íŒŒë¼ë¯¸í„°ì—ì„œ ì„¤ì • ë¡œë“œ\n",
    "        self.api_key = api_key or os.getenv('TMDB_API_KEY')\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"TMDB API key not found. Set TMDB_API_KEY in .env file or pass as parameter\")\n",
    "        \n",
    "        self.start_date = start_date or os.getenv('START_DATE', '2005-01-01')\n",
    "        self.end_date = end_date or os.getenv('END_DATE', '2025-11-30')\n",
    "        self.min_vote_count = min_vote_count or int(os.getenv('MIN_VOTE_COUNT', '30'))\n",
    "        \n",
    "        self.base_url = \"https://api.themoviedb.org/3\"\n",
    "        self.headers = {\"accept\": \"application/json\"}\n",
    "        \n",
    "        # Rate limiting\n",
    "        self.rate_limit = max_workers or int(os.getenv('MAX_WORKERS', '40'))\n",
    "        self.rate_period = 1.0\n",
    "        self.semaphore = asyncio.Semaphore(self.rate_limit)\n",
    "        \n",
    "        # íŒŒì¼ ê²½ë¡œ\n",
    "        self.checkpoint_file = \"checkpoint_async.json\"\n",
    "        self.output_file = \"tmdb_tv_series_data.parquet\"\n",
    "        \n",
    "        # ë°ì´í„° ì €ì¥\n",
    "        self.collected_data = []\n",
    "        \n",
    "        # í†µê³„\n",
    "        self.stats = {\n",
    "            'total_requests': 0,\n",
    "            'successful_requests': 0,\n",
    "            'failed_requests': 0,\n",
    "            'series_with_imdb': 0,\n",
    "            'series_without_imdb': 0,\n",
    "            'start_time': None,\n",
    "            'end_time': None\n",
    "        }\n",
    "        \n",
    "    async def _rate_limited_request(self, session: aiohttp.ClientSession, url: str, params: dict) -> Optional[dict]:\n",
    "        \"\"\"Rate limitingì„ ì ìš©í•œ API ìš”ì²­\"\"\"\n",
    "        async with self.semaphore:\n",
    "            self.stats['total_requests'] += 1\n",
    "            \n",
    "            try:\n",
    "                async with session.get(url, params=params, headers=self.headers, timeout=aiohttp.ClientTimeout(total=30)) as response:\n",
    "                    if response.status == 200:\n",
    "                        self.stats['successful_requests'] += 1\n",
    "                        return await response.json()\n",
    "                    elif response.status == 429:  # Too Many Requests\n",
    "                        retry_after = int(response.headers.get('Retry-After', 1))\n",
    "                        logger.warning(f\"Rate limited. Waiting {retry_after}s\")\n",
    "                        await asyncio.sleep(retry_after)\n",
    "                        return await self._rate_limited_request(session, url, params)\n",
    "                    else:\n",
    "                        self.stats['failed_requests'] += 1\n",
    "                        logger.error(f\"Error {response.status}: {url}\")\n",
    "                        return None\n",
    "            except asyncio.TimeoutError:\n",
    "                self.stats['failed_requests'] += 1\n",
    "                logger.error(f\"Timeout: {url}\")\n",
    "                return None\n",
    "            except Exception as e:\n",
    "                self.stats['failed_requests'] += 1\n",
    "                logger.error(f\"Request failed: {e}\")\n",
    "                return None\n",
    "            finally:\n",
    "                await asyncio.sleep(1.0 / self.rate_limit)\n",
    "    \n",
    "    async def fetch_discover_page(self, session: aiohttp.ClientSession, \n",
    "                                  start_date: str, end_date: str, page: int) -> Optional[dict]:\n",
    "        \"\"\"Discover APIë¡œ TV ì‹œë¦¬ì¦ˆ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
    "        url = f\"{self.base_url}/discover/tv\"\n",
    "        params = {\n",
    "            \"api_key\": self.api_key,\n",
    "            \"first_air_date.gte\": start_date,\n",
    "            \"first_air_date.lte\": end_date,\n",
    "            \"vote_count.gte\": self.min_vote_count,\n",
    "            \"include_adult\": \"false\",\n",
    "            \"include_null_first_air_dates\": \"false\",\n",
    "            \"language\": \"en-US\",\n",
    "            \"page\": page,\n",
    "            \"sort_by\": \"first_air_date.asc\"\n",
    "        }\n",
    "        return await self._rate_limited_request(session, url, params)\n",
    "    \n",
    "    async def fetch_external_ids(self, session: aiohttp.ClientSession, series_id: int) -> Optional[dict]:\n",
    "        \"\"\"External IDs (IMDB ID í¬í•¨) ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
    "        url = f\"{self.base_url}/tv/{series_id}/external_ids\"\n",
    "        params = {\"api_key\": self.api_key}\n",
    "        return await self._rate_limited_request(session, url, params)\n",
    "    \n",
    "    async def fetch_credits(self, session: aiohttp.ClientSession, series_id: int) -> Optional[dict]:\n",
    "        \"\"\"í¬ë ˆë”§ ì •ë³´ (ë°°ìš°, ì œì‘ì§„) ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
    "        url = f\"{self.base_url}/tv/{series_id}/credits\"\n",
    "        params = {\n",
    "            \"api_key\": self.api_key,\n",
    "            \"language\": \"en-US\"\n",
    "        }\n",
    "        return await self._rate_limited_request(session, url, params)\n",
    "    \n",
    "    def extract_executive_producers(self, crew_list: List[dict]) -> Dict[str, str]:\n",
    "        \"\"\"ì´ê´„ í”„ë¡œë“€ì„œ ì •ë³´ ì¶”ì¶œ\"\"\"\n",
    "        producers = [c for c in crew_list if c.get(\"job\") == \"Executive Producer\"]\n",
    "        \n",
    "        if not producers:\n",
    "            return {\n",
    "                \"executive_producer_name\": \"\",\n",
    "                \"executive_producer_ids\": \"\",\n",
    "                \"executive_producer_gender\": \"\",\n",
    "                \"executive_producer_profile_path\": \"\"\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"executive_producer_name\": \"; \".join([p.get(\"name\", \"\") for p in producers]),\n",
    "            \"executive_producer_ids\": \"; \".join([str(p.get(\"id\", \"\")) for p in producers]),\n",
    "            \"executive_producer_gender\": \"; \".join([str(p.get(\"gender\", \"\")) for p in producers]),\n",
    "            \"executive_producer_profile_path\": \"; \".join([p.get(\"profile_path\", \"\") or \"\" for p in producers])\n",
    "        }\n",
    "    \n",
    "    def extract_writers(self, crew_list: List[dict]) -> Dict[str, str]:\n",
    "        \"\"\"ì‘ê°€ ì •ë³´ ì¶”ì¶œ\"\"\"\n",
    "        writer_jobs = {\"Writer\", \"Screenplay\", \"Story\", \"Creator\", \"Novel\", \"Comic Book\"}\n",
    "        writers = [\n",
    "            c for c in crew_list\n",
    "            if c.get(\"job\") in writer_jobs or c.get(\"department\") == \"Writing\"\n",
    "        ]\n",
    "        \n",
    "        if not writers:\n",
    "            return {\n",
    "                \"writers_name\": \"\",\n",
    "                \"writer_roles\": \"\",\n",
    "                \"writer_ids\": \"\",\n",
    "                \"writer_gender\": \"\",\n",
    "                \"writer_profile_path\": \"\"\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"writers_name\": \"; \".join([w.get(\"name\", \"\") for w in writers]),\n",
    "            \"writer_roles\": \"; \".join([w.get(\"job\", \"\") for w in writers]),\n",
    "            \"writer_ids\": \"; \".join([str(w.get(\"id\", \"\")) for w in writers]),\n",
    "            \"writer_gender\": \"; \".join([str(w.get(\"gender\", \"\")) for w in writers]),\n",
    "            \"writer_profile_path\": \"; \".join([w.get(\"profile_path\", \"\") or \"\" for w in writers])\n",
    "        }\n",
    "    \n",
    "    def extract_top_cast(self, cast_list: List[dict]) -> Dict[str, str]:\n",
    "        \"\"\"ì£¼ì—° ë°°ìš° ìƒìœ„ 5ëª… ì •ë³´ ì¶”ì¶œ (order ê¸°ì¤€)\"\"\"\n",
    "        sorted_cast = sorted(cast_list, key=lambda x: x.get(\"order\", 999))\n",
    "        top_cast = [c for c in sorted_cast if c.get(\"character\")][:5]\n",
    "        \n",
    "        if not top_cast:\n",
    "            return {\n",
    "                \"top_cast_order\": \"\",\n",
    "                \"top_cast\": \"\",\n",
    "                \"character\": \"\",\n",
    "                \"top_cast_ids\": \"\",\n",
    "                \"top_cast_gender\": \"\",\n",
    "                \"top_cast_profile_path\": \"\"\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"top_cast_order\": \"; \".join([str(c.get(\"order\", \"\")) for c in top_cast]),\n",
    "            \"top_cast\": \"; \".join([c.get(\"name\", \"\") for c in top_cast]),\n",
    "            \"character\": \"; \".join([c.get(\"character\", \"\") for c in top_cast]),\n",
    "            \"top_cast_ids\": \"; \".join([str(c.get(\"id\", \"\")) for c in top_cast]),\n",
    "            \"top_cast_gender\": \"; \".join([str(c.get(\"gender\", \"\")) for c in top_cast]),\n",
    "            \"top_cast_profile_path\": \"; \".join([c.get(\"profile_path\", \"\") or \"\" for c in top_cast])\n",
    "        }\n",
    "    \n",
    "    async def process_series(self, session: aiohttp.ClientSession, series_data: dict) -> Optional[dict]:\n",
    "        \"\"\"ê°œë³„ TV ì‹œë¦¬ì¦ˆ ì²˜ë¦¬\"\"\"\n",
    "        series_id = series_data.get(\"id\")\n",
    "        \n",
    "        # External IDs ê°€ì ¸ì˜¤ê¸°\n",
    "        external_ids = await self.fetch_external_ids(session, series_id)\n",
    "        if not external_ids or not external_ids.get(\"imdb_id\"):\n",
    "            self.stats['series_without_imdb'] += 1\n",
    "            logger.debug(f\"Series {series_id} has no IMDB ID, skipping\")\n",
    "            return None\n",
    "        \n",
    "        self.stats['series_with_imdb'] += 1\n",
    "        \n",
    "        # Credits ê°€ì ¸ì˜¤ê¸°\n",
    "        credits = await self.fetch_credits(session, series_id)\n",
    "        if not credits:\n",
    "            logger.debug(f\"Failed to fetch credits for series {series_id}\")\n",
    "            return None\n",
    "        \n",
    "        # ë°ì´í„° ì¶”ì¶œ\n",
    "        cast_list = credits.get(\"cast\", [])\n",
    "        crew_list = credits.get(\"crew\", [])\n",
    "        \n",
    "        row_data = {\n",
    "            \"imdb_id\": external_ids.get(\"imdb_id\"),\n",
    "            \"series_id\": series_id,\n",
    "            \"title\": series_data.get(\"name\", \"\"),\n",
    "            \"original_name\": series_data.get(\"original_name\", \"\"),\n",
    "            \"first_air_date\": series_data.get(\"first_air_date\", \"\"),\n",
    "            \"vote_average\": series_data.get(\"vote_average\", 0),\n",
    "            \"vote_count\": series_data.get(\"vote_count\", 0),\n",
    "            \"popularity\": series_data.get(\"popularity\", 0),\n",
    "        }\n",
    "        \n",
    "        # ì´ê´„ í”„ë¡œë“€ì„œ, ì‘ê°€, ì£¼ì—° ë°°ìš° ì •ë³´ ì¶”ê°€\n",
    "        row_data.update(self.extract_executive_producers(crew_list))\n",
    "        row_data.update(self.extract_writers(crew_list))\n",
    "        row_data.update(self.extract_top_cast(cast_list))\n",
    "        \n",
    "        return row_data\n",
    "    \n",
    "    async def collect_date_range(self, session: aiohttp.ClientSession, \n",
    "                                start_date: str, end_date: str) -> None:\n",
    "        \"\"\"íŠ¹ì • ë‚ ì§œ ë²”ìœ„ì˜ ë°ì´í„° ìˆ˜ì§‘\"\"\"\n",
    "        logger.info(f\"Collecting: {start_date} to {end_date}\")\n",
    "        page = 1\n",
    "        \n",
    "        while page <= 500:  # TMDB 500í˜ì´ì§€ ì œí•œ\n",
    "            result = await self.fetch_discover_page(session, start_date, end_date, page)\n",
    "            \n",
    "            if not result or \"results\" not in result:\n",
    "                break\n",
    "            \n",
    "            results = result[\"results\"]\n",
    "            if not results:\n",
    "                break\n",
    "            \n",
    "            total_pages = min(result.get(\"total_pages\", 0), 500)\n",
    "            logger.info(f\"Processing page {page}/{total_pages} ({start_date} ~ {end_date})\")\n",
    "            \n",
    "            # ê° ì‹œë¦¬ì¦ˆ ì²˜ë¦¬\n",
    "            tasks = [self.process_series(session, series) for series in results]\n",
    "            processed_data = await asyncio.gather(*tasks)\n",
    "            \n",
    "            # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì¶”ê°€\n",
    "            valid_data = [d for d in processed_data if d is not None]\n",
    "            self.collected_data.extend(valid_data)\n",
    "            \n",
    "            logger.info(f\"Collected {len(valid_data)} valid series from this page (Total: {len(self.collected_data)})\")\n",
    "            \n",
    "            if page >= total_pages:\n",
    "                break\n",
    "            \n",
    "            page += 1\n",
    "        \n",
    "        logger.info(f\"Completed range {start_date} ~ {end_date}\")\n",
    "    \n",
    "    def generate_date_ranges(self, start_date: str, end_date: str, months: int = 3) -> List[tuple]:\n",
    "        \"\"\"ë‚ ì§œ ë²”ìœ„ë¥¼ ì—¬ëŸ¬ êµ¬ê°„ìœ¼ë¡œ ë¶„í•  (500í˜ì´ì§€ ì œí•œ ëŒ€ì‘)\"\"\"\n",
    "        start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "        \n",
    "        ranges = []\n",
    "        current = start\n",
    "        \n",
    "        while current < end:\n",
    "            next_date = current + timedelta(days=months * 30)\n",
    "            if next_date > end:\n",
    "                next_date = end\n",
    "            \n",
    "            ranges.append((\n",
    "                current.strftime(\"%Y-%m-%d\"),\n",
    "                next_date.strftime(\"%Y-%m-%d\")\n",
    "            ))\n",
    "            current = next_date + timedelta(days=1)\n",
    "        \n",
    "        return ranges\n",
    "    \n",
    "    def save_checkpoint(self, completed_ranges: List[tuple]) -> None:\n",
    "        \"\"\"ì§„í–‰ ìƒí™© ì €ì¥\"\"\"\n",
    "        checkpoint = {\n",
    "            \"completed_ranges\": completed_ranges,\n",
    "            \"collected_count\": len(self.collected_data),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"stats\": self.stats\n",
    "        }\n",
    "        with open(self.checkpoint_file, \"w\") as f:\n",
    "            json.dump(checkpoint, f, indent=2)\n",
    "        logger.info(f\"Checkpoint saved: {len(completed_ranges)} ranges completed\")\n",
    "    \n",
    "    def load_checkpoint(self) -> List[tuple]:\n",
    "        \"\"\"ì €ì¥ëœ ì§„í–‰ ìƒí™© ë¶ˆëŸ¬ì˜¤ê¸°\"\"\"\n",
    "        if not Path(self.checkpoint_file).exists():\n",
    "            return []\n",
    "        \n",
    "        with open(self.checkpoint_file, \"r\") as f:\n",
    "            checkpoint = json.load(f)\n",
    "            logger.info(f\"Loaded checkpoint: {checkpoint['collected_count']} series previously collected\")\n",
    "            return [tuple(r) for r in checkpoint.get(\"completed_ranges\", [])]\n",
    "    \n",
    "    def save_to_parquet(self) -> None:\n",
    "        \"\"\"ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ Parquet íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        if not self.collected_data:\n",
    "            logger.warning(\"No data to save\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.collected_data)\n",
    "        df.to_parquet(self.output_file, index=False, engine='pyarrow')\n",
    "        logger.info(f\"Data saved to {self.output_file}: {len(df)} series\")\n",
    "    \n",
    "    def print_statistics(self) -> None:\n",
    "        \"\"\"ìˆ˜ì§‘ í†µê³„ ì¶œë ¥\"\"\"\n",
    "        logger.info(\"\\n\" + \"=\"*60)\n",
    "        logger.info(\"Collection Statistics\")\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(f\"Total Requests: {self.stats['total_requests']}\")\n",
    "        logger.info(f\"Successful Requests: {self.stats['successful_requests']}\")\n",
    "        logger.info(f\"Failed Requests: {self.stats['failed_requests']}\")\n",
    "        logger.info(f\"Series with IMDB ID: {self.stats['series_with_imdb']}\")\n",
    "        logger.info(f\"Series without IMDB ID: {self.stats['series_without_imdb']}\")\n",
    "        logger.info(f\"Final Collected Series: {len(self.collected_data)}\")\n",
    "        \n",
    "        if self.stats['start_time'] and self.stats['end_time']:\n",
    "            duration = self.stats['end_time'] - self.stats['start_time']\n",
    "            logger.info(f\"Total Duration: {duration:.2f} seconds ({duration/60:.2f} minutes)\")\n",
    "            if self.stats['total_requests'] > 0:\n",
    "                logger.info(f\"Average Request Rate: {self.stats['total_requests']/duration:.2f} req/s\")\n",
    "        \n",
    "        logger.info(\"=\"*60)\n",
    "    \n",
    "    async def run(self) -> None:\n",
    "        \"\"\"ë©”ì¸ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤\"\"\"\n",
    "        self.stats['start_time'] = time.time()\n",
    "        \n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(\"TMDB TV Series Data Collector - Async Version\")\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(f\"Period: {self.start_date} ~ {self.end_date}\")\n",
    "        logger.info(f\"Min Vote Count: {self.min_vote_count}\")\n",
    "        logger.info(f\"Rate Limit: {self.rate_limit} req/s\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        # ë‚ ì§œ ë²”ìœ„ ìƒì„±\n",
    "        all_ranges = self.generate_date_ranges(self.start_date, self.end_date, months=3)\n",
    "        completed_ranges = self.load_checkpoint()\n",
    "        \n",
    "        # ì•„ì§ ìˆ˜ì§‘í•˜ì§€ ì•Šì€ ë²”ìœ„ í•„í„°ë§\n",
    "        remaining_ranges = [r for r in all_ranges if r not in completed_ranges]\n",
    "        logger.info(f\"Total ranges: {len(all_ranges)}, Remaining: {len(remaining_ranges)}\")\n",
    "        \n",
    "        if not remaining_ranges:\n",
    "            logger.info(\"All ranges already collected!\")\n",
    "            self.stats['end_time'] = time.time()\n",
    "            self.print_statistics()\n",
    "            return\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for i, (range_start, range_end) in enumerate(remaining_ranges, 1):\n",
    "                logger.info(f\"\\n{'='*60}\")\n",
    "                logger.info(f\"Range {i}/{len(remaining_ranges)}\")\n",
    "                \n",
    "                await self.collect_date_range(session, range_start, range_end)\n",
    "                \n",
    "                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "                completed_ranges.append((range_start, range_end))\n",
    "                self.save_checkpoint(completed_ranges)\n",
    "                \n",
    "                # ì¤‘ê°„ ì €ì¥ (ë§¤ 5ê°œ ë²”ìœ„ë§ˆë‹¤)\n",
    "                if i % 5 == 0:\n",
    "                    self.save_to_parquet()\n",
    "        \n",
    "        # ìµœì¢… ì €ì¥\n",
    "        self.save_to_parquet()\n",
    "        \n",
    "        self.stats['end_time'] = time.time()\n",
    "        self.print_statistics()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    try:\n",
    "        collector = TMDBAsyncCollector()\n",
    "        await collector.run()\n",
    "    except ValueError as e:\n",
    "        logger.error(f\"Configuration error: {e}\")\n",
    "        logger.error(\"Please create a .env file with your TMDB_API_KEY\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
