import pandas as pd
import aiohttp
import asyncio
from bs4 import BeautifulSoup
import time
import re
from tqdm.asyncio import tqdm
import random
from datetime import datetime

# ============================================================
# ì„¤ì •
# ============================================================
# ë™ì‹œ ìš”ì²­ ìˆ˜ (ë„ˆë¬´ ë†’ìœ¼ë©´ ì°¨ë‹¨ë  ìˆ˜ ìˆìŒ)
MAX_CONCURRENT_REQUESTS = 15  # 10-20ì´ ì ì •

# Rate Limiting (ì´ˆë‹¹ ìš”ì²­ ìˆ˜)
MAX_REQUESTS_PER_SECOND = 5  # 3-5ê°€ ì•ˆì „

# íƒ€ì„ì•„ì›ƒ
TIMEOUT = aiohttp.ClientTimeout(total=30, connect=10)

# ì¬ì‹œë„
MAX_RETRIES = 3


# ============================================================
# Rate Limiter
# ============================================================
class RateLimiter:
    """ì´ˆë‹¹ ìš”ì²­ ìˆ˜ë¥¼ ì œí•œí•˜ëŠ” Rate Limiter"""
    def __init__(self, rate):
        self.rate = rate
        self.tokens = rate
        self.updated_at = time.monotonic()
        self.lock = asyncio.Lock()

    async def acquire(self):
        async with self.lock:
            now = time.monotonic()
            elapsed = now - self.updated_at
            
            # í† í° ë³´ì¶©
            self.tokens = min(self.rate, self.tokens + elapsed * self.rate)
            self.updated_at = now

            # í† í°ì´ ë¶€ì¡±í•˜ë©´ ëŒ€ê¸°
            if self.tokens < 1:
                sleep_time = (1 - self.tokens) / self.rate
                await asyncio.sleep(sleep_time)
                self.tokens = 1

            self.tokens -= 1


rate_limiter = RateLimiter(MAX_REQUESTS_PER_SECOND)


# ============================================================
# ë¹„ë™ê¸° ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜
# ============================================================
async def get_imdb_data_async(session, imdb_id, semaphore, max_retries=MAX_RETRIES):
    """
    ë¹„ë™ê¸°ë¡œ IMDB ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        session: aiohttp ClientSession
        imdb_id: IMDB ID
        semaphore: ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œìš© ì„¸ë§ˆí¬ì–´
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    """
    url = f"https://www.imdb.com/title/{imdb_id}/"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive',
    }
    
    async with semaphore:  # ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ
        for attempt in range(max_retries):
            try:
                # Rate limiting
                await rate_limiter.acquire()
                
                async with session.get(url, headers=headers) as response:
                    response.raise_for_status()
                    html = await response.text()
                    
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # IMDB Rating ì¶”ì¶œ (ì—¬ëŸ¬ ê°€ëŠ¥í•œ í´ë˜ìŠ¤ ì‹œë„)
                    imdb_rating = None
                    
                    # ë°©ë²• 1: íŠ¹ì • í´ë˜ìŠ¤
                    rating_span = soup.find('span', class_='sc-4dc495c1-1')
                    if rating_span:
                        imdb_rating = rating_span.text.strip()
                    
                    # ë°©ë²• 2: ë‹¤ë¥¸ ê°€ëŠ¥í•œ í´ë˜ìŠ¤
                    if not imdb_rating:
                        rating_span = soup.find('span', {'data-testid': 'rating-value'})
                        if rating_span:
                            imdb_rating = rating_span.text.strip()
                    
                    # ë°©ë²• 3: ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì°¾ê¸°
                    if not imdb_rating:
                        rating_pattern = re.compile(r'(\d+\.\d+)/10')
                        match = rating_pattern.search(html)
                        if match:
                            imdb_rating = match.group(1)
                    
                    # Metascore ì¶”ì¶œ
                    metascore = None
                    
                    # ë°©ë²• 1: íŠ¹ì • í´ë˜ìŠ¤
                    metascore_span = soup.find('span', class_='sc-9fe7b0ef-0')
                    if metascore_span:
                        metascore = metascore_span.text.strip()
                    
                    # ë°©ë²• 2: metacritic í‚¤ì›Œë“œ ê²€ìƒ‰
                    if not metascore:
                        metascore_span = soup.find('span', class_=re.compile('metacritic-score'))
                        if metascore_span:
                            metascore = metascore_span.text.strip()
                    
                    return {
                        'imdb_id': imdb_id,
                        'imdb_rating': imdb_rating,
                        'metascore': metascore,
                        'status': 'success',
                        'url': url
                    }
                    
            except aiohttp.ClientError as e:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    return {
                        'imdb_id': imdb_id,
                        'imdb_rating': None,
                        'metascore': None,
                        'status': f'error: {str(e)}',
                        'url': url
                    }
            except asyncio.TimeoutError:
                if attempt < max_retries - 1:
                    await asyncio.sleep((attempt + 1) * 2)
                    continue
                else:
                    return {
                        'imdb_id': imdb_id,
                        'imdb_rating': None,
                        'metascore': None,
                        'status': 'error: timeout',
                        'url': url
                    }
            except Exception as e:
                return {
                    'imdb_id': imdb_id,
                    'imdb_rating': None,
                    'metascore': None,
                    'status': f'parsing error: {str(e)}',
                    'url': url
                }
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        return {
            'imdb_id': imdb_id,
            'imdb_rating': None,
            'metascore': None,
            'status': 'error: max retries exceeded',
            'url': url
        }


# ============================================================
# ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================
async def process_batch(session, batch_data, semaphore, pbar):
    """
    ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        session: aiohttp ClientSession
        batch_data: ì²˜ë¦¬í•  ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        semaphore: ì„¸ë§ˆí¬ì–´
        pbar: ì§„í–‰ í‘œì‹œì¤„
    """
    tasks = []
    for row_data in batch_data:
        imdb_id = row_data['imdb_id']
        task = get_imdb_data_async(session, imdb_id, semaphore)
        tasks.append(task)
    
    # ëª¨ë“  íƒœìŠ¤í¬ë¥¼ ë™ì‹œì— ì‹¤í–‰
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # ê²°ê³¼ ì²˜ë¦¬
    processed_results = []
    for row_data, scraping_result in zip(batch_data, results):
        if isinstance(scraping_result, dict):
            # ì›ë³¸ ë°ì´í„°ì™€ ë³‘í•©
            result = {
                'imdb_id': row_data['imdb_id'],
                'series_name': row_data.get('name', ''),
                'original_vote_count': row_data.get('vote_count', ''),
                'imdb_rating': scraping_result['imdb_rating'],
                'metascore': scraping_result['metascore'],
                'url': scraping_result['url'],
                'status': scraping_result['status']
            }
            processed_results.append(result)
            pbar.update(1)
        else:
            # ì˜ˆì™¸ ë°œìƒ
            result = {
                'imdb_id': row_data['imdb_id'],
                'series_name': row_data.get('name', ''),
                'original_vote_count': row_data.get('vote_count', ''),
                'imdb_rating': None,
                'metascore': None,
                'url': f"https://www.imdb.com/title/{row_data['imdb_id']}/",
                'status': f'exception: {str(scraping_result)}'
            }
            processed_results.append(result)
            pbar.update(1)
    
    return processed_results


# ============================================================
# ë©”ì¸ ë¹„ë™ê¸° í•¨ìˆ˜
# ============================================================
async def main_async(df_filtered):
    """
    ë©”ì¸ ë¹„ë™ê¸° ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜
    
    Args:
        df_filtered: í•„í„°ë§ëœ DataFrame
    """
    print()
    print("ğŸš€ ë¹„ë™ê¸° ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    print(f"âš™ï¸  ë™ì‹œ ìš”ì²­ ìˆ˜: {MAX_CONCURRENT_REQUESTS}")
    print(f"âš™ï¸  ì´ˆë‹¹ ìš”ì²­ ìˆ˜: {MAX_REQUESTS_PER_SECOND}")
    print("-" * 60)
    
    # ì„¸ë§ˆí¬ì–´ ìƒì„± (ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ)
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
    
    # DataFrameì„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    data_list = df_filtered.to_dict('records')
    
    # ë°°ì¹˜ í¬ê¸° (ì¤‘ê°„ ì €ì¥ ë‹¨ìœ„)
    batch_size = 100
    
    # ê²°ê³¼ ì €ì¥
    all_results = []
    
    # Connection poolingì„ ìœ„í•œ ì»¤ë„¥í„° ì„¤ì •
    connector = aiohttp.TCPConnector(
        limit=MAX_CONCURRENT_REQUESTS * 2,  # ì»¤ë„¥ì…˜ í’€ í¬ê¸°
        limit_per_host=MAX_CONCURRENT_REQUESTS,
        force_close=False,  # ì»¤ë„¥ì…˜ ì¬ì‚¬ìš©
        enable_cleanup_closed=True
    )
    
    # aiohttp ClientSession ìƒì„±
    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as session:
        # ì§„í–‰ í‘œì‹œì¤„
        with tqdm(total=len(data_list), desc="ì§„í–‰ ìƒí™©", unit="ê°œ") as pbar:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for i in range(0, len(data_list), batch_size):
                batch = data_list[i:i + batch_size]
                
                # ë°°ì¹˜ ì²˜ë¦¬
                batch_results = await process_batch(session, batch, semaphore, pbar)
                all_results.extend(batch_results)
                
                # ì¤‘ê°„ ì €ì¥
                if len(all_results) % batch_size == 0:
                    temp_df = pd.DataFrame(all_results)
                    temp_df.to_csv('imdb_scraping_temp.csv', index=False, encoding='utf-8-sig')
    
    return all_results


# ============================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================
def main():
    print("=" * 60)
    print("IMDB Rating & Metascore ë¹„ë™ê¸° ìŠ¤í¬ë˜í•‘")
    print("âš¡ ë™ê¸° ë°©ì‹ë³´ë‹¤ 5-10ë°° ë¹ ë¦…ë‹ˆë‹¤!")
    print("=" * 60)
    print()
    
    # ì‹œì‘ ì‹œê°„
    start_time = datetime.now()
    
    # CSV íŒŒì¼ ì½ê¸°
    try:
        df_series = pd.read_csv("tv_series_2005_2015_FULL.csv")
        df_seasons = pd.read_csv("tv_seasons_2005_2015_FULL.csv")
        print(f"âœ“ CSV íŒŒì¼ ë¡œë”© ì™„ë£Œ")
        print(f"  - ì „ì²´ ì‹œë¦¬ì¦ˆ: {len(df_series)}ê°œ")
    except FileNotFoundError as e:
        print(f"âœ— CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return None
    
    # ì¡°ê±´ì— ë§ëŠ” ë°ì´í„° í•„í„°ë§
    df_filtered = df_series[(df_series['vote_count'] >= 30) & (df_series['imdb_id'].notna())]
    
    print(f"âœ“ í•„í„°ë§ ì™„ë£Œ (vote_count >= 30 & imdb_id ì¡´ì¬)")
    print(f"  - í•„í„°ë§ëœ ì‹œë¦¬ì¦ˆ: {len(df_filtered)}ê°œ")
    print()
    
    # ì˜ˆìƒ ì†Œìš” ì‹œê°„ ê³„ì‚° (ë¹„ë™ê¸° ë°©ì‹)
    # ë™ê¸°: 1.5ì´ˆ * Nê°œ
    # ë¹„ë™ê¸°: (N / ë™ì‹œìš”ì²­ìˆ˜) / ì´ˆë‹¹ìš”ì²­ìˆ˜
    estimated_time_sync = len(df_filtered) * 1.5 / 60
    estimated_time_async = (len(df_filtered) / MAX_CONCURRENT_REQUESTS) / MAX_REQUESTS_PER_SECOND / 60
    
    print(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„:")
    print(f"  - ë™ê¸° ë°©ì‹: ì•½ {estimated_time_sync:.1f}ë¶„")
    print(f"  - ë¹„ë™ê¸° ë°©ì‹: ì•½ {estimated_time_async:.1f}ë¶„ âš¡")
    print(f"  - ì†ë„ í–¥ìƒ: ì•½ {estimated_time_sync/estimated_time_async:.1f}ë°° ë¹ ë¦„!")
    print()
    
    response = input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
    if response.lower() != 'y':
        print("ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return None
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    try:
        results = asyncio.run(main_async(df_filtered))
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return None
    
    print()
    print("-" * 60)
    
    # ìµœì¢… ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    result_df = pd.DataFrame(results)
    
    # CSV ì €ì¥
    output_file = 'imdb_ratings_metascores_async.csv'
    result_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    
    # ì¢…ë£Œ ì‹œê°„
    end_time = datetime.now()
    elapsed = (end_time - start_time).total_seconds()
    
    print()
    print("=" * 60)
    print("ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
    print("=" * 60)
    print()
    print(f"âœ“ ì´ {len(result_df)}ê°œì˜ ì‹œë¦¬ì¦ˆ ì²˜ë¦¬")
    print(f"âœ“ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
    print(f"âœ“ í‰ê·  ì†ë„: {len(result_df)/elapsed:.1f}ê°œ/ì´ˆ")
    print(f"âœ“ ê²°ê³¼ íŒŒì¼: {output_file}")
    print()
    
    # ìƒì„¸ í†µê³„
    success_count = len(result_df[result_df['status'] == 'success'])
    rating_count = result_df['imdb_rating'].notna().sum()
    metascore_count = result_df['metascore'].notna().sum()
    
    print("=" * 60)
    print("í†µê³„")
    print("=" * 60)
    print(f"ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë¨:    {success_count:4d}ê°œ ({success_count/len(result_df)*100:.1f}%)")
    print(f"IMDB Rating ìˆìŒ:     {rating_count:4d}ê°œ ({rating_count/len(result_df)*100:.1f}%)")
    print(f"Metascore ìˆìŒ:       {metascore_count:4d}ê°œ ({metascore_count/len(result_df)*100:.1f}%)")
    print(f"ë‘˜ ë‹¤ ìˆìŒ:           {result_df[(result_df['imdb_rating'].notna()) & (result_df['metascore'].notna())].shape[0]:4d}ê°œ")
    print()
    
    # ì—ëŸ¬ ë¶„ì„
    error_df = result_df[result_df['status'] != 'success']
    if len(error_df) > 0:
        print(f"âš  ì—ëŸ¬ ë°œìƒ:          {len(error_df):4d}ê°œ")
        print("\nì—ëŸ¬ ìœ í˜•:")
        for status in error_df['status'].value_counts().head(5).items():
            print(f"  - {status[0][:50]}: {status[1]}ê°œ")
    
    print()
    print("=" * 60)
    print("âš¡ ì„±ëŠ¥ ë¹„êµ")
    print("=" * 60)
    sync_time = len(result_df) * 1.5
    speedup = sync_time / elapsed if elapsed > 0 else 0
    print(f"ë™ê¸° ë°©ì‹ ì˜ˆìƒ ì‹œê°„: {sync_time:.1f}ì´ˆ ({sync_time/60:.1f}ë¶„)")
    print(f"ë¹„ë™ê¸° ë°©ì‹ ì‹¤ì œ ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
    print(f"ì†ë„ í–¥ìƒ: {speedup:.1f}ë°° âš¡âš¡âš¡")
    
    return result_df


if __name__ == "__main__":
    result_df = main()
    
    if result_df is not None:
        print()
        print("=" * 60)
        print("ìƒ˜í”Œ ê²°ê³¼ (ì²˜ìŒ 10ê°œ)")
        print("=" * 60)
        print(result_df[['imdb_id', 'series_name', 'imdb_rating', 'metascore', 'status']].head(10).to_string())
