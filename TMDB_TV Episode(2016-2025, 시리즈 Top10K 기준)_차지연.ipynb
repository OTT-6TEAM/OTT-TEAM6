{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMDB APIë¡œ ë°ì´í„° ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TMDB TV Series & Episode ë°ì´í„° ìˆ˜ì§‘ (Top 10,000 ì‹œë¦¬ì¦ˆ ê¸°ì¤€)\n",
    "\n",
    "1. ìˆ˜ì§‘ ëª©ì \n",
    "  - ê¸°ì¡´ TMDB TV Series ë°ì´í„°ëŠ” series-level ì •ë³´ê°€ ë¶ˆì™„ì „í•˜ì—¬ ê²°ì¸¡ì¹˜ê°€ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ë§ìŒ (ì˜ˆ: episode_runtime, directorial credit, writers, detailed cast, season/episode granularity ë“±)\n",
    "  - ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ episode-level ë°ì´í„°ë¥¼ ì¶”ê°€ ìˆ˜ì§‘í•˜ì—¬ êµ¬ì¡°Â·ì—°ì¶œÂ·ì„œì‚¬ ë¶„ì„ì´ ê°€ëŠ¥í•œ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ê³ ì í•¨\n",
    "2. ìˆ˜ì§‘ ëŒ€ìƒ: TMDB TV Series(ì—í”¼ì†Œë“œ í¬í•¨) ë°ì´í„°\n",
    "3. ìˆ˜ì§‘ ê¸°ê°„: 2016ë…„ 1ì›” 1ì¼ ~ 2025ë…„ 11ì›” 29ì¼ (ì•½ 11ë…„ì¹˜)\n",
    "4. ìˆ˜ì§‘ ë²”ìœ„: vote_count ê¸°ì¤€ ìƒìœ„ 10,000ê°œ ì—í”¼ì†Œë“œ ë°ì´í„°\n",
    "  - Series-level : ì‹œë¦¬ì¦ˆ ë©”íƒ€ë°ì´í„°ëŠ” ì „ìˆ˜ ìˆ˜ì§‘ (Discover ê¸°ì¤€ ëª¨ë“  TV Series IDë¥¼ ìˆ˜ì§‘)\n",
    "  - Episode-level : ì—í”¼ì†Œë“œ ë°ì´í„°ëŠ” ìˆ˜ì§‘ íš¨ìœ¨ì„ ê³ ë ¤í•´ ì‹¤ì§ˆì  ë¶„ì„ ê°€ì¹˜ê°€ ë†’ì€ ì‹œë¦¬ì¦ˆ(Top 10,000)ì— í•œí•´ ì „ì²´ ì—í”¼ì†Œë“œ ìˆ˜ì§‘ (vote_count ê¸°ë°˜ ì •ë ¬)\n",
    "    - ì‚¬ìœ : ì‹œë¦¬ì¦ˆë‹¹ ì‹œì¦Œ ì•½ 3~8ê°œ Ã— ì—í”¼ì†Œë“œ 8~24ê°œ â†’ ì „ì²´ ì‹œë¦¬ì¦ˆ(99,000ê°œ) Ã— ì „ì²´ ì—í”¼ì†Œë“œ ìˆ˜ì§‘ ì‹œ ì•½ 9,900,000 API ìš”ì²­ í•„ìš” â†’ 3~5ì¼ ì´ìƒ ì†Œìš”\n",
    "5. ìˆ˜ì§‘ ë°©ì‹ (ë¹„ë™ê¸° ê¸°ë°˜ High-performance Pipeline)\n",
    "  - â‘  Discover ë‹¨ê³„\n",
    "    - /discover/tv ì‚¬ìš©\n",
    "    - Discoverì˜ 500í˜ì´ì§€ ì œí•œì„ íšŒí”¼í•˜ê¸° ìœ„í•´ ìë™ ë‚ ì§œ ë¶„í•  ë°©ì‹ ì ìš©\n",
    "  - â‘¡ Series ë‹¨ê³„ (ë¹„ë™ê¸°)\n",
    "    - TV ì‹œë¦¬ì¦ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘\n",
    "    - /tv/{id} with append_to_response=aggregate_credits, keywords, reviews\n",
    "    - /tv/{id}/watch/providers ë³„ë„ í˜¸ì¶œ\n",
    "    - rate limiterë¡œ ì´ˆë‹¹ ~30ìš”ì²­ ìœ ì§€\n",
    "  - â‘¢ Top10k ì‹œë¦¬ì¦ˆ ì„ ì •\n",
    "    - series-level ë°ì´í„°ë¥¼ ì •ë ¬í•˜ì—¬ vote_count ê¸°ë°˜ Top 10,000 ì¶”ì¶œ\n",
    "  - â‘£ Episode ë‹¨ê³„ (ë¹„ë™ê¸°)\n",
    "    - Top10k ì‹œë¦¬ì¦ˆì˜ ëª¨ë“  episode ì •ë³´ ìˆ˜ì§‘/tv/{id}/season/{sn}/episode/{ep}?append_to_response=credits í˜¸ì¶œ\n",
    "    - ì‹œë¦¬ì¦ˆ-levelì—ëŠ” ëˆ„ë½ëœ ê°ë…/ì‘ê°€ ì •ë³´ ë³´ì™„ (episode_runtime, episode_directors/writers, episode_top_cast ë“±)\n",
    "6. ìˆ˜ì§‘ ê²°ê³¼: 53ê°œ ì»¬ëŸ¼, 285415ê°œ ë°ì´í„°, ì•½ 2ì‹œê°„ 52ë¶„ ì†Œìš”\n",
    "7. ê¸°íƒ€ ì‚¬í•­: TV ì‹œë¦¬ì¦ˆ & ì—í”¼ì†Œë“œ ì „ìˆ˜ ìˆ˜ì§‘ ì¶”ê°€ ì§„í–‰ì¤‘ (ìµœì†Œ 2ì¼ ì´ìƒ ì†Œìš” ì˜ˆì •)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… nest_asyncio ì ìš© ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# TMDB TV series ë°ì´í„° ìˆ˜ì§‘ (A3 NIGHT)\n",
    "# - ê¸°ê°„: 2016-01-01 ~ 2025-11-29\n",
    "# - ë‹¨ê³„:\n",
    "#   1) Discover: ì „ì²´ TV ì‹œë¦¬ì¦ˆ ID ìˆ˜ì§‘ (ìë™ ë‚ ì§œ ë¶„í• )\n",
    "#   2) Series: ëª¨ë“  TV ì‹œë¦¬ì¦ˆ ìƒì„¸ ìˆ˜ì§‘ (53ì»¬ëŸ¼)\n",
    "#   3) Top10k: vote_count ê¸°ì¤€ ìƒìœ„ 10,000 ì‹œë¦¬ì¦ˆ ì„ ì •\n",
    "#   4) Episode: Top 10,000 ì‹œë¦¬ì¦ˆì˜ ì „ì²´ ì—í”¼ì†Œë“œ + credits(ê°ë…/ì‘ê°€/ì¶œì—°) ìˆ˜ì§‘\n",
    "#\n",
    "# íŠ¹ì§•:\n",
    "# - 100% ë¹„ë™ê¸°(asyncio + aiohttp) / ë©€í‹°ìŠ¤ë ˆë“œ ì‚¬ìš© ì•ˆ í•¨\n",
    "# - TMDB rate limit ê³ ë ¤ (ì•½ 30 req/s)\n",
    "# - Discover 500 page limit ëŒ€ë¹„ ìë™ ë‚ ì§œ ë¶„í• \n",
    "# - ì‹œë¦¬ì¦ˆ/ì—í”¼ì†Œë“œ ë‹¨ê³„ë³„ ì¤‘ë‹¨/ì¬ê°œ ê°€ëŠ¥ (done íŒŒì¼)\n",
    "# - ìµœì¢…ì ìœ¼ë¡œ:\n",
    "#     tv_series_2016_2025_DISCOVER_v4.csv\n",
    "#     tv_top10000_by_votecount.csv\n",
    "#     tv_episodes_top10000_FULL.csv\n",
    "#   ì„¸ ê°œì˜ í•µì‹¬ íŒŒì¼ ìƒì„±\n",
    "# ======================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# nest_asyncio: Jupyterì—ì„œ async ì¤‘ë³µ ë£¨í”„ ì—ëŸ¬ ë°©ì§€ìš©\n",
    "# ------------------------------------------------------\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    print(\"âœ… nest_asyncio ì ìš© ì™„ë£Œ\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ nest_asyncio ì—†ìŒ (ìµœì‹  JupyterëŠ” ë¶ˆí•„ìš”í•  ìˆ˜ ìˆìŒ)\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ nest_asyncio ì ìš© ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# ======================================================\n",
    "# 0. ê¸°ë³¸ ì„¤ì •\n",
    "# ======================================================\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"TMDB_API_KEY\")\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "HEADERS = {\"accept\": \"application/json\"}\n",
    "TIMEOUT = 10           # ê° ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)\n",
    "MAX_RETRIES = 3        # ì¬ì‹œë„ ìµœëŒ€ íšŸìˆ˜\n",
    "\n",
    "# Rate limit / ë™ì‹œì„± ì„¤ì • (ë¹„ë™ê¸°ìš©)\n",
    "RATE_LIMIT_CALLS = 30      # ì´ˆë‹¹ ìµœëŒ€ í˜¸ì¶œ ìˆ˜ (TMDB 40 ë¯¸ë§Œìœ¼ë¡œ ì—¬ìœ  ìˆê²Œ)\n",
    "RATE_LIMIT_PERIOD = 1.0    # ê¸°ì¤€ ì‹œê°„(ì´ˆ)\n",
    "MAX_CONCURRENT_REQUESTS = 30  # ì „ì²´ ë™ì‹œ ìš”ì²­ ìˆ˜ (global semaphore)\n",
    "\n",
    "# ì›Œì»¤ ìˆ˜\n",
    "SERIES_WORKERS = 25        # ì‹œë¦¬ì¦ˆ ë™ì‹œ ì²˜ë¦¬ ê°œìˆ˜\n",
    "EPISODE_WORKERS = 5        # ì—í”¼ì†Œë“œ ë™ì‹œ ì²˜ë¦¬ ê°œìˆ˜ (ì•ˆì • ìš°ì„ )\n",
    "\n",
    "# ìˆ˜ì§‘ ê¸°ê°„\n",
    "GLOBAL_START = date(2016, 1, 1)\n",
    "GLOBAL_END = date(2025, 11, 29)\n",
    "\n",
    "# ì¶œë ¥ íŒŒì¼ ì´ë¦„\n",
    "SERIES_OUT = \"tv_series_2016_2025_DISCOVER_v4.csv\"\n",
    "TOP10K_OUT = \"tv_top10000_by_votecount.csv\"\n",
    "EPISODE_OUT = \"tv_episodes_top10000_FULL.csv\"\n",
    "\n",
    "# ë¡œê·¸ / done íŒŒì¼\n",
    "SERIES_DONE_FILE = \"series_done.txt\"          # ì‹œë¦¬ì¦ˆ ìƒì„¸ ìˆ˜ì§‘ ì™„ë£Œ ID\n",
    "SERIES_FAILED_FILE = \"series_failed.txt\"      # ì‹œë¦¬ì¦ˆ ì‹¤íŒ¨ ë¡œê·¸\n",
    "EPISODE_DONE_FILE = \"episode_series_done.txt\" # ì—í”¼ì†Œë“œ ìˆ˜ì§‘ ì™„ë£Œ ì‹œë¦¬ì¦ˆ ID\n",
    "EPISODE_FAILED_FILE = \"episode_series_failed.txt\"\n",
    "\n",
    "# ì—í”¼ì†Œë“œ ì»¬ëŸ¼ ì •ì˜ (ìµœì¢… csv ì»¬ëŸ¼ ìˆœì„œ)\n",
    "EPISODE_COLUMNS = [\n",
    "    \"series_id\",\"season_number\",\"episode_number\",\"episode_name\",\n",
    "    \"episode_air_date\",\"episode_runtime\",\"episode_overview\",\n",
    "    \"episode_still_path\",\"episode_vote_average\",\"episode_vote_count\",\n",
    "    \"episode_top_cast\",\"episode_directors\",\"episode_writers\",\n",
    "]\n",
    "\n",
    "# Discover ê´€ë ¨ ì„¤ì •\n",
    "DISCOVER_PAGE_LIMIT = 500          # TMDBê°€ í—ˆìš©í•˜ëŠ” ìµœëŒ€ í˜ì´ì§€\n",
    "DISCOVER_SPLIT_THRESHOLD = 450     # ì´ ê°’ë³´ë‹¤ í¬ë©´ ë‚ ì§œ ë²”ìœ„ë¥¼ ë‘˜ë¡œ ë‚˜ëˆ”\n",
    "\n",
    "# ======================================================\n",
    "# 1. Rate Limiter + HTTP ê³µìš© í•¨ìˆ˜ (ë¹„ë™ê¸°)\n",
    "# ======================================================\n",
    "class AsyncRateLimiter:\n",
    "    \"\"\"\n",
    "    ì´ˆë‹¹ ìš”ì²­ ìˆ˜ë¥¼ ì œí•œí•˜ê¸° ìœ„í•œ ë¹„ë™ê¸° Rate Limiter.\n",
    "    - ìµœê·¼ 'period' ì´ˆ ë™ì•ˆ 'max_calls' ê°œë³´ë‹¤ ë§ì´ í˜¸ì¶œë˜ì§€ ì•Šë„ë¡ ì œì–´.\n",
    "    - 429 ì‘ë‹µ ì‹œ, global_backoff_until ì‹œê°„ê¹Œì§€ ì¶”ê°€ ëŒ€ê¸°.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_calls=RATE_LIMIT_CALLS, period=RATE_LIMIT_PERIOD):\n",
    "        self.max_calls = max_calls\n",
    "        self.period = period\n",
    "        self.calls = []              # ìµœê·¼ í˜¸ì¶œ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë¡\n",
    "        self.global_backoff_until = 0.0\n",
    "        self._lock = asyncio.Lock()\n",
    "\n",
    "    async def wait(self):\n",
    "        \"\"\"\n",
    "        ìš”ì²­ ì „ì— í˜¸ì¶œí•´ì„œ rate limitë¥¼ ì§€í‚¤ë„ë¡ ëŒ€ê¸°í•˜ëŠ” ë©”ì„œë“œ.\n",
    "        \"\"\"\n",
    "        async with self._lock:\n",
    "            now = time.time()\n",
    "\n",
    "            # 429 í˜¹ì€ ì„œë²„ ê¶Œê³ ë¡œ global backoffê°€ ì„¤ì •ëœ ê²½ìš°\n",
    "            if self.global_backoff_until > now:\n",
    "                await asyncio.sleep(self.global_backoff_until - now)\n",
    "                now = time.time()\n",
    "\n",
    "            # period(1ì´ˆ)ë³´ë‹¤ ì˜¤ë˜ëœ í˜¸ì¶œ ê¸°ë¡ ì œê±°\n",
    "            self.calls = [c for c in self.calls if now - c < self.period]\n",
    "\n",
    "            # í˜„ì¬ windowì—ì„œ ë„ˆë¬´ ë§ì´ í˜¸ì¶œí–ˆìœ¼ë©´ ëŒ€ê¸°\n",
    "            if len(self.calls) >= self.max_calls:\n",
    "                sleep_time = self.period - (now - self.calls[0]) + 0.01\n",
    "                if sleep_time > 0:\n",
    "                    await asyncio.sleep(sleep_time)\n",
    "\n",
    "            # í˜„ì¬ í˜¸ì¶œ ì‹œê°„ ê¸°ë¡\n",
    "            self.calls.append(time.time())\n",
    "\n",
    "    def set_global_backoff(self, seconds: float):\n",
    "        \"\"\"\n",
    "        429 Retry-After ì‘ë‹µ ë“±ì„ ê¸°ë°˜ìœ¼ë¡œ ì¼ì • ì‹œê°„ ë™ì•ˆ ì „ì²´ ìš”ì²­ì„ ì‰¬ë„ë¡ ì„¤ì •.\n",
    "        \"\"\"\n",
    "        self.global_backoff_until = max(self.global_backoff_until, time.time() + seconds)\n",
    "\n",
    "\n",
    "# ì „ì—­ ì„¸ì…˜ / ì„¸ë§ˆí¬ì–´\n",
    "rate_limiter = AsyncRateLimiter()\n",
    "session: aiohttp.ClientSession | None = None\n",
    "sem = asyncio.BoundedSemaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "\n",
    "async def tmdb_get(path: str, params: dict | None = None, retry_count: int = 0):\n",
    "    \"\"\"\n",
    "    TMDB GET ìš”ì²­ì„ ìˆ˜í–‰í•˜ëŠ” ê³µìš© í•¨ìˆ˜.\n",
    "    - aiohttp + asyncio ê¸°ë°˜ ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸\n",
    "    - 429 / 5xx / Timeout ì— ëŒ€í•´ ì¬ì‹œë„ ë¡œì§ í¬í•¨\n",
    "    - API í‚¤ ìë™ ì¶”ê°€\n",
    "    \"\"\"\n",
    "    if retry_count >= MAX_RETRIES:\n",
    "        return None\n",
    "    if API_KEY is None:\n",
    "        raise RuntimeError(\"TMDB_API_KEY not found ('.env'ì— ì„¤ì • í•„ìš”)\")\n",
    "\n",
    "    global session\n",
    "    if session is None or session.closed:\n",
    "        session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=TIMEOUT))\n",
    "\n",
    "    params = params.copy() if params else {}\n",
    "    params[\"api_key\"] = API_KEY\n",
    "    url = f\"{BASE_URL}{path}\"\n",
    "\n",
    "    async with sem:\n",
    "        await rate_limiter.wait()\n",
    "        try:\n",
    "            async with session.get(url, params=params, headers=HEADERS) as resp:\n",
    "                # Rate limit ì´ˆê³¼\n",
    "                if resp.status == 429:\n",
    "                    retry_after = resp.headers.get(\"Retry-After\")\n",
    "                    # Retry-Afterê°€ ë„ˆë¬´ ê¸¸ì–´ë„ ìµœëŒ€ 10ì´ˆê¹Œì§€ë§Œ ëŒ€ê¸°\n",
    "                    wait_sec = min(int(retry_after), 10) if retry_after else min(10 * (2 ** retry_count), 10)\n",
    "                    print(f\"\\nğŸ”´ 429 Rate Limit: {wait_sec}s wait... ({path})\")\n",
    "                    rate_limiter.set_global_backoff(wait_sec + 2)\n",
    "                    await asyncio.sleep(wait_sec + 2)\n",
    "                    return await tmdb_get(path, params, retry_count + 1)\n",
    "\n",
    "                # ë¦¬ì†ŒìŠ¤ ì—†ìŒ\n",
    "                if resp.status == 404:\n",
    "                    return None\n",
    "\n",
    "                # ì„œë²„ ì—ëŸ¬(5xx) ì¬ì‹œë„\n",
    "                if 500 <= resp.status < 600:\n",
    "                    if retry_count < MAX_RETRIES:\n",
    "                        await asyncio.sleep(2 ** retry_count)\n",
    "                        return await tmdb_get(path, params, retry_count + 1)\n",
    "                    return None\n",
    "\n",
    "                resp.raise_for_status()\n",
    "                return await resp.json()\n",
    "\n",
    "        except (aiohttp.ClientError, asyncio.TimeoutError):\n",
    "            # ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ / íƒ€ì„ì•„ì›ƒë„ ì¬ì‹œë„\n",
    "            if retry_count < MAX_RETRIES:\n",
    "                await asyncio.sleep(2 ** retry_count)\n",
    "                return await tmdb_get(path, params, retry_count + 1)\n",
    "            return None\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 2. Discover Range Splitter (ì „ì²´ ì‹œë¦¬ì¦ˆ ID ìˆ˜ì§‘)\n",
    "# ======================================================\n",
    "async def discover_tv_for_range(start_d: date, end_d: date, depth: int = 0) -> set[int]:\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ë‚ ì§œ ë²”ìœ„ [start_d, end_d]ì— ëŒ€í•´ /discover/tvë¥¼ í˜¸ì¶œí•˜ì—¬\n",
    "    TV ì‹œë¦¬ì¦ˆ IDë¥¼ ëª¨ë‘ ìˆ˜ì§‘.\n",
    "    - total_pages > DISCOVER_SPLIT_THRESHOLD ì´ë©´ ë‚ ì§œ ë²”ìœ„ë¥¼ ë‘˜ë¡œ ë‚˜ëˆ„ì–´ ì¬ê·€ í˜¸ì¶œ\n",
    "    - TMDBì˜ 500 page limitë¥¼ í”¼í•˜ê¸° ìœ„í•´ ë‚ ì§œë¥¼ ìë™ìœ¼ë¡œ ì˜ê²Œ split\n",
    "    \"\"\"\n",
    "    indent = \"  \" * depth\n",
    "    params = {\n",
    "        \"language\": \"en-US\",\n",
    "        \"include_adult\": \"false\",\n",
    "        \"include_null_first_air_dates\": \"false\",\n",
    "        \"sort_by\": \"first_air_date.asc\",\n",
    "        \"first_air_date.gte\": start_d.isoformat(),\n",
    "        \"first_air_date.lte\": end_d.isoformat(),\n",
    "        \"page\": 1,\n",
    "    }\n",
    "\n",
    "    data = await tmdb_get(\"/discover/tv\", params=params)\n",
    "    if not data:\n",
    "        print(f\"{indent}âš ï¸ Discover ì‹¤íŒ¨: {start_d} ~ {end_d}\")\n",
    "        return set()\n",
    "\n",
    "    total_pages = data.get(\"total_pages\", 1)\n",
    "    total_results = data.get(\"total_results\", 0)\n",
    "    print(f\"{indent}ğŸ“… {start_d} ~ {end_d} â†’ total_pages={total_pages}, total_results={total_results}\")\n",
    "\n",
    "    # í˜ì´ì§€ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ë‚ ì§œ ë²”ìœ„ë¥¼ ë°˜ìœ¼ë¡œ ë‚˜ëˆ ì„œ ì¬ê·€ì ìœ¼ë¡œ ìˆ˜ì§‘\n",
    "    if total_pages > DISCOVER_SPLIT_THRESHOLD and start_d < end_d:\n",
    "        mid = start_d + (end_d - start_d) // 2\n",
    "        left_ids = await discover_tv_for_range(start_d, mid, depth + 1)\n",
    "        right_ids = await discover_tv_for_range(mid + timedelta(days=1), end_d, depth + 1)\n",
    "        return left_ids | right_ids\n",
    "\n",
    "    # í˜„ì¬ ë²”ìœ„ì—ì„œ ì²« í˜ì´ì§€ ê²°ê³¼ ì²˜ë¦¬\n",
    "    ids: set[int] = set()\n",
    "    for item in data.get(\"results\", []):\n",
    "        if item.get(\"id\"):\n",
    "            ids.add(int(item[\"id\"]))\n",
    "\n",
    "    # ë‚˜ë¨¸ì§€ í˜ì´ì§€ë“¤ì„ ë¹„ë™ê¸°ë¡œ ìˆ˜ì§‘\n",
    "    max_page = min(total_pages, DISCOVER_PAGE_LIMIT)\n",
    "    tasks = []\n",
    "    for page in range(2, max_page + 1):\n",
    "        p = params.copy()\n",
    "        p[\"page\"] = page\n",
    "        tasks.append(tmdb_get(\"/discover/tv\", p))\n",
    "\n",
    "    for coro in asyncio.as_completed(tasks):\n",
    "        d = await coro\n",
    "        if not d:\n",
    "            continue\n",
    "        for item in d.get(\"results\", []):\n",
    "            if item.get(\"id\"):\n",
    "                ids.add(int(item[\"id\"]))\n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "async def collect_all_series_ids() -> list[int]:\n",
    "    \"\"\"\n",
    "    ì „ì²´ ê¸°ê°„(GLOBAL_START ~ GLOBAL_END)ì— ëŒ€í•´ Discoverë¥¼ ìˆ˜í–‰,\n",
    "    ê³ ìœ  TV ì‹œë¦¬ì¦ˆ id ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ“Œ Discover ì „ì²´ ì‹œë¦¬ì¦ˆ ID ìˆ˜ì§‘ ì‹œì‘\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    allids = await discover_tv_for_range(GLOBAL_START, GLOBAL_END)\n",
    "    ids_list = sorted(allids)\n",
    "    print(f\"\\nâœ… Discover ì™„ë£Œ: ê³ ìœ  ì‹œë¦¬ì¦ˆ {len(ids_list):,}ê°œ\\n\")\n",
    "    return ids_list\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 3. Series-level ìƒì„¸ + providers + credits\n",
    "# ======================================================\n",
    "def safe_json_dumps(x):\n",
    "    \"\"\"\n",
    "    JSON ì§ë ¬í™” ë„ìš°ë¯¸.\n",
    "    - None -> ë¹ˆ ë¬¸ìì—´\n",
    "    - ì§ë ¬í™” ì‹¤íŒ¨ ì‹œì—ë„ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (ì—ëŸ¬ ë°©ì§€)\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.dumps(x, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def flat_created_by(lst):\n",
    "    \"\"\"\n",
    "    created_by í•„ë“œë¥¼\n",
    "    - JSON ì „ì²´ (created_by)\n",
    "    - ì´ë¦„ë§Œ ì½¤ë§ˆ ì¡°ì¸(\"created by\")\n",
    "    ë‘ ê°€ì§€ í˜•íƒœë¡œ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    if not isinstance(lst, list):\n",
    "        return \"\", \"\"\n",
    "    names = [c[\"name\"] for c in lst if c.get(\"name\")]\n",
    "    return safe_json_dumps(lst), \", \".join(names)\n",
    "\n",
    "\n",
    "def extract_keywords(obj):\n",
    "    \"\"\"\n",
    "    /keywords ì‘ë‹µì—ì„œ keyword ì´ë¦„ë“¤ì„ ì½¤ë§ˆë¡œ í•©ì³ ë¬¸ìì—´ë¡œ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    if not obj:\n",
    "        return \"\"\n",
    "    arr = obj.get(\"results\", [])\n",
    "    return \", \".join(sorted({x[\"name\"] for x in arr if x.get(\"name\")}))\n",
    "\n",
    "\n",
    "def extract_review(obj, max_chars: int = 1000):\n",
    "    \"\"\"\n",
    "    /reviews ì‘ë‹µì—ì„œ ì²« ë²ˆì§¸ ë¦¬ë·° ë‚´ìš©ì„ ìµœëŒ€ max_charsê¹Œì§€ ì˜ë¼ì„œ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    if not obj:\n",
    "        return \"\"\n",
    "    arr = obj.get(\"results\", [])\n",
    "    if not arr:\n",
    "        return \"\"\n",
    "    t = arr[0].get(\"content\", \"\").replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \").strip()\n",
    "    return t[:max_chars] + (\"...\" if len(t) > max_chars else \"\")\n",
    "\n",
    "\n",
    "def extract_cast_crew(obj):\n",
    "    \"\"\"\n",
    "    aggregate_credits ì‘ë‹µì—ì„œ\n",
    "    - top_cast: ìƒìœ„ 4ëª… ë°°ìš° ì´ë¦„\n",
    "    - directors: ê°ë… ì´ë¦„ë“¤\n",
    "    - writers: ì‘ê°€/ì‹œë‚˜ë¦¬ì˜¤/ìŠ¤í† ë¦¬ ê´€ë ¨ ìŠ¤íƒœí”„ ì´ë¦„ë“¤\n",
    "    ì„ ì¶”ì¶œí•˜ì—¬ ë¬¸ìì—´ë¡œ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    if not obj:\n",
    "        return \"\", \"\", \"\"\n",
    "    cast = obj.get(\"cast\", [])\n",
    "    crew = obj.get(\"crew\", [])\n",
    "    top_cast = \", \".join([c[\"name\"] for c in cast[:4] if c.get(\"name\")])\n",
    "\n",
    "    directors = set()\n",
    "    writers = set()\n",
    "    for c in crew:\n",
    "        name = c.get(\"name\")\n",
    "        job = (c.get(\"job\", \"\") + \" \" + c.get(\"known_for_department\", \"\"))\n",
    "        if not name:\n",
    "            continue\n",
    "        if \"Direct\" in job:\n",
    "            directors.add(name)\n",
    "        if any(k in job for k in [\"Write\", \"Script\", \"Story\", \"Screenplay\"]):\n",
    "            writers.add(name)\n",
    "\n",
    "    return top_cast, \", \".join(sorted(directors)), \", \".join(sorted(writers))\n",
    "\n",
    "\n",
    "def extract_providers(obj):\n",
    "    \"\"\"\n",
    "    /watch/providers ì‘ë‹µì—ì„œ\n",
    "    - êµ­ê°€ì½”ë“œ:íƒ€ì…:provider_name\n",
    "    í˜•íƒœì˜ ë¬¸ìì—´ì„ ëª¨ë‘ ì¶”ì¶œí•˜ì—¬ ì½¤ë§ˆë¡œ í•©ì³ ë°˜í™˜.\n",
    "    ex) \"US:flatrate:Netflix, US:ads:Hulu, KR:flatrate:Netflix\"\n",
    "    \"\"\"\n",
    "    if not obj:\n",
    "        return \"\"\n",
    "    results = obj.get(\"results\", {})\n",
    "    out = set()\n",
    "    for cc, info in results.items():\n",
    "        if not isinstance(info, dict):\n",
    "            continue\n",
    "        for typ in [\"flatrate\", \"ads\", \"free\", \"rent\", \"buy\"]:\n",
    "            lst = info.get(typ)\n",
    "            if not lst:\n",
    "                continue\n",
    "            items = list(lst.values()) if isinstance(lst, dict) else lst\n",
    "            for p in items:\n",
    "                if not isinstance(p, dict):\n",
    "                    continue\n",
    "                nm = p.get(\"provider_name\")\n",
    "                if nm:\n",
    "                    out.add(f\"{cc}:{typ}:{nm}\")\n",
    "    return \", \".join(sorted(out))\n",
    "\n",
    "\n",
    "async def fetch_series_full(tv_id: int):\n",
    "    \"\"\"\n",
    "    í•œ ê°œì˜ TV ì‹œë¦¬ì¦ˆì— ëŒ€í•´:\n",
    "    - /tv/{id}?append_to_response=aggregate_credits,keywords,reviews\n",
    "    - /tv/{id}/watch/providers\n",
    "    ë¥¼ í˜¸ì¶œí•´ì„œ\n",
    "    53ê°œ series-level ì»¬ëŸ¼ + ê°•í™”ëœ credits/provider ì •ë³´ë¥¼ í•œ í–‰(row)ìœ¼ë¡œ êµ¬ì„±.\n",
    "    ë°˜í™˜ê°’: (row(dict), tv_raw_json(dict))\n",
    "    \"\"\"\n",
    "    tv = await tmdb_get(\n",
    "        f\"/tv/{tv_id}\",\n",
    "        params={\n",
    "            \"language\": \"en-US\",\n",
    "            \"append_to_response\": \"aggregate_credits,keywords,reviews\",\n",
    "        },\n",
    "    )\n",
    "    if not tv:\n",
    "        return None\n",
    "\n",
    "    agg = tv.get(\"aggregate_credits\") or {}\n",
    "    kw = tv.get(\"keywords\") or {}\n",
    "    rv = tv.get(\"reviews\") or {}\n",
    "\n",
    "    providers = await tmdb_get(f\"/tv/{tv_id}/watch/providers\")\n",
    "\n",
    "    cjson, cflat = flat_created_by(tv.get(\"created_by\", []))\n",
    "    genres = tv.get(\"genres\") or []\n",
    "    gjson = safe_json_dumps(genres)\n",
    "    gids = \", \".join(str(g[\"id\"]) for g in genres if g.get(\"id\"))\n",
    "\n",
    "    # episode_run_time ë°°ì—´ -> í‰ê· ê°’ìœ¼ë¡œ ëŒ€í‘œ ëŸ¬ë‹íƒ€ì„ ê³„ì‚°\n",
    "    ert = tv.get(\"episode_run_time\") or []\n",
    "    if isinstance(ert, list) and ert:\n",
    "        try:\n",
    "            runtime = int(round(sum(ert) / len(ert)))\n",
    "        except Exception:\n",
    "            runtime = None\n",
    "    else:\n",
    "        runtime = None\n",
    "\n",
    "    last_ep = tv.get(\"last_episode_to_air\") or {}\n",
    "    next_ep = tv.get(\"next_episode_to_air\")\n",
    "\n",
    "    languages = tv.get(\"languages\") or []\n",
    "    languages_str = \", \".join(languages) if isinstance(languages, list) else str(languages)\n",
    "\n",
    "    topcast, dirs, wrs = extract_cast_crew(agg)\n",
    "\n",
    "    row = {\n",
    "        \"id\": tv_id,\n",
    "        \"title\": tv.get(\"name\"),\n",
    "        \"type\": \"tv_series\",\n",
    "        \"adult\": tv.get(\"adult\", False),\n",
    "        \"backdrop_path\": tv.get(\"backdrop_path\"),\n",
    "        \"created_by\": cjson,\n",
    "        \"episode_run_time\": runtime,\n",
    "        \"first_air_date\": tv.get(\"first_air_date\"),\n",
    "        \"genres\": gjson,\n",
    "        \"genre_ids\": gids,\n",
    "        \"homepage\": tv.get(\"homepage\"),\n",
    "        \"in_production\": tv.get(\"in_production\"),\n",
    "        \"languages\": languages_str,\n",
    "        \"last_air_date\": tv.get(\"last_air_date\"),\n",
    "\n",
    "        # last_episode_to_air ì „ì²´ í•„ë“œ\n",
    "        \"last_episode_to_air_id\": last_ep.get(\"id\"),\n",
    "        \"last_episode_to_air_name\": last_ep.get(\"name\"),\n",
    "        \"last_episode_to_air_overview\": last_ep.get(\"overview\"),\n",
    "        \"last_episode_to_air_vote_average\": last_ep.get(\"vote_average\"),\n",
    "        \"last_episode_to_air_vote_count\": last_ep.get(\"vote_count\"),\n",
    "        \"last_episode_to_air_air_date\": last_ep.get(\"air_date\"),\n",
    "        \"last_episode_to_air_episode_number\": last_ep.get(\"episode_number\"),\n",
    "        \"last_episode_to_air_production_code\": last_ep.get(\"production_code\"),\n",
    "        \"last_episode_to_air_runtime\": last_ep.get(\"runtime\"),\n",
    "        \"last_episode_to_air_season_number\": last_ep.get(\"season_number\"),\n",
    "        \"last_episode_to_air_show_id\": last_ep.get(\"show_id\"),\n",
    "        \"last_episode_to_air_still_path\": last_ep.get(\"still_path\"),\n",
    "\n",
    "        \"name\": tv.get(\"name\"),\n",
    "        \"next_episode_to_air\": safe_json_dumps(next_ep),\n",
    "        \"networks\": safe_json_dumps(tv.get(\"networks\")),\n",
    "        \"number_of_episodes\": tv.get(\"number_of_episodes\"),\n",
    "        \"number_of_seasons\": tv.get(\"number_of_seasons\"),\n",
    "        \"origin_country\": \", \".join(tv.get(\"origin_country\", [])),\n",
    "        \"original_language\": tv.get(\"original_language\"),\n",
    "        \"original_name\": tv.get(\"original_name\"),\n",
    "        \"overview\": tv.get(\"overview\"),\n",
    "        \"popularity\": tv.get(\"popularity\"),\n",
    "        \"poster_path\": tv.get(\"poster_path\"),\n",
    "        \"production_companies\": safe_json_dumps(tv.get(\"production_companies\")),\n",
    "        \"production_countries\": safe_json_dumps(tv.get(\"production_countries\")),\n",
    "        \"seasons\": safe_json_dumps(tv.get(\"seasons\")),\n",
    "        \"spoken_languages\": safe_json_dumps(tv.get(\"spoken_languages\")),\n",
    "        \"status\": tv.get(\"status\"),\n",
    "        \"tagline\": tv.get(\"tagline\"),\n",
    "        \"type_detail\": tv.get(\"type\"),\n",
    "        \"vote_average\": tv.get(\"vote_average\"),\n",
    "        \"vote_count\": tv.get(\"vote_count\"),\n",
    "\n",
    "        \"review\": extract_review(rv),\n",
    "        \"keyword\": extract_keywords(kw),\n",
    "        \"top_cast\": topcast,\n",
    "        \"directors\": dirs,\n",
    "        \"writers\": wrs,\n",
    "        \"created by\": cflat,\n",
    "        \"providers\": extract_providers(providers),\n",
    "    }\n",
    "    return row, tv\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 4. Episode-level ìˆ˜ì§‘ (Top 10,000 ì‹œë¦¬ì¦ˆ í•œì •)\n",
    "# ======================================================\n",
    "async def fetch_episode(tv_id: int, sn: int, ep_no: int):\n",
    "    \"\"\"\n",
    "    /tv/{tv_id}/season/{sn}/episode/{ep_no}?append_to_response=credits\n",
    "    ë¥¼ í˜¸ì¶œí•˜ì—¬ ê°œë³„ ì—í”¼ì†Œë“œ ì •ë³´ë¥¼ ìˆ˜ì§‘.\n",
    "    - episode ê¸°ë³¸ ì •ë³´\n",
    "    - credits ê¸°ë°˜ top_cast / directors / writers ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    d = await tmdb_get(\n",
    "        f\"/tv/{tv_id}/season/{sn}/episode/{ep_no}\",\n",
    "        params={\"language\": \"en-US\", \"append_to_response\": \"credits\"},\n",
    "    )\n",
    "    if not d:\n",
    "        return None\n",
    "\n",
    "    cast = d.get(\"credits\", {}).get(\"cast\", [])\n",
    "    crew = d.get(\"credits\", {}).get(\"crew\", [])\n",
    "\n",
    "    top = \", \".join([c[\"name\"] for c in cast[:4] if c.get(\"name\")])\n",
    "    dirs = set()\n",
    "    wrs = set()\n",
    "    for c in crew:\n",
    "        nm = c.get(\"name\")\n",
    "        job = (c.get(\"job\", \"\") + \" \" + c.get(\"known_for_department\", \"\"))\n",
    "        if not nm:\n",
    "            continue\n",
    "        if \"Direct\" in job:\n",
    "            dirs.add(nm)\n",
    "        if any(k in job for k in [\"Write\", \"Script\", \"Story\", \"Screenplay\"]):\n",
    "            wrs.add(nm)\n",
    "\n",
    "    return {\n",
    "        \"series_id\": tv_id,\n",
    "        \"season_number\": sn,\n",
    "        \"episode_number\": d.get(\"episode_number\"),\n",
    "        \"episode_name\": d.get(\"name\"),\n",
    "        \"episode_air_date\": d.get(\"air_date\"),\n",
    "        \"episode_runtime\": d.get(\"runtime\"),\n",
    "        \"episode_overview\": d.get(\"overview\"),\n",
    "        \"episode_still_path\": d.get(\"still_path\"),\n",
    "        \"episode_vote_average\": d.get(\"vote_average\"),\n",
    "        \"episode_vote_count\": d.get(\"vote_count\"),\n",
    "        \"episode_top_cast\": top,\n",
    "        \"episode_directors\": \", \".join(sorted(dirs)),\n",
    "        \"episode_writers\": \", \".join(sorted(wrs)),\n",
    "    }\n",
    "\n",
    "\n",
    "async def collect_episodes_for_series(tv_data: dict):\n",
    "    \"\"\"\n",
    "    TMDB TV ìƒì„¸ ì‘ë‹µ(tv_data)ì—ì„œ seasons ì •ë³´ë¥¼ ë³´ê³ \n",
    "    - ê° ì‹œì¦Œì˜ episode_countë§Œí¼\n",
    "    - 1í™”ë¶€í„° Ní™”ê¹Œì§€ fetch_episodeë¥¼ ëŒë ¤ì„œ\n",
    "    ì „ì²´ ì—í”¼ì†Œë“œ ì •ë³´ë¥¼ ìˆ˜ì§‘.\n",
    "    \"\"\"\n",
    "    if not tv_data:\n",
    "        return []\n",
    "\n",
    "    sid = tv_data.get(\"id\")\n",
    "    seasons = tv_data.get(\"seasons\", [])\n",
    "    tasks = []\n",
    "\n",
    "    for s in seasons:\n",
    "        sn = s.get(\"season_number\")\n",
    "        ep_c = s.get(\"episode_count\")\n",
    "        if sn is None or ep_c is None:\n",
    "            continue\n",
    "        try:\n",
    "            sn_int = int(sn)\n",
    "            ep_count_int = int(ep_c)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if ep_count_int <= 0:\n",
    "            continue\n",
    "\n",
    "        for ep in range(1, ep_count_int + 1):\n",
    "            tasks.append(fetch_episode(sid, sn_int, ep))\n",
    "\n",
    "    rows = []\n",
    "    sem_ep = asyncio.Semaphore(EPISODE_WORKERS)\n",
    "\n",
    "    async def ep_worker(coro):\n",
    "        async with sem_ep:\n",
    "            return await coro\n",
    "\n",
    "    wrapped = [ep_worker(t) for t in tasks]\n",
    "\n",
    "    for coro in asyncio.as_completed(wrapped):\n",
    "        r = await coro\n",
    "        if r:\n",
    "            rows.append(r)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 5. done / failed íŒŒì¼ ìœ í‹¸\n",
    "# ======================================================\n",
    "def load_done_set(path: str) -> set[int]:\n",
    "    \"\"\"\n",
    "    ê° ë‹¨ê³„ì—ì„œ ì´ë¯¸ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ì‹œë¦¬ì¦ˆ IDë¥¼ ì €ì¥í•œ íŒŒì¼ì„ ì½ì–´,\n",
    "    set[int] í˜•íƒœë¡œ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    if not Path(path).exists():\n",
    "        return set()\n",
    "    out: set[int] = set()\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                out.add(int(line))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return out\n",
    "\n",
    "\n",
    "def append_done_id(path: str, sid: int):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ sidë¥¼ done íŒŒì¼ì— í•œ ì¤„ë¡œ ì¶”ê°€ ê¸°ë¡.\n",
    "    \"\"\"\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{sid}\\n\")\n",
    "\n",
    "\n",
    "def append_failed_id(path: str, sid: int, msg: str):\n",
    "    \"\"\"\n",
    "    ì‹¤íŒ¨í•œ ì‹œë¦¬ì¦ˆ IDì™€ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ íƒ­ìœ¼ë¡œ êµ¬ë¶„í•´ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡.\n",
    "    \"\"\"\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{sid}\\t{msg}\\n\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 6. ì‹œë¦¬ì¦ˆ 1ê°œ ì²˜ë¦¬ (series + episode ë‘˜ ë‹¤ì—ì„œ ì‚¬ìš©)\n",
    "# ======================================================\n",
    "async def process_one_series_for_series_stage(tv_id: int):\n",
    "    \"\"\"\n",
    "    ì‹œë¦¬ì¦ˆ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë˜í¼:\n",
    "    - fetch_series_full í˜¸ì¶œ\n",
    "    - ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ì™€ í•¨ê»˜ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res = await fetch_series_full(tv_id)\n",
    "    except Exception as e:\n",
    "        return tv_id, None, None, f\"{e}\"\n",
    "\n",
    "    if not res:\n",
    "        return tv_id, None, None, \"tv_null\"\n",
    "\n",
    "    row, tv_data = res\n",
    "    return tv_id, row, tv_data, None\n",
    "\n",
    "\n",
    "async def process_one_series_for_episode_stage(tv_id: int):\n",
    "    \"\"\"\n",
    "    ì—í”¼ì†Œë“œ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë˜í¼:\n",
    "    - ì‹œë¦¬ì¦ˆ ìƒì„¸(fetch_series_full) í˜¸ì¶œ\n",
    "      (ì´ë¯¸ series csvê°€ ìˆì–´ë„ episode ìˆ˜ì§‘ì„ ìœ„í•´ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜´)\n",
    "    - collect_episodes_for_series í˜¸ì¶œí•˜ì—¬ ì „ì²´ ì—í”¼ì†Œë“œ ìˆ˜ì§‘\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res = await fetch_series_full(tv_id)\n",
    "    except Exception as e:\n",
    "        return tv_id, None, None, f\"{e}\"\n",
    "\n",
    "    if not res:\n",
    "        return tv_id, None, None, \"tv_null\"\n",
    "\n",
    "    row, tv_data = res  # rowëŠ” ì—¬ê¸°ì„  ì‚¬ìš©í•˜ì§€ ì•Šê³ , tv_dataë§Œ ì‚¬ìš©\n",
    "    ep_rows = await collect_episodes_for_series(tv_data)\n",
    "    return tv_id, ep_rows, None, None\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 7. Main íŒŒì´í”„ë¼ì¸\n",
    "# ======================================================\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰:\n",
    "    1) Discover â†’ ì „ì²´ ì‹œë¦¬ì¦ˆ ID\n",
    "    2) Series ë‹¨ê³„ â†’ ëª¨ë“  ì‹œë¦¬ì¦ˆ ìƒì„¸ ìˆ˜ì§‘ + csv ì €ì¥\n",
    "    3) vote_count ê¸°ì¤€ ìƒìœ„ 10,000 ì‹œë¦¬ì¦ˆ ì„ ì • â†’ tv_top10000_by_votecount.csv\n",
    "    4) Episode ë‹¨ê³„ â†’ Top 10,000 ì‹œë¦¬ì¦ˆ ì „ì²´ ì—í”¼ì†Œë“œ ìˆ˜ì§‘ + csv ì €ì¥\n",
    "    \"\"\"\n",
    "    if API_KEY is None:\n",
    "        raise RuntimeError(\"TMDB_API_KEY missing (.env íŒŒì¼ì— TMDB_API_KEY=... ì„¤ì • í•„ìš”)\")\n",
    "\n",
    "    global session\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # STEP 1) Discover - ì „ì²´ ì‹œë¦¬ì¦ˆ ID\n",
    "    # --------------------------------------------------\n",
    "    all_ids = await collect_all_series_ids()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # STEP 2) Series-level ìƒì„¸ ìˆ˜ì§‘\n",
    "    # --------------------------------------------------\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ“Œ STEP 2: TV series ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    done_series = load_done_set(SERIES_DONE_FILE)\n",
    "    ids_to_process_series = [sid for sid in all_ids if sid not in done_series]\n",
    "\n",
    "    print(f\"âœ… ì´ë¯¸ ì™„ë£Œëœ ì‹œë¦¬ì¦ˆ(Series ë‹¨ê³„): {len(done_series):,}ê°œ\")\n",
    "    print(f\"ğŸ“Œ ì´ë²ˆì— ì²˜ë¦¬í•  ì‹œë¦¬ì¦ˆ(Series ë‹¨ê³„): {len(ids_to_process_series):,}ê°œ\\n\")\n",
    "\n",
    "    write_header_s = not Path(SERIES_OUT).exists()\n",
    "    total_start = time.time()\n",
    "    failed_series = 0\n",
    "    processed_series = 0\n",
    "    total_series = len(ids_to_process_series)\n",
    "\n",
    "    sem_series = asyncio.Semaphore(SERIES_WORKERS)\n",
    "\n",
    "    async def series_worker(sid: int):\n",
    "        async with sem_series:\n",
    "            return await process_one_series_for_series_stage(sid)\n",
    "\n",
    "    series_tasks = [asyncio.create_task(series_worker(sid)) for sid in ids_to_process_series]\n",
    "\n",
    "    try:\n",
    "        for coro in asyncio.as_completed(series_tasks):\n",
    "            sid, srow, tv_data, err = await coro\n",
    "\n",
    "            processed_series += 1\n",
    "            # ì§„í–‰ë¥  ë¡œê·¸\n",
    "            if processed_series % 20 == 0 or processed_series == 1:\n",
    "                elapsed = time.time() - total_start\n",
    "                rate = processed_series / (elapsed / 60) if elapsed > 0 else 0\n",
    "                remain = (total_series - processed_series) / rate if rate > 0 else 0\n",
    "                print(\n",
    "                    f\"\\r[Series] ì§„í–‰: {processed_series}/{total_series} \"\n",
    "                    f\"({processed_series / total_series * 100:.1f}%) | \"\n",
    "                    f\"ì†ë„ {rate:.1f}/ë¶„ | ë‚¨ìŒ {remain:.1f}ë¶„ | ì‹¤íŒ¨ {failed_series}\",\n",
    "                    end=\"\",\n",
    "                    flush=True,\n",
    "                )\n",
    "\n",
    "            if err or not srow:\n",
    "                msg = err or \"unknown\"\n",
    "                append_failed_id(SERIES_FAILED_FILE, sid, msg[:200])\n",
    "                append_done_id(SERIES_DONE_FILE, sid)  # ì‹¤íŒ¨ë„ doneìœ¼ë¡œ ì²˜ë¦¬ (ì¬ì‹œë„ ì›í•˜ë©´ íŒŒì¼ ì§€ìš°ê³ )\n",
    "                failed_series += 1\n",
    "                continue\n",
    "\n",
    "            # Series CSVì— append ì €ì¥\n",
    "            pd.DataFrame([srow]).to_csv(\n",
    "                SERIES_OUT,\n",
    "                mode=\"a\",\n",
    "                index=False,\n",
    "                encoding=\"utf-8-sig\",\n",
    "                header=write_header_s,\n",
    "                quoting=csv.QUOTE_NONNUMERIC,\n",
    "            )\n",
    "            write_header_s = False\n",
    "\n",
    "            append_done_id(SERIES_DONE_FILE, sid)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâ¹ [Series] KeyboardInterrupt! í˜„ì¬ê¹Œì§€ ì €ì¥ëœ ë‚´ìš© ìœ ì§€\")\n",
    "    finally:\n",
    "        # ì„¸ì…˜ì€ ì—í”¼ì†Œë“œ ë‹¨ê³„ì—ì„œë„ ê³„ì† ì‚¬ìš©í•˜ë¯€ë¡œ ì—¬ê¸°ì„œ ë‹«ì§€ ì•ŠìŒ\n",
    "        pass\n",
    "\n",
    "    elapsed_series = time.time() - total_start\n",
    "    print(f\"\\n\\nâ±  [Series] ì†Œìš”ì‹œê°„: {elapsed_series / 3600:.2f}ì‹œê°„\")\n",
    "    print(f\"âœ… [Series] ì²˜ë¦¬ ì™„ë£Œ: {processed_series:,}ê°œ, ì‹¤íŒ¨: {failed_series}ê°œ\\n\")\n",
    "\n",
    "    # ---------------- ì‹œë¦¬ì¦ˆ ì¤‘ë³µ ì œê±° + ì»¬ëŸ¼ ì •ë ¬ ----------------\n",
    "    if Path(SERIES_OUT).exists():\n",
    "        print(\"ğŸ“Š [Series] ì¤‘ë³µ ì œê±° ë° ì»¬ëŸ¼ ì •ë ¬ ì¤‘...\")\n",
    "        series_df = pd.read_csv(SERIES_OUT)\n",
    "\n",
    "        series_cols_order = [\n",
    "            \"id\",\"title\",\"type\",\"adult\",\"backdrop_path\",\"created_by\",\n",
    "            \"episode_run_time\",\"first_air_date\",\"genres\",\"genre_ids\",\n",
    "            \"homepage\",\"in_production\",\"languages\",\"last_air_date\",\n",
    "            \"last_episode_to_air_id\",\"last_episode_to_air_name\",\n",
    "            \"last_episode_to_air_overview\",\"last_episode_to_air_vote_average\",\n",
    "            \"last_episode_to_air_vote_count\",\"last_episode_to_air_air_date\",\n",
    "            \"last_episode_to_air_episode_number\",\"last_episode_to_air_production_code\",\n",
    "            \"last_episode_to_air_runtime\",\"last_episode_to_air_season_number\",\n",
    "            \"last_episode_to_air_show_id\",\"last_episode_to_air_still_path\",\n",
    "            \"name\",\"next_episode_to_air\",\"networks\",\"number_of_episodes\",\n",
    "            \"number_of_seasons\",\"origin_country\",\"original_language\",\n",
    "            \"original_name\",\"overview\",\"popularity\",\"poster_path\",\n",
    "            \"production_companies\",\"production_countries\",\"seasons\",\n",
    "            \"spoken_languages\",\"status\",\"tagline\",\"type_detail\",\n",
    "            \"vote_average\",\"vote_count\",\"review\",\"keyword\",\n",
    "            \"top_cast\",\"directors\",\"writers\",\"created by\",\"providers\",\n",
    "        ]\n",
    "\n",
    "        # ëˆ„ë½ ì»¬ëŸ¼ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "        for col in series_cols_order:\n",
    "            if col not in series_df.columns:\n",
    "                series_df[col] = None\n",
    "\n",
    "        series_df = series_df[series_cols_order]\n",
    "\n",
    "        before = len(series_df)\n",
    "        series_df = series_df.drop_duplicates(subset=[\"id\"], keep=\"first\")\n",
    "        after = len(series_df)\n",
    "        removed = before - after\n",
    "\n",
    "        if removed > 0:\n",
    "            print(f\"âš ï¸ [Series] ì¤‘ë³µ ì œê±°: {removed:,}í–‰ ì œê±°\")\n",
    "\n",
    "        series_df.to_csv(\n",
    "            SERIES_OUT,\n",
    "            index=False,\n",
    "            encoding=\"utf-8-sig\",\n",
    "            quoting=csv.QUOTE_NONNUMERIC,\n",
    "        )\n",
    "        print(f\"ğŸ’¾ [Series] ìµœì¢… ì €ì¥: {SERIES_OUT} (ì´ {after:,}ê°œ)\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # STEP 3) vote_count ê¸°ì¤€ Top 10,000 ì‹œë¦¬ì¦ˆ ì„ ì •\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“Œ STEP 3: vote_count ê¸°ì¤€ Top 10,000 ì‹œë¦¬ì¦ˆ ì„ ì •\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    if not Path(SERIES_OUT).exists():\n",
    "        raise RuntimeError(f\"{SERIES_OUT} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Series ë‹¨ê³„ê°€ ëë‚¬ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "    series_df = pd.read_csv(SERIES_OUT)\n",
    "\n",
    "    # vote_count ê²°ì¸¡ â†’ 0ìœ¼ë¡œ ëŒ€ì²´, ìˆ«ìí˜• ë³€í™˜\n",
    "    series_df[\"vote_count\"] = pd.to_numeric(series_df[\"vote_count\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # 0ë³´ë‹¤ í° ê²ƒë“¤ë§Œ ëŒ€ìƒ (í•„ìš”í•˜ë©´ 0ë„ í¬í•¨ ê°€ëŠ¥)\n",
    "    valid = series_df[series_df[\"vote_count\"] > 0].copy()\n",
    "\n",
    "    # ì •ë ¬ ê¸°ì¤€: vote_count ë‚´ë¦¼ì°¨ìˆœ, tie-breakerë¡œ popularity/vote_averageë„ ì¶”ê°€ ê°€ëŠ¥\n",
    "    valid[\"popularity\"] = pd.to_numeric(valid[\"popularity\"], errors=\"coerce\").fillna(0.0)\n",
    "    valid[\"vote_average\"] = pd.to_numeric(valid[\"vote_average\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    valid = valid.sort_values(\n",
    "        by=[\"vote_count\", \"popularity\", \"vote_average\"],\n",
    "        ascending=[False, False, False],\n",
    "    )\n",
    "\n",
    "    top10k = valid.head(10000).copy()\n",
    "    top10k[[\"id\", \"title\", \"vote_count\", \"popularity\", \"vote_average\"]].to_csv(\n",
    "        TOP10K_OUT,\n",
    "        index=False,\n",
    "        encoding=\"utf-8-sig\",\n",
    "        quoting=csv.QUOTE_NONNUMERIC,\n",
    "    )\n",
    "\n",
    "    top_ids = top10k[\"id\"].astype(int).tolist()\n",
    "    print(f\"âœ… Top 10,000 ì‹œë¦¬ì¦ˆ ì„ ì • ì™„ë£Œ â†’ {TOP10K_OUT}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # STEP 4) Episode ë‹¨ê³„ - Top 10,000 ì „ì²´ ì—í”¼ì†Œë“œ ìˆ˜ì§‘\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“Œ STEP 4: Top 10,000 ì‹œë¦¬ì¦ˆ ì „ì²´ ì—í”¼ì†Œë“œ ìˆ˜ì§‘ ì‹œì‘\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    done_episode = load_done_set(EPISODE_DONE_FILE)\n",
    "    ids_to_process_ep = [sid for sid in top_ids if sid not in done_episode]\n",
    "\n",
    "    print(f\"âœ… ì´ë¯¸ ì™„ë£Œëœ ì‹œë¦¬ì¦ˆ(Episode ë‹¨ê³„): {len(done_episode):,}ê°œ\")\n",
    "    print(f\"ğŸ“Œ ì´ë²ˆì— ì²˜ë¦¬í•  ì‹œë¦¬ì¦ˆ(Episode ë‹¨ê³„): {len(ids_to_process_ep):,}ê°œ\\n\")\n",
    "\n",
    "    write_header_e = not Path(EPISODE_OUT).exists()\n",
    "\n",
    "    total_start_ep = time.time()\n",
    "    failed_ep_series = 0\n",
    "    processed_ep_series = 0\n",
    "    total_ep_series = len(ids_to_process_ep)\n",
    "\n",
    "    async def ep_series_worker(sid: int):\n",
    "        # Top 10,000 ì‹œë¦¬ì¦ˆ ê°ê°ì— ëŒ€í•´ episode ì „ë¶€ ìˆ˜ì§‘\n",
    "        return await process_one_series_for_episode_stage(sid)\n",
    "\n",
    "    ep_tasks = [asyncio.create_task(ep_series_worker(sid)) for sid in ids_to_process_ep]\n",
    "\n",
    "    try:\n",
    "        for coro in asyncio.as_completed(ep_tasks):\n",
    "            sid, ep_rows, _, err = await coro\n",
    "\n",
    "            processed_ep_series += 1\n",
    "            if processed_ep_series % 10 == 0 or processed_ep_series == 1:\n",
    "                elapsed = time.time() - total_start_ep\n",
    "                rate = processed_ep_series / (elapsed / 60) if elapsed > 0 else 0\n",
    "                remain = (total_ep_series - processed_ep_series) / rate if rate > 0 else 0\n",
    "                print(\n",
    "                    f\"\\r[Episode] ì§„í–‰: {processed_ep_series}/{total_ep_series} \"\n",
    "                    f\"({processed_ep_series / total_ep_series * 100:.1f}%) | \"\n",
    "                    f\"ì†ë„ {rate:.1f}/ë¶„ | ë‚¨ìŒ {remain:.1f}ë¶„ | ì‹¤íŒ¨ {failed_ep_series}\",\n",
    "                    end=\"\",\n",
    "                    flush=True,\n",
    "                )\n",
    "\n",
    "            if err or ep_rows is None:\n",
    "                msg = err or \"unknown\"\n",
    "                append_failed_id(EPISODE_FAILED_FILE, sid, msg[:200])\n",
    "                append_done_id(EPISODE_DONE_FILE, sid)\n",
    "                failed_ep_series += 1\n",
    "                continue\n",
    "\n",
    "            if ep_rows:\n",
    "                # ì§€ì •í•œ EPISODE_COLUMNS ìˆœì„œì— ë§ì¶° ì €ì¥\n",
    "                ep_df = pd.DataFrame(ep_rows).reindex(columns=EPISODE_COLUMNS)\n",
    "                ep_df.to_csv(\n",
    "                    EPISODE_OUT,\n",
    "                    mode=\"a\",\n",
    "                    index=False,\n",
    "                    encoding=\"utf-8-sig\",\n",
    "                    header=write_header_e,\n",
    "                )\n",
    "                write_header_e = False\n",
    "\n",
    "            append_done_id(EPISODE_DONE_FILE, sid)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâ¹ [Episode] KeyboardInterrupt! í˜„ì¬ê¹Œì§€ ì €ì¥ëœ ë‚´ìš© ìœ ì§€\")\n",
    "    finally:\n",
    "        if session and not session.closed:\n",
    "            await session.close()\n",
    "\n",
    "    elapsed_ep = time.time() - total_start_ep\n",
    "    print(f\"\\n\\nâ±  [Episode] ì†Œìš”ì‹œê°„: {elapsed_ep / 3600:.2f}ì‹œê°„\")\n",
    "    print(f\"âœ… [Episode] ì‹œë¦¬ì¦ˆ ë‹¨ìœ„ ì²˜ë¦¬ ì™„ë£Œ: {processed_ep_series:,}ê°œ, ì‹¤íŒ¨: {failed_ep_series}ê°œ\\n\")\n",
    "\n",
    "    # ----------- ì—í”¼ì†Œë“œ ì¤‘ë³µ ì œê±° + ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬ -----------\n",
    "    if Path(EPISODE_OUT).exists():\n",
    "        print(\"ğŸ“Š [Episode] ì¤‘ë³µ ì œê±° ë° ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬ ì¤‘...\")\n",
    "        ep_df_all = pd.read_csv(EPISODE_OUT)\n",
    "        before_ep = len(ep_df_all)\n",
    "        ep_df_all = ep_df_all.drop_duplicates(\n",
    "            subset=[\"series_id\", \"season_number\", \"episode_number\"],\n",
    "            keep=\"first\",\n",
    "        )\n",
    "        after_ep = len(ep_df_all)\n",
    "        removed_ep = before_ep - after_ep\n",
    "\n",
    "        if removed_ep > 0:\n",
    "            print(f\"âš ï¸ [Episode] ì¤‘ë³µ ì œê±°: {removed_ep:,}í–‰ ì œê±°\")\n",
    "\n",
    "        ep_df_all = ep_df_all.reindex(columns=EPISODE_COLUMNS)\n",
    "        ep_df_all.to_csv(EPISODE_OUT, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"ğŸ’¾ [Episode] ìµœì¢… ì €ì¥: {EPISODE_OUT} (ì´ {after_ep:,}ê°œ)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ğŸ“Œ Series CSV:      {SERIES_OUT}\")\n",
    "    print(f\"ğŸ“Œ Top10k ë¦¬ìŠ¤íŠ¸:    {TOP10K_OUT}\")\n",
    "    print(f\"ğŸ“Œ Episode CSV:     {EPISODE_OUT}\")\n",
    "    print(f\"ğŸ“Œ Series done_set: {SERIES_DONE_FILE}\")\n",
    "    print(f\"ğŸ“Œ Series ì‹¤íŒ¨ ë¡œê·¸:{SERIES_FAILED_FILE}\")\n",
    "    print(f\"ğŸ“Œ Episode done_set:{EPISODE_DONE_FILE}\")\n",
    "    print(f\"ğŸ“Œ Episode ì‹¤íŒ¨ ë¡œê·¸:{EPISODE_FAILED_FILE}\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Œ Discover ì „ì²´ ì‹œë¦¬ì¦ˆ ID ìˆ˜ì§‘ ì‹œì‘\n",
      "================================================================================\n",
      "ğŸ“… 2016-01-01 ~ 2025-11-29 â†’ total_pages=4967, total_results=99328\n",
      "  ğŸ“… 2016-01-01 ~ 2020-12-15 â†’ total_pages=1945, total_results=38889\n",
      "    ğŸ“… 2016-01-01 ~ 2018-06-24 â†’ total_pages=855, total_results=17089\n",
      "      ğŸ“… 2016-01-01 ~ 2017-03-28 â†’ total_pages=410, total_results=8194\n",
      "      ğŸ“… 2017-03-29 ~ 2018-06-24 â†’ total_pages=445, total_results=8895\n",
      "    ğŸ“… 2018-06-25 ~ 2020-12-15 â†’ total_pages=1090, total_results=21800\n",
      "      ğŸ“… 2018-06-25 ~ 2019-09-20 â†’ total_pages=500, total_results=9991\n",
      "        ğŸ“… 2018-06-25 ~ 2019-02-06 â†’ total_pages=252, total_results=5022\n",
      "        ğŸ“… 2019-02-07 ~ 2019-09-20 â†’ total_pages=249, total_results=4969\n",
      "      ğŸ“… 2019-09-21 ~ 2020-12-15 â†’ total_pages=591, total_results=11809\n",
      "        ğŸ“… 2019-09-21 ~ 2020-05-03 â†’ total_pages=304, total_results=6079\n",
      "        ğŸ“… 2020-05-04 ~ 2020-12-15 â†’ total_pages=287, total_results=5730\n",
      "  ğŸ“… 2020-12-16 ~ 2025-11-29 â†’ total_pages=3022, total_results=60439\n",
      "    ğŸ“… 2020-12-16 ~ 2023-06-08 â†’ total_pages=1443, total_results=28858\n",
      "      ğŸ“… 2020-12-16 ~ 2022-03-13 â†’ total_pages=720, total_results=14398\n",
      "        ğŸ“… 2020-12-16 ~ 2021-07-30 â†’ total_pages=338, total_results=6751\n",
      "        ğŸ“… 2021-07-31 ~ 2022-03-13 â†’ total_pages=383, total_results=7647\n",
      "      ğŸ“… 2022-03-14 ~ 2023-06-08 â†’ total_pages=723, total_results=14460\n",
      "        ğŸ“… 2022-03-14 ~ 2022-10-25 â†’ total_pages=360, total_results=7184\n",
      "        ğŸ“… 2022-10-26 ~ 2023-06-08 â†’ total_pages=364, total_results=7276\n",
      "    ğŸ“… 2023-06-09 ~ 2025-11-29 â†’ total_pages=1580, total_results=31581\n",
      "      ğŸ“… 2023-06-09 ~ 2024-09-03 â†’ total_pages=837, total_results=16737\n",
      "        ğŸ“… 2023-06-09 ~ 2024-01-21 â†’ total_pages=405, total_results=8096\n",
      "        ğŸ“… 2024-01-22 ~ 2024-09-03 â†’ total_pages=433, total_results=8641\n",
      "      ğŸ“… 2024-09-04 ~ 2025-11-29 â†’ total_pages=743, total_results=14844\n",
      "        ğŸ“… 2024-09-04 ~ 2025-04-17 â†’ total_pages=415, total_results=8298\n",
      "        ğŸ“… 2025-04-18 ~ 2025-11-29 â†’ total_pages=328, total_results=6546\n",
      "\n",
      "âœ… Discover ì™„ë£Œ: ê³ ìœ  ì‹œë¦¬ì¦ˆ 99,328ê°œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Œ STEP 2: TV series ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘\n",
      "================================================================================\n",
      "âœ… ì´ë¯¸ ì™„ë£Œëœ ì‹œë¦¬ì¦ˆ(Series ë‹¨ê³„): 0ê°œ\n",
      "ğŸ“Œ ì´ë²ˆì— ì²˜ë¦¬í•  ì‹œë¦¬ì¦ˆ(Series ë‹¨ê³„): 99,328ê°œ\n",
      "\n",
      "[Series] ì§„í–‰: 99320/99328 (100.0%) | ì†ë„ 848.2/ë¶„ | ë‚¨ìŒ 0.0ë¶„ | ì‹¤íŒ¨ 00\n",
      "\n",
      "â±  [Series] ì†Œìš”ì‹œê°„: 1.95ì‹œê°„\n",
      "âœ… [Series] ì²˜ë¦¬ ì™„ë£Œ: 99,328ê°œ, ì‹¤íŒ¨: 0ê°œ\n",
      "\n",
      "ğŸ“Š [Series] ì¤‘ë³µ ì œê±° ë° ì»¬ëŸ¼ ì •ë ¬ ì¤‘...\n",
      "ğŸ’¾ [Series] ìµœì¢… ì €ì¥: tv_series_2016_2025_DISCOVER_v4.csv (ì´ 99,328ê°œ)\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Œ STEP 3: vote_count ê¸°ì¤€ Top 10,000 ì‹œë¦¬ì¦ˆ ì„ ì •\n",
      "================================================================================\n",
      "âœ… Top 10,000 ì‹œë¦¬ì¦ˆ ì„ ì • ì™„ë£Œ â†’ tv_top10000_by_votecount.csv\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Œ STEP 4: Top 10,000 ì‹œë¦¬ì¦ˆ ì „ì²´ ì—í”¼ì†Œë“œ ìˆ˜ì§‘ ì‹œì‘\n",
      "================================================================================\n",
      "âœ… ì´ë¯¸ ì™„ë£Œëœ ì‹œë¦¬ì¦ˆ(Episode ë‹¨ê³„): 0ê°œ\n",
      "ğŸ“Œ ì´ë²ˆì— ì²˜ë¦¬í•  ì‹œë¦¬ì¦ˆ(Episode ë‹¨ê³„): 10,000ê°œ\n",
      "\n",
      "[Episode] ì§„í–‰: 10000/10000 (100.0%) | ì†ë„ 58.0/ë¶„ | ë‚¨ìŒ 0.0ë¶„ | ì‹¤íŒ¨ 0\n",
      "\n",
      "â±  [Episode] ì†Œìš”ì‹œê°„: 2.87ì‹œê°„\n",
      "âœ… [Episode] ì‹œë¦¬ì¦ˆ ë‹¨ìœ„ ì²˜ë¦¬ ì™„ë£Œ: 10,000ê°œ, ì‹¤íŒ¨: 0ê°œ\n",
      "\n",
      "ğŸ“Š [Episode] ì¤‘ë³µ ì œê±° ë° ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xr/h3sspt_54ygby4y5666yc_380000gn/T/ipykernel_26301/750079805.py:933: DtypeWarning: Columns (11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ep_df_all = pd.read_csv(EPISODE_OUT)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ [Episode] ìµœì¢… ì €ì¥: tv_episodes_top10000_FULL.csv (ì´ 285,415ê°œ)\n",
      "\n",
      "================================================================================\n",
      "ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\n",
      "================================================================================\n",
      "ğŸ“Œ Series CSV:      tv_series_2016_2025_DISCOVER_v4.csv\n",
      "ğŸ“Œ Top10k ë¦¬ìŠ¤íŠ¸:    tv_top10000_by_votecount.csv\n",
      "ğŸ“Œ Episode CSV:     tv_episodes_top10000_FULL.csv\n",
      "ğŸ“Œ Series done_set: series_done.txt\n",
      "ğŸ“Œ Series ì‹¤íŒ¨ ë¡œê·¸:series_failed.txt\n",
      "ğŸ“Œ Episode done_set:episode_series_done.txt\n",
      "ğŸ“Œ Episode ì‹¤íŒ¨ ë¡œê·¸:episode_series_failed.txt\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
