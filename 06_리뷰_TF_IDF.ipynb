{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "í† í”½ë³„ ë¦¬ë·° ì •ë³´ TF-IDF\n",
    "\n",
    "ë¦¬ë·° íŒŒì¼ ë¡œë“œ ë¶ˆê°€ë¡œ ì‹¤í–‰ ë¶ˆê°€ - ê²½ë¡œë§Œ ìˆ˜ì •\n",
    "\n",
    "ê²°ê³¼ íŒŒì¼ì€ ë”°ë¡œ ì—†ìŠµë‹ˆë‹¤."
   ],
   "id": "a87b2107ad90d9b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T04:33:44.557091Z",
     "start_time": "2025-12-29T04:33:18.996330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ëª¨ë“ˆ\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ì„¤ì •\n",
    "REVIEW_FILE_PATH = \"files/final_files/review_all.parquet\"\n",
    "OUTPUT_DIR = \"files/review_tfidf_analysis\"\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)"
   ],
   "id": "ea830476fd0424b5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-29T05:10:25.319442Z",
     "start_time": "2025-12-29T05:10:25.318319Z"
    }
   },
   "source": [
    "# ë¦¬ë·° ë°ì´í„°\n",
    "df_review = pd.read_parquet(REVIEW_FILE_PATH)\n",
    "\n",
    "# BERTopic ê²°ê³¼ íŒŒì¼ë“¤ (ì•ì—ì„œ ìƒì„±í•œ íŒŒì¼ë“¤)\n",
    "\n",
    "# ë“œë¼ë§ˆ í† í”½ ë§µ\n",
    "df_hit_drama = pd.read_parquet('files/bertopic_results/drama_hit/drama_hit_topics.parquet')\n",
    "df_flop_drama = pd.read_parquet('files/bertopic_results/drama_flop/drama_flop_topics.parquet')\n",
    "\n",
    "# ì˜í™” í† í”½ ë§µ\n",
    "df_hit_movie = pd.read_parquet('files/bertopic_results/movie_hit/movie_hit_topics.parquet')\n",
    "df_flop_movie = pd.read_parquet('files/bertopic_results/movie_flop/movie_flop_topics.parquet')\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„° ì •ë³´ ë¡œë“œ (ìˆë‹¤ë©´)\n",
    "try:\n",
    "    df_hit_drama_cluster = pd.read_csv('files/bertopic_results/drama_hit/topic_clusters.csv')\n",
    "    df_flop_drama_cluster = pd.read_csv('files/bertopic_results/drama_flop/topic_clusters.csv')\n",
    "    df_hit_movie_cluster = pd.read_csv('files/bertopic_results/movie_hit/topic_clusters.csv')\n",
    "    df_flop_movie_cluster = pd.read_csv('files/bertopic_results/movie_flop/topic_clusters.csv')\n",
    "\n",
    "    # topic â†’ cluster ë§¤í•‘ ìƒì„±\n",
    "    topic_to_cluster_hit_drama = dict(zip(df_hit_drama_cluster['topic_num'], df_hit_drama_cluster['cluster']))\n",
    "    topic_to_cluster_flop_drama = dict(zip(df_flop_drama_cluster['topic_num'], df_flop_drama_cluster['cluster']))\n",
    "    topic_to_cluster_hit_movie = dict(zip(df_hit_movie_cluster['topic_num'], df_hit_movie_cluster['cluster']))\n",
    "    topic_to_cluster_flop_movie = dict(zip(df_flop_movie_cluster['topic_num'], df_flop_movie_cluster['cluster']))\n",
    "\n",
    "    # í† í”½ ë§µì— í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶”ê°€\n",
    "    df_hit_drama['cluster'] = df_hit_drama['topic'].map(topic_to_cluster_hit_drama)\n",
    "    df_flop_drama['cluster'] = df_flop_drama['topic'].map(topic_to_cluster_flop_drama)\n",
    "    df_hit_movie['cluster'] = df_hit_movie['topic'].map(topic_to_cluster_hit_movie)\n",
    "    df_flop_movie['cluster'] = df_flop_movie['topic'].map(topic_to_cluster_flop_movie)\n",
    "\n",
    "except Exception as e:\n",
    "    # í´ëŸ¬ìŠ¤í„° ì •ë³´ê°€ ì—†ìœ¼ë©´ topicì„ clusterë¡œ ì‚¬ìš©\n",
    "    df_hit_drama['cluster'] = df_hit_drama['topic']\n",
    "    df_flop_drama['cluster'] = df_flop_drama['topic']\n",
    "    df_hit_movie['cluster'] = df_hit_movie['topic']\n",
    "    df_flop_movie['cluster'] = df_flop_movie['topic']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_top_tfidf_terms(tfidf_matrix, feature_names, top_n=20):\n",
    "    \"\"\"\n",
    "    TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ì—ì„œ ìƒìœ„ Nê°œ ë‹¨ì–´ ì¶”ì¶œ\n",
    "\n",
    "    Args:\n",
    "        tfidf_matrix: TF-IDF í–‰ë ¬\n",
    "        feature_names: íŠ¹ì„± ì´ë¦„ (ë‹¨ì–´)\n",
    "        top_n: ì¶”ì¶œí•  ìƒìœ„ ë‹¨ì–´ ê°œìˆ˜\n",
    "\n",
    "    Returns:\n",
    "        list: (ë‹¨ì–´, ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    mean_tfidf = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "    top_indices = mean_tfidf.argsort()[-top_n:][::-1]\n",
    "\n",
    "    top_terms = [(feature_names[i], mean_tfidf[i]) for i in top_indices]\n",
    "    return top_terms\n",
    "\n",
    "\n",
    "def analyze_sentiment_tfidf(df_review, df_topic_map, content_type, hit_status, group_by='topic'):\n",
    "    \"\"\"\n",
    "    íŠ¹ì • ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ í† í”½/í´ëŸ¬ìŠ¤í„°ë³„ sentiment ë¶„ì„\n",
    "\n",
    "    Args:\n",
    "        df_review: ë¦¬ë·° ë°ì´í„°í”„ë ˆì„\n",
    "        df_topic_map: í† í”½ ë§¤í•‘ ë°ì´í„°í”„ë ˆì„\n",
    "        content_type: 'movie' ë˜ëŠ” 'drama'\n",
    "        hit_status: 'hit' ë˜ëŠ” 'flop'\n",
    "        group_by: 'topic' ë˜ëŠ” 'cluster'\n",
    "\n",
    "    Returns:\n",
    "        dict: ë¶„ì„ ê²°ê³¼\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ” {hit_status.upper()} {content_type.upper()} - Group by {group_by.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # ë¦¬ë·°ì™€ í† í”½ ë§µ ë³‘í•©\n",
    "    df_merged = df_review.merge(\n",
    "        df_topic_map[['imdb_id', 'topic', 'cluster']],\n",
    "        on='imdb_id',\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    results = {}\n",
    "    groups = df_merged[group_by].unique()\n",
    "    groups = [g for g in groups if g != -1]  # outlier ì œì™¸\n",
    "\n",
    "    for group_id in sorted(groups):\n",
    "        group_data = df_merged[df_merged[group_by] == group_id].copy()\n",
    "\n",
    "        # ìµœì†Œ ë¦¬ë·° ê°œìˆ˜ í™•ì¸\n",
    "        if len(group_data) < 40:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- {group_by.capitalize()} {group_id} ---\")\n",
    "        print(f\"   ì´ ë¦¬ë·°: {len(group_data):,}ê°œ\")\n",
    "\n",
    "        # sentiment_scoreë¡œ ì •ë ¬\n",
    "        group_data_sorted = group_data.sort_values('sentiment_score', ascending=False)\n",
    "        top20 = group_data_sorted.head(20)\n",
    "        bottom20 = group_data_sorted.tail(20)\n",
    "\n",
    "        print(f\"   ê¸ì •(Top 20) ë²”ìœ„: {top20['sentiment_score'].min():.4f} ~ {top20['sentiment_score'].max():.4f}\")\n",
    "        print(f\"   ë¶€ì •(Bottom 20) ë²”ìœ„: {bottom20['sentiment_score'].min():.4f} ~ {bottom20['sentiment_score'].max():.4f}\")\n",
    "\n",
    "        results[group_id] = {}\n",
    "\n",
    "        # Top20 TF-IDF ë¶„ì„ (ê¸ì • ë¦¬ë·°)\n",
    "        try:\n",
    "            vectorizer_top = TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                ngram_range=(2, 2),  # 2-gram\n",
    "                min_df=2,\n",
    "                max_df=0.9,\n",
    "                token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                max_features=1500\n",
    "            )\n",
    "            tfidf_matrix_top = vectorizer_top.fit_transform(top20['review_text_clean'])\n",
    "            feature_names_top = vectorizer_top.get_feature_names_out()\n",
    "            top_terms_top20 = get_top_tfidf_terms(tfidf_matrix_top, feature_names_top, top_n=20)\n",
    "\n",
    "            for term, score in top_terms_top20[:5]:\n",
    "                print(f\"      â€¢ {term}: {score:.4f}\")\n",
    "\n",
    "            results[group_id]['top20'] = {\n",
    "                'terms': top_terms_top20,\n",
    "                'sentiment_range': (top20['sentiment_score'].min(), top20['sentiment_score'].max()),\n",
    "                'avg_sentiment': top20['sentiment_score'].mean()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            results[group_id]['top20'] = None\n",
    "\n",
    "        # Bottom20 TF-IDF ë¶„ì„ (ë¶€ì • ë¦¬ë·°)\n",
    "        try:\n",
    "            vectorizer_bottom = TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                ngram_range=(2, 2),\n",
    "                min_df=2,\n",
    "                max_df=0.9,\n",
    "                token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                max_features=1500\n",
    "            )\n",
    "            tfidf_matrix_bottom = vectorizer_bottom.fit_transform(bottom20['review_text_clean'])\n",
    "            feature_names_bottom = vectorizer_bottom.get_feature_names_out()\n",
    "            top_terms_bottom20 = get_top_tfidf_terms(tfidf_matrix_bottom, feature_names_bottom, top_n=20)\n",
    "\n",
    "            print(f\"\\n ë¶€ì • ë¦¬ë·° ìƒìœ„ TF-IDF ë‹¨ì–´:\")\n",
    "            for term, score in top_terms_bottom20[:5]:\n",
    "                print(f\"      â€¢ {term}: {score:.4f}\")\n",
    "\n",
    "            results[group_id]['bottom20'] = {\n",
    "                'terms': top_terms_bottom20,\n",
    "                'sentiment_range': (bottom20['sentiment_score'].min(), bottom20['sentiment_score'].max()),\n",
    "                'avg_sentiment': bottom20['sentiment_score'].mean()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            results[group_id]['bottom20'] = None\n",
    "\n",
    "    return results"
   ],
   "id": "6f1c51e4ae03b4bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸš€ ì „ì²´ ë¶„ì„ ì‹œì‘\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "# 1. Hit Drama - Topic\n",
    "all_results['hit_drama_topic'] = analyze_sentiment_tfidf(\n",
    "    df_review, df_hit_drama, 'drama', 'hit', group_by='topic'\n",
    ")\n",
    "\n",
    "# 2. Hit Drama - Cluster\n",
    "all_results['hit_drama_cluster'] = analyze_sentiment_tfidf(\n",
    "    df_review, df_hit_drama, 'drama', 'hit', group_by='cluster'\n",
    ")\n",
    "\n",
    "# 3. Flop Drama - Topic\n",
    "all_results['flop_drama_topic'] = analyze_sentiment_tfidf(\n",
    "    df_review, df_flop_drama, 'drama', 'flop', group_by='topic'\n",
    ")\n",
    "\n",
    "# 4. Flop Drama - Cluster\n",
    "all_results['flop_drama_cluster'] = analyze_sentiment_tfidf(\n",
    "    df_review, df_flop_drama, 'drama', 'flop', group_by='cluster'\n",
    ")\n",
    "\n",
    "# 5. Hit Movie - Topic\n",
    "all_results['hit_movie_topic'] = analyze_sentiment_tfidf(\n",
    "    df_review, df_hit_movie, 'movie', 'hit', group_by='topic'\n",
    ")\n",
    "\n",
    "# 6. Hit Movie - Cluster\n",
    "all_results['hit_movie_cluster'] = analyze_sentiment_tfidf(\n",
    "    df_review, df_hit_movie, 'movie', 'hit', group_by='cluster'\n",
    ")\n",
    "\n",
    "# 7. Flop Movie - Topic\n",
    "all_results['flop_movie_topic'] = analyze_sentiment_tfidf(\n",
    "    df_review, df_flop_movie, 'movie', 'flop', group_by='topic'\n",
    ")\n",
    "\n",
    "# 8. Flop Movie - Cluster\n",
    "all_results['flop_movie_cluster'] = analyze_sentiment_tfidf(\n",
    "    df_review, df_flop_movie, 'movie', 'flop', group_by='cluster'\n",
    ")"
   ],
   "id": "a4f6ece66192bdb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ì¹´í…Œê³ ë¦¬ë³„ ê°œë³„ íŒŒì¼ ì €ì¥\n",
    "for category, results in all_results.items():\n",
    "    rows = []\n",
    "\n",
    "    for group_id, group_results in results.items():\n",
    "        if group_results is None:\n",
    "            continue\n",
    "\n",
    "        # Top20 ê²°ê³¼ (ê¸ì •)\n",
    "        if group_results.get('top20'):\n",
    "            for rank, (term, score) in enumerate(group_results['top20']['terms'], 1):\n",
    "                rows.append({\n",
    "                    'category': category,\n",
    "                    'group_type': category.split('_')[-1],  # 'topic' or 'cluster'\n",
    "                    'group_id': group_id,\n",
    "                    'sentiment_type': 'high',\n",
    "                    'rank': rank,\n",
    "                    'term': term,\n",
    "                    'tfidf_score': score,\n",
    "                    'avg_sentiment': group_results['top20']['avg_sentiment'],\n",
    "                    'min_sentiment': group_results['top20']['sentiment_range'][0],\n",
    "                    'max_sentiment': group_results['top20']['sentiment_range'][1]\n",
    "                })\n",
    "\n",
    "        # Bottom20 ê²°ê³¼ (ë¶€ì •)\n",
    "        if group_results.get('bottom20'):\n",
    "            for rank, (term, score) in enumerate(group_results['bottom20']['terms'], 1):\n",
    "                rows.append({\n",
    "                    'category': category,\n",
    "                    'group_type': category.split('_')[-1],\n",
    "                    'group_id': group_id,\n",
    "                    'sentiment_type': 'low',\n",
    "                    'rank': rank,\n",
    "                    'term': term,\n",
    "                    'tfidf_score': score,\n",
    "                    'avg_sentiment': group_results['bottom20']['avg_sentiment'],\n",
    "                    'min_sentiment': group_results['bottom20']['sentiment_range'][0],\n",
    "                    'max_sentiment': group_results['bottom20']['sentiment_range'][1]\n",
    "                })\n",
    "\n",
    "    if rows:\n",
    "        df_result = pd.DataFrame(rows)\n",
    "        output_file = f'{OUTPUT_DIR}/{category}_tfidf_results.csv'\n",
    "        df_result.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"{output_file} ({len(df_result):,}í–‰)\")"
   ],
   "id": "b9c16c4a1aa117c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ì „ì²´ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ì €ì¥\n",
    "all_rows = []\n",
    "\n",
    "for category, results in all_results.items():\n",
    "    for group_id, group_results in results.items():\n",
    "        if group_results is None:\n",
    "            continue\n",
    "\n",
    "        # Top20\n",
    "        if group_results.get('top20'):\n",
    "            for rank, (term, score) in enumerate(group_results['top20']['terms'], 1):\n",
    "                all_rows.append({\n",
    "                    'category': category,\n",
    "                    'group_type': category.split('_')[-1],\n",
    "                    'group_id': group_id,\n",
    "                    'sentiment_type': 'high',\n",
    "                    'rank': rank,\n",
    "                    'term': term,\n",
    "                    'tfidf_score': score,\n",
    "                    'avg_sentiment': group_results['top20']['avg_sentiment'],\n",
    "                    'min_sentiment': group_results['top20']['sentiment_range'][0],\n",
    "                    'max_sentiment': group_results['top20']['sentiment_range'][1]\n",
    "                })\n",
    "\n",
    "        # Bottom20\n",
    "        if group_results.get('bottom20'):\n",
    "            for rank, (term, score) in enumerate(group_results['bottom20']['terms'], 1):\n",
    "                all_rows.append({\n",
    "                    'category': category,\n",
    "                    'group_type': category.split('_')[-1],\n",
    "                    'group_id': group_id,\n",
    "                    'sentiment_type': 'low',\n",
    "                    'rank': rank,\n",
    "                    'term': term,\n",
    "                    'tfidf_score': score,\n",
    "                    'avg_sentiment': group_results['bottom20']['avg_sentiment'],\n",
    "                    'min_sentiment': group_results['bottom20']['sentiment_range'][0],\n",
    "                    'max_sentiment': group_results['bottom20']['sentiment_range'][1]\n",
    "                })\n",
    "\n",
    "if all_rows:\n",
    "    df_all_results = pd.DataFrame(all_rows)\n",
    "    output_file = f'{OUTPUT_DIR}/all_sentiment_tfidf_results.csv'\n",
    "    df_all_results.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"í†µí•© ê²°ê³¼ ì €ì¥: {output_file} ({len(df_all_results):,}í–‰)\")\n",
    "else:\n",
    "    print(\"ì €ì¥í•  ê²°ê³¼ ì—†ìŒ\")\n",
    "    df_all_results = pd.DataFrame()"
   ],
   "id": "ac1e757b0fcdba5b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
