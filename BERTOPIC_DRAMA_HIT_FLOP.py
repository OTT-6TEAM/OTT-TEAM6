"""
ë“œë¼ë§ˆ í¥í–‰/ë¹„í¥í–‰ BERTopic ë¶„ì„
- ì‚¬ì „ ê³„ì‚°ëœ ì„ë² ë”©(Qwen/Qwen3-Embedding-0.6B) í™œìš©
- hit_score ê¸°ì¤€ ìƒìœ„ 20% = í¥í–‰, í•˜ìœ„ 40% = ë¹„í¥í–‰
- ëª¨ë“  ì¶œë ¥ë¬¼ì€ 'ë“œë¼ë§ˆë°ì´í„°BERTOPIC' í´ë”ì— ì €ì¥
"""

import pandas as pd
import numpy as np
import os
from bertopic import BERTopic
from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings('ignore')

# ==========================================================
# 0. ì¶œë ¥ í´ë” ìƒì„±
# ==========================================================
OUTPUT_DIR = "files/ë“œë¼ë§ˆë°ì´í„°BERTOPIC"
os.makedirs(OUTPUT_DIR, exist_ok=True)
print(f"ì¶œë ¥ í´ë” ìƒì„±: {OUTPUT_DIR}/")

# ==========================================================
# 1. ë¶ˆìš©ì–´ ì„¤ì •
# ==========================================================
base_stopwords = list(ENGLISH_STOP_WORDS)

additional_drama_stopwords = [
    # ========== ë“œë¼ë§ˆ í¬ë§·/ë©”íƒ€ ==========
    'tv', 'television', 'show', 'series', 'episode', 'episodes',
    'season', 'seasons', 'installment',
    'pilot', 'finale',

    # ========== ì œì‘/í˜•ì‹ ì •ë³´ ==========
    'drama', 'dramas',
    'network', 'broadcast', 'air', 'airs',
    'production', 'produced',
    'creator', 'creators',
    'cast', 'crew',
    'actor', 'actors', 'actress', 'actresses',
    'director', 'directors',
    'writer', 'writers',

    # ========== ì¤„ê±°ë¦¬ ì„œìˆ  ìƒíˆ¬ì–´ ==========
    'story', 'stories', 'plot',
    'follows', 'following',
    'centers', 'centred', 'revolves',
    'tells', 'depicts', 'chronicles',
    'focuses', 'explores',
    'takes', 'place',
    'begins', 'starts', 'ends',
    'finds', 'discovers', 'faces', 'way', 'actually', 'la',

    # ========== ì¼ë°˜ì  ì‹œê°„ í‘œí˜„ ==========
    'time', 'times',
    'day', 'days',
    'year', 'years',
    'night', 'nights',
    'past', 'present', 'future',
    'later', 'earlier', 'soon',

    # ========== ìˆœì„œ/ì „ê°œ í‘œí˜„ ==========
    'first', 'second', 'third',
    'last', 'next', 'previous',
    'early', 'late',

    # ========== ì¼ë°˜ì  ì¸ë¬¼ ì§€ì¹­ ==========
    'man', 'woman', 'men', 'women',
    'person', 'people',
    'group', 'groups',
    'team', 'teams',
    'members', 'characters',
    # ========== ë„ˆë¬´ ì¼ë°˜ì ì¸ ì‚¬ê±´ í˜•ìš©ì‚¬ ==========
    'high', 'characters', 'just', 'new',
    # ========== ë„ˆë¬´ ì¼ë°˜ì ì¸ ì‚¬ê±´ ë™ì‚¬ ==========
    'life', 'lives',
    'work', 'works',
    'deal', 'deals', 'step', 'gets','decides',
    'struggle', 'struggles', 'make', 'sees', 'set',
    # ========== ê³ ìœ ëª…ì‚¬ ==========
    'ryan', 'henry', 'james', 'xun', 'gu', 'ma ri', 'ri', 'ma', 'fernanda', 'rosendo', 'tyler',
    'carmina','mariela', 'lou'
    # ë¶ˆìš©ì–´ì— ì¶”ê°€ ê°€ëŠ¥
    'Ã¶ykÃ¼', 'demir', 'hanzawa', 'leonardo', 'damiÃ¡n', 'eva', 'elisa', 'esteban', 'tori', "eliseo", "sam", "ellen","charlotte", "jarndyce","alex", 
]

english_stopwords_drama = list(set(base_stopwords + additional_drama_stopwords))
print(f"ì¶”ê°€ ë¶ˆìš©ì–´ ìˆ˜: {len(additional_drama_stopwords)}ê°œ")
print(f"ìµœì¢… ë¶ˆìš©ì–´ ìˆ˜: {len(english_stopwords_drama)}ê°œ")

# ==========================================================
# 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ==========================================================
print("\n" + "="*60)
print("ë°ì´í„° ë¡œë“œ ì¤‘...")
print("="*60)

# ë°ì´í„° ë¡œë“œ (ê²½ë¡œëŠ” ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
drama_df = pd.read_parquet(r"files/final_files/drama/drama_text_embedding_qwen3.parquet")
hit_score_df = pd.read_parquet("files/final_files/00_hit_score.parquet")

print(f"ë“œë¼ë§ˆ ë°ì´í„°: {len(drama_df)}ê°œ")
print(f"Hit Score ë°ì´í„°: {len(hit_score_df)}ê°œ")

# Left Join
df_merged = drama_df.merge(hit_score_df, on='imdb_id', how='left')
print(f"ë³‘í•© í›„ ë°ì´í„°: {len(df_merged)}ê°œ")

# hit_scoreê°€ ìˆëŠ” ë°ì´í„°ë§Œ í•„í„°ë§
df_with_score = df_merged[df_merged['hit_score'].notna()].copy()
print(f"hit_scoreê°€ ìˆëŠ” ë°ì´í„°: {len(df_with_score)}ê°œ")

# ==========================================================
# 3. í¥í–‰/ë¹„í¥í–‰ ë¶„ë¥˜ (ìƒìœ„ 20%, í•˜ìœ„ 20%)
# ==========================================================
print("\n" + "="*60)
print("í¥í–‰/ë¹„í¥í–‰ ë¶„ë¥˜ ì¤‘...")
print("="*60)

# í¼ì„¼íƒ€ì¼ ê³„ì‚°
hit_threshold = df_with_score['hit_score'].quantile(0.80)  # ìƒìœ„ 20% ê²½ê³„
flop_threshold = df_with_score['hit_score'].quantile(0.40)  # í•˜ìœ„ 20% ê²½ê³„

print(f"ìƒìœ„ 20% ê²½ê³„ (hit_score >= {hit_threshold:.4f}): í¥í–‰")
print(f"í•˜ìœ„ 40% ê²½ê³„ (hit_score <= {flop_threshold:.4f}): ë¹„í¥í–‰")

# ë¶„ë¥˜
df_hit = df_with_score[df_with_score['hit_score'] >= hit_threshold].copy()
df_flop = df_with_score[df_with_score['hit_score'] <= flop_threshold].copy()

print(f"\ní¥í–‰ì‘ ìˆ˜: {len(df_hit)}ê°œ")
print(f"ë¹„í¥í–‰ì‘ ìˆ˜: {len(df_flop)}ê°œ")

# ==========================================================
# 4. ì„ë² ë”© ì¶”ì¶œ
# ==========================================================
print("\n" + "="*60)
print("ì„ë² ë”© ì¶”ì¶œ ì¤‘...")
print("="*60)

# embedding ì»¬ëŸ¼ì—ì„œ numpy arrayë¡œ ë³€í™˜
embeddings_hit = np.vstack(df_hit['embedding'].values)
embeddings_flop = np.vstack(df_flop['embedding'].values)

print(f"í¥í–‰ì‘ ì„ë² ë”© shape: {embeddings_hit.shape}")
print(f"ë¹„í¥í–‰ì‘ ì„ë² ë”© shape: {embeddings_flop.shape}")

# í…ìŠ¤íŠ¸ ì¤€ë¹„ (combined_text ì‚¬ìš©)
texts_hit = df_hit['combined_text'].tolist()
texts_flop = df_flop['combined_text'].tolist()

# ==========================================================
# 5. BERTopic ëª¨ë¸ ìƒì„± í•¨ìˆ˜
# ==========================================================

# ì„ë² ë”© ëª¨ë¸ (BERTopic ë‚´ë¶€ìš© - ì‹¤ì œë¡œëŠ” ì‚¬ì „ ê³„ì‚°ëœ ì„ë² ë”© ì‚¬ìš©)
embedding_model = SentenceTransformer('Qwen/Qwen3-Embedding-0.6B')

def create_bertopic_model(n_neighbors, min_cluster_size, stopwords):
    """
    BERTopic ëª¨ë¸ ìƒì„±
    
    Args:
        n_neighbors: UMAP ì´ì›ƒ ìˆ˜ (ì‘ì„ìˆ˜ë¡ ì„¸ë°€í•œ í† í”½)
        min_cluster_size: HDBSCAN ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸° (ì‘ì„ìˆ˜ë¡ í† í”½ ìˆ˜ ì¦ê°€)
        stopwords: ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
    """
    # CountVectorizer ì„¤ì •
    vectorizer_model = CountVectorizer(
        stop_words=stopwords,
        ngram_range=(1, 2),
        min_df=1,
        max_df=0.95
    )
    
    return BERTopic(
        embedding_model=embedding_model,
        
        # UMAP: ì°¨ì› ì¶•ì†Œ
        umap_model=UMAP(
            n_neighbors=n_neighbors,
            n_components=10,
            min_dist=0.05,
            metric='cosine',
            random_state=42
        ),
        
        # HDBSCAN: ë°€ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
        hdbscan_model=HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=3,
            metric='euclidean',
            cluster_selection_method='leaf',
            prediction_data=True
        ),
        
        vectorizer_model=vectorizer_model,
        verbose=True
    )

# ==========================================================
# 6. í¥í–‰ì‘ í† í”½ ë¶„ì„
# ==========================================================
print("\n" + "="*60)
print("í¥í–‰ì‘ BERTopic ë¶„ì„")
print("="*60)

# íŒŒë¼ë¯¸í„° ì„¤ì •
hit_n_neighbors = min(10, len(df_hit) - 1)
hit_min_cluster = max(15, len(df_hit) // 100)

print(f"í¥í–‰ì‘ ìˆ˜: {len(df_hit)}")
print(f"UMAP n_neighbors: {hit_n_neighbors}")
print(f"HDBSCAN min_cluster_size: {hit_min_cluster}")

# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
hit_topic_model = create_bertopic_model(hit_n_neighbors, hit_min_cluster, english_stopwords_drama)

print("\ní¥í–‰ì‘ BERTopic ëª¨ë¸ í•™ìŠµ ì¤‘...")
# BERTopic fit_transform ì‹œ textsë¥¼ ì¤„ê±°ë¦¬ë§Œìœ¼ë¡œ ë³€ê²½
texts_hit_for_ctfidf = df_hit['overview'].tolist()  # ì¤„ê±°ë¦¬ë§Œ

topics_hit, probs_hit = hit_topic_model.fit_transform(
    texts_hit_for_ctfidf,  # â† c-TF-IDFìš© í…ìŠ¤íŠ¸ (ì¤„ê±°ë¦¬ë§Œ)
    embeddings=embeddings_hit  # â† ì„ë² ë”©ì€ ê¸°ì¡´ ê²ƒ ì‚¬ìš© (ì¥ë¥´+ì¤„ê±°ë¦¬)
)

# documents ì—ëŠ” í…ìŠ¤íŠ¸ë¥¼, embeddings ì—ëŠ” ë²¡í„°ë¥¼ ë„£ìŠµë‹ˆë‹¤.
new_topics_hit = hit_topic_model.reduce_outliers(
    documents=texts_hit_for_ctfidf,           # ì²« ë²ˆì§¸ ì¸ì: ë°˜ë“œì‹œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    topics=topics_hit,            # ë‘ ë²ˆì§¸ ì¸ì: ê¸°ì¡´ í† í”½ ê²°ê³¼
    strategy="embeddings",          # ì „ëµ ì„ íƒ
    embeddings=embeddings_hit,    # ì„ë² ë”© ë²¡í„° ì§ì ‘ ì „ë‹¬ (ì†ë„ í–¥ìƒ)
    threshold=0.6                   # ìœ ì‚¬ë„ ë¬¸í„±ê°’
)

# 3. ê²°ê³¼ ë°˜ì˜ (í•„ìˆ˜)
hit_topic_model.update_topics(
    texts_hit_for_ctfidf,
    topics=new_topics_hit,
    vectorizer_model = CountVectorizer(
        stop_words=english_stopwords_drama,
        ngram_range=(1, 2),
        min_df=1,
        max_df=0.95
    )
)

# ê²°ê³¼ ì¶œë ¥
hit_topic_info = hit_topic_model.get_topic_info()
print(f"\n[í¥í–‰ì‘ í† í”½ ê°œìš”] - ì´ {len(hit_topic_info) - 1}ê°œ í† í”½ (Topic -1 ì œì™¸)")
print(hit_topic_info)

# ê° í† í”½ë³„ í‚¤ì›Œë“œ ì¶œë ¥
print("\n[í¥í–‰ì‘ í† í”½ë³„ ìƒìœ„ í‚¤ì›Œë“œ]")
for topic_id in hit_topic_info['Topic'].values:
    if topic_id != -1:  # ë…¸ì´ì¦ˆ í† í”½ ì œì™¸
        keywords = hit_topic_model.get_topic(topic_id)
        keyword_str = ", ".join([f"{word}({score:.3f})" for word, score in keywords[:10]])
        print(f"Topic {topic_id}: {keyword_str}")

# ==========================================================
# 7. ë¹„í¥í–‰ì‘ í† í”½ ë¶„ì„
# ==========================================================
print("\n" + "="*60)
print("ë¹„í¥í–‰ì‘ BERTopic ë¶„ì„")
print("="*60)

# íŒŒë¼ë¯¸í„° ì„¤ì •
flop_n_neighbors = min(10, len(df_flop) - 1)
flop_min_cluster = max(15, len(df_flop) // 100)

print(f"ë¹„í¥í–‰ì‘ ìˆ˜: {len(df_flop)}")
print(f"UMAP n_neighbors: {flop_n_neighbors}")
print(f"HDBSCAN min_cluster_size: {flop_min_cluster}")

# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
flop_topic_model = create_bertopic_model(flop_n_neighbors, flop_min_cluster, english_stopwords_drama)

print("\në¹„í¥í–‰ì‘ BERTopic ëª¨ë¸ í•™ìŠµ ì¤‘...")
# â˜…â˜…â˜… ìˆ˜ì •: ì¤„ê±°ë¦¬ë§Œ ì‚¬ìš© â˜…â˜…â˜…
texts_flop_for_ctfidf = df_flop['overview'].tolist()  # ì¤„ê±°ë¦¬ë§Œ

topics_flop, probs_flop = flop_topic_model.fit_transform(
    texts_flop_for_ctfidf,  # â† c-TF-IDFìš© í…ìŠ¤íŠ¸ (ì¤„ê±°ë¦¬ë§Œ)
    embeddings=embeddings_flop  # â† ì„ë² ë”©ì€ ê¸°ì¡´ ê²ƒ ì‚¬ìš© (ì¥ë¥´+ì¤„ê±°ë¦¬)
)

# documents ì—ëŠ” í…ìŠ¤íŠ¸ë¥¼, embeddings ì—ëŠ” ë²¡í„°ë¥¼ ë„£ìŠµë‹ˆë‹¤.
new_topics_flop = flop_topic_model.reduce_outliers(
    documents=texts_flop_for_ctfidf,           # ì²« ë²ˆì§¸ ì¸ì: ë°˜ë“œì‹œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    topics=topics_flop,            # ë‘ ë²ˆì§¸ ì¸ì: ê¸°ì¡´ í† í”½ ê²°ê³¼
    strategy="embeddings",          # ì „ëµ ì„ íƒ
    embeddings=embeddings_flop,    # ì„ë² ë”© ë²¡í„° ì§ì ‘ ì „ë‹¬ (ì†ë„ í–¥ìƒ)
    threshold=0.6                   # ìœ ì‚¬ë„ ë¬¸í„±ê°’
)

# 3. ê²°ê³¼ ë°˜ì˜ (í•„ìˆ˜)
flop_topic_model.update_topics(
    texts_flop_for_ctfidf,
    topics=new_topics_flop,
    vectorizer_model = CountVectorizer(
        stop_words=english_stopwords_drama,
        ngram_range=(1, 2),
        min_df=1,
        max_df=0.95
    )
)

# ê²°ê³¼ ì¶œë ¥
flop_topic_info = flop_topic_model.get_topic_info()
print(f"\n[ë¹„í¥í–‰ì‘ í† í”½ ê°œìš”] - ì´ {len(flop_topic_info) - 1}ê°œ í† í”½ (Topic -1 ì œì™¸)")
print(flop_topic_info)

# ê° í† í”½ë³„ í‚¤ì›Œë“œ ì¶œë ¥
print("\n[ë¹„í¥í–‰ì‘ í† í”½ë³„ ìƒìœ„ í‚¤ì›Œë“œ]")
for topic_id in flop_topic_info['Topic'].values:
    if topic_id != -1:  # ë…¸ì´ì¦ˆ í† í”½ ì œì™¸
        keywords = flop_topic_model.get_topic(topic_id)
        keyword_str = ", ".join([f"{word}({score:.3f})" for word, score in keywords[:10]])
        print(f"Topic {topic_id}: {keyword_str}")

# ==========================================================
# 8. ê²°ê³¼ ìš”ì•½ ë° ë¹„êµ
# ==========================================================
print("\n" + "="*60)
print("í¥í–‰ vs ë¹„í¥í–‰ í† í”½ ë¹„êµ ìš”ì•½")
print("="*60)

print(f"\n[í¥í–‰ì‘]")
print(f"  - ì´ ë“œë¼ë§ˆ ìˆ˜: {len(df_hit)}")
print(f"  - ë°œê²¬ëœ í† í”½ ìˆ˜: {len(hit_topic_info) - 1}")
print(f"  - ë…¸ì´ì¦ˆ(Topic -1) ë¬¸ì„œ ìˆ˜: {sum(1 for t in new_topics_hit if t == -1)}")

print(f"\n[ë¹„í¥í–‰ì‘]")
print(f"  - ì´ ë“œë¼ë§ˆ ìˆ˜: {len(df_flop)}")
print(f"  - ë°œê²¬ëœ í† í”½ ìˆ˜: {len(flop_topic_info) - 1}")
print(f"  - ë…¸ì´ì¦ˆ(Topic -1) ë¬¸ì„œ ìˆ˜: {sum(1 for t in new_topics_flop if t == -1)}")

# ==========================================================
# 9. ì‹œê°í™” ì €ì¥
# ==========================================================
print("\n" + "="*60)
print("ì‹œê°í™” ìƒì„± ì¤‘...")
print("="*60)

# ----- í¥í–‰ì‘ ì‹œê°í™” -----

# 1) í† í”½ë³„ í‚¤ì›Œë“œ ë°”ì°¨íŠ¸
try:
    fig_hit_barchart = hit_topic_model.visualize_barchart(top_n_topics=10)
    fig_hit_barchart.write_html(f"{OUTPUT_DIR}/hit_topics_barchart.html")
    print(f"  âœ“ {OUTPUT_DIR}/hit_topics_barchart.html")
except Exception as e:
    print(f"  âœ— í¥í–‰ì‘ ë°”ì°¨íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

# 2) í† í”½ ê°„ ê±°ë¦¬ë§µ (Intertopic Distance Map)
try:
    fig_hit_intertopic = hit_topic_model.visualize_topics()
    fig_hit_intertopic.write_html(f"{OUTPUT_DIR}/hit_topics_intertopic.html")
    print(f"  âœ“ {OUTPUT_DIR}/hit_topics_intertopic.html")
except Exception as e:
    print(f"  âœ— í¥í–‰ì‘ ê±°ë¦¬ë§µ ì €ì¥ ì‹¤íŒ¨: {e}")

# 3) ê³„ì¸µì  í† í”½ êµ¬ì¡° (Hierarchical Topics)
try:
    hierarchical_topics_hit = hit_topic_model.hierarchical_topics(texts_hit_for_ctfidf)  # â† ìˆ˜ì •
    fig_hit_hierarchy = hit_topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics_hit)
    fig_hit_hierarchy.write_html(f"{OUTPUT_DIR}/hit_topics_hierarchy.html")
    print(f"  âœ“ {OUTPUT_DIR}/hit_topics_hierarchy.html")
except Exception as e:
    print(f"  âœ— í¥í–‰ì‘ ê³„ì¸µêµ¬ì¡° ì €ì¥ ì‹¤íŒ¨: {e}")

# 4) í† í”½ íˆíŠ¸ë§µ (Topic Similarity Heatmap)
try:
    fig_hit_heatmap = hit_topic_model.visualize_heatmap()
    fig_hit_heatmap.write_html(f"{OUTPUT_DIR}/hit_topics_heatmap.html")
    print(f"  âœ“ {OUTPUT_DIR}/hit_topics_heatmap.html")
except Exception as e:
    print(f"  âœ— í¥í–‰ì‘ íˆíŠ¸ë§µ ì €ì¥ ì‹¤íŒ¨: {e}")

# 5) ë¬¸ì„œ-í† í”½ ë¶„í¬ (Document Distribution)
try:
    fig_hit_docs = hit_topic_model.visualize_documents(
        texts_hit_for_ctfidf,  # â† ìˆ˜ì •
        embeddings=embeddings_hit,
        hide_annotations=True
    )
    fig_hit_docs.write_html(f"{OUTPUT_DIR}/hit_topics_documents.html")
    print(f"  âœ“ {OUTPUT_DIR}/hit_topics_documents.html")
except Exception as e:
    print(f"  âœ— í¥í–‰ì‘ ë¬¸ì„œë¶„í¬ ì €ì¥ ì‹¤íŒ¨: {e}")

# ----- ë¹„í¥í–‰ì‘ ì‹œê°í™” -----

# 1) í† í”½ë³„ í‚¤ì›Œë“œ ë°”ì°¨íŠ¸
try:
    fig_flop_barchart = flop_topic_model.visualize_barchart(top_n_topics=10)
    fig_flop_barchart.write_html(f"{OUTPUT_DIR}/flop_topics_barchart.html")
    print(f"  âœ“ {OUTPUT_DIR}/flop_topics_barchart.html")
except Exception as e:
    print(f"  âœ— ë¹„í¥í–‰ì‘ ë°”ì°¨íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

# 2) í† í”½ ê°„ ê±°ë¦¬ë§µ
try:
    fig_flop_intertopic = flop_topic_model.visualize_topics()
    fig_flop_intertopic.write_html(f"{OUTPUT_DIR}/flop_topics_intertopic.html")
    print(f"  âœ“ {OUTPUT_DIR}/flop_topics_intertopic.html")
except Exception as e:
    print(f"  âœ— ë¹„í¥í–‰ì‘ ê±°ë¦¬ë§µ ì €ì¥ ì‹¤íŒ¨: {e}")

# 3) ê³„ì¸µì  í† í”½ êµ¬ì¡°
try:
    hierarchical_topics_flop = flop_topic_model.hierarchical_topics(texts_flop_for_ctfidf)  # â† ìˆ˜ì •
    fig_flop_hierarchy = flop_topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics_flop)
    fig_flop_hierarchy.write_html(f"{OUTPUT_DIR}/flop_topics_hierarchy.html")
    print(f"  âœ“ {OUTPUT_DIR}/flop_topics_hierarchy.html")
except Exception as e:
    print(f"  âœ— ë¹„í¥í–‰ì‘ ê³„ì¸µêµ¬ì¡° ì €ì¥ ì‹¤íŒ¨: {e}")

# 4) í† í”½ íˆíŠ¸ë§µ
try:
    fig_flop_heatmap = flop_topic_model.visualize_heatmap()
    fig_flop_heatmap.write_html(f"{OUTPUT_DIR}/flop_topics_heatmap.html")
    print(f"  âœ“ {OUTPUT_DIR}/flop_topics_heatmap.html")
except Exception as e:
    print(f"  âœ— ë¹„í¥í–‰ì‘ íˆíŠ¸ë§µ ì €ì¥ ì‹¤íŒ¨: {e}")

# 5) ë¬¸ì„œ-í† í”½ ë¶„í¬
try:
    fig_flop_docs = flop_topic_model.visualize_documents(
        texts_flop_for_ctfidf,  # â† ìˆ˜ì •
        embeddings=embeddings_flop,
        hide_annotations=True
    )
    fig_flop_docs.write_html(f"{OUTPUT_DIR}/flop_topics_documents.html")
    print(f"  âœ“ {OUTPUT_DIR}/flop_topics_documents.html")
except Exception as e:
    print(f"  âœ— ë¹„í¥í–‰ì‘ ë¬¸ì„œë¶„í¬ ì €ì¥ ì‹¤íŒ¨: {e}")


# ==========================================================
# 10-1. Representative_Docsì— ë“œë¼ë§ˆ ì œëª© ë§¤í•‘ (ì¶”ê°€ ì½”ë“œ)
# ==========================================================
print("\n" + "="*60)
print("Representative_Docsì— ë“œë¼ë§ˆ ì œëª© ë§¤í•‘ ì¤‘...")
print("="*60)

import ast

def map_representative_docs_to_titles(topic_info_df, texts_list, titles_list):
    """
    Representative_Docsì˜ ì¤„ê±°ë¦¬ë¥¼ ë“œë¼ë§ˆ ì œëª©ê³¼ ë§¤í•‘
    
    Args:
        topic_info_df: BERTopicì˜ get_topic_info() ê²°ê³¼ DataFrame
        texts_list: fit_transformì— ì‚¬ìš©ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (overview)
        titles_list: ëŒ€ì‘ë˜ëŠ” ì œëª© ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì œëª©ì´ ì¶”ê°€ëœ DataFrame
    """
    # í…ìŠ¤íŠ¸ -> ì œëª© ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    text_to_title = {text: title for text, title in zip(texts_list, titles_list)}
    
    # Representative_Docs_Titles ì»¬ëŸ¼ ìƒì„±
    representative_titles = []
    
    for idx, row in topic_info_df.iterrows():
        if row['Topic'] == -1:
            representative_titles.append([])
            continue
            
        rep_docs = row['Representative_Docs']
        
        # rep_docsê°€ ë¬¸ìì—´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if isinstance(rep_docs, str):
            try:
                rep_docs = ast.literal_eval(rep_docs)
            except:
                rep_docs = [rep_docs]
        
        # ê° ëŒ€í‘œ ë¬¸ì„œì— ëŒ€ì‘í•˜ëŠ” ì œëª© ì°¾ê¸°
        titles = []
        for doc in rep_docs:
            title = text_to_title.get(doc, "ì œëª© ì—†ìŒ")
            titles.append(title)
        
        representative_titles.append(titles)
    
    # ìƒˆ ì»¬ëŸ¼ ì¶”ê°€
    topic_info_df = topic_info_df.copy()
    topic_info_df['Representative_Docs_Titles'] = representative_titles
    
    return topic_info_df

# ----- í¥í–‰ì‘ ì²˜ë¦¬ -----
titles_hit = df_hit['title'].tolist()

hit_topic_info_with_titles = map_representative_docs_to_titles(
    hit_topic_info, 
    texts_hit_for_ctfidf, 
    titles_hit
)

# CSV ì €ì¥ (ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜)
hit_topic_info_with_titles_csv = hit_topic_info_with_titles.copy()
hit_topic_info_with_titles_csv['Representative_Docs_Titles'] = hit_topic_info_with_titles_csv['Representative_Docs_Titles'].apply(
    lambda x: ' | '.join(x) if isinstance(x, list) else x
)
hit_topic_info_with_titles_csv.to_csv(f"{OUTPUT_DIR}/hit_topic_info_with_titles.csv", index=False, encoding='utf-8-sig')
print(f"  âœ“ {OUTPUT_DIR}/hit_topic_info_with_titles.csv")

# ----- ë¹„í¥í–‰ì‘ ì²˜ë¦¬ -----
titles_flop = df_flop['title'].tolist()

flop_topic_info_with_titles = map_representative_docs_to_titles(
    flop_topic_info, 
    texts_flop_for_ctfidf, 
    titles_flop
)

# CSV ì €ì¥
flop_topic_info_with_titles_csv = flop_topic_info_with_titles.copy()
flop_topic_info_with_titles_csv['Representative_Docs_Titles'] = flop_topic_info_with_titles_csv['Representative_Docs_Titles'].apply(
    lambda x: ' | '.join(x) if isinstance(x, list) else x
)
flop_topic_info_with_titles_csv.to_csv(f"{OUTPUT_DIR}/flop_topic_info_with_titles.csv", index=False, encoding='utf-8-sig')
print(f"  âœ“ {OUTPUT_DIR}/flop_topic_info_with_titles.csv")

# ----- ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° -----
print("\n[í¥í–‰ì‘ í† í”½ë³„ ëŒ€í‘œ ë“œë¼ë§ˆ]")
for _, row in hit_topic_info_with_titles.iterrows():
    if row['Topic'] != -1:
        titles_str = ", ".join(row['Representative_Docs_Titles'][:3])
        print(f"  Topic {row['Topic']}: {titles_str}")

print("\n[ë¹„í¥í–‰ì‘ í† í”½ë³„ ëŒ€í‘œ ë“œë¼ë§ˆ]")
for _, row in flop_topic_info_with_titles.iterrows():
    if row['Topic'] != -1:
        titles_str = ", ".join(row['Representative_Docs_Titles'][:3])
        print(f"  Topic {row['Topic']}: {titles_str}")


# ==========================================================
# 10. ë°ì´í„° íŒŒì¼ ì €ì¥
# ==========================================================
print("\n" + "="*60)
print("ë°ì´í„° íŒŒì¼ ì €ì¥ ì¤‘...")
print("="*60)

# ----- í¥í–‰ì‘ ê²°ê³¼ ì €ì¥ -----

# 1) ë“œë¼ë§ˆë³„ í† í”½ í• ë‹¹ ê²°ê³¼
df_hit_result = df_hit[['imdb_id', 'title', 'combined_text', 'hit_score']].copy()
df_hit_result['topic'] = new_topics_hit

# â˜… ìˆ˜ì •: probs í˜•íƒœì— ë”°ë¼ ì²˜ë¦¬ (ì—ëŸ¬ ë°©ì§€)
if isinstance(probs_hit, np.ndarray) and probs_hit.ndim == 1:
    df_hit_result['topic_prob'] = probs_hit
else:
    df_hit_result['topic_prob'] = [max(p) if hasattr(p, '__len__') and len(p) > 0 else float(p) for p in probs_hit]

df_hit_result.to_parquet(f"{OUTPUT_DIR}/hit_drama_topics.parquet", index=False)
df_hit_result.to_csv(f"{OUTPUT_DIR}/hit_drama_topics.csv", index=False, encoding='utf-8-sig')
print(f"  âœ“ {OUTPUT_DIR}/hit_drama_topics.parquet")
print(f"  âœ“ {OUTPUT_DIR}/hit_drama_topics.csv")

# 2) í† í”½ ì •ë³´ ìš”ì•½
hit_topic_info.to_csv(f"{OUTPUT_DIR}/hit_topic_info.csv", index=False, encoding='utf-8-sig')
print(f"  âœ“ {OUTPUT_DIR}/hit_topic_info.csv")

# 3) í† í”½ë³„ ìƒì„¸ í‚¤ì›Œë“œ
hit_keywords_data = []
for topic_id in hit_topic_info['Topic'].values:
    if topic_id != -1:
        keywords = hit_topic_model.get_topic(topic_id)
        for rank, (word, score) in enumerate(keywords[:20], 1):
            hit_keywords_data.append({
                'topic': topic_id,
                'rank': rank,
                'keyword': word,
                'score': score
            })
df_hit_keywords = pd.DataFrame(hit_keywords_data)
df_hit_keywords.to_csv(f"{OUTPUT_DIR}/hit_topic_keywords.csv", index=False, encoding='utf-8-sig')
print(f"  âœ“ {OUTPUT_DIR}/hit_topic_keywords.csv")

# ----- ë¹„í¥í–‰ì‘ ê²°ê³¼ ì €ì¥ -----

# 1) ë“œë¼ë§ˆë³„ í† í”½ í• ë‹¹ ê²°ê³¼
df_flop_result = df_flop[['imdb_id', 'title', 'combined_text', 'hit_score']].copy()
df_flop_result['topic'] = new_topics_flop

# â˜… ìˆ˜ì •: probs í˜•íƒœì— ë”°ë¼ ì²˜ë¦¬ (ì—ëŸ¬ ë°©ì§€)
if isinstance(probs_flop, np.ndarray) and probs_flop.ndim == 1:
    df_flop_result['topic_prob'] = probs_flop
else:
    df_flop_result['topic_prob'] = [max(p) if hasattr(p, '__len__') and len(p) > 0 else float(p) for p in probs_flop]

df_flop_result.to_parquet(f"{OUTPUT_DIR}/flop_drama_topics.parquet", index=False)
df_flop_result.to_csv(f"{OUTPUT_DIR}/flop_drama_topics.csv", index=False, encoding='utf-8-sig')
print(f"  âœ“ {OUTPUT_DIR}/flop_drama_topics.parquet")
print(f"  âœ“ {OUTPUT_DIR}/flop_drama_topics.csv")

# 2) í† í”½ ì •ë³´ ìš”ì•½
flop_topic_info.to_csv(f"{OUTPUT_DIR}/flop_topic_info.csv", index=False, encoding='utf-8-sig')
print(f"  âœ“ {OUTPUT_DIR}/flop_topic_info.csv")

# 3) í† í”½ë³„ ìƒì„¸ í‚¤ì›Œë“œ
flop_keywords_data = []
for topic_id in flop_topic_info['Topic'].values:
    if topic_id != -1:
        keywords = flop_topic_model.get_topic(topic_id)
        for rank, (word, score) in enumerate(keywords[:20], 1):
            flop_keywords_data.append({
                'topic': topic_id,
                'rank': rank,
                'keyword': word,
                'score': score
            })
df_flop_keywords = pd.DataFrame(flop_keywords_data)
df_flop_keywords.to_csv(f"{OUTPUT_DIR}/flop_topic_keywords.csv", index=False, encoding='utf-8-sig')
print(f"  âœ“ {OUTPUT_DIR}/flop_topic_keywords.csv")

# ==========================================================
# 11. BERTopic ëª¨ë¸ ì €ì¥ (ì„ íƒì  - ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ í•„ìš”)
# ==========================================================
print("\n" + "="*60)
print("BERTopic ëª¨ë¸ ì €ì¥ ì¤‘...")
print("="*60)

# â˜… ìˆ˜ì •: safetensors ì‚¬ìš© + embedding_model ì œì™¸ (ìš©ëŸ‰ ì ˆì•½)
try:
    hit_topic_model.save(
        f"{OUTPUT_DIR}/hit_bertopic_model", 
        serialization="safetensors", 
        save_ctfidf=True, 
        save_embedding_model=False
    )
    print(f"  âœ“ {OUTPUT_DIR}/hit_bertopic_model/")
except Exception as e:
    print(f"  âœ— í¥í–‰ì‘ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

try:
    flop_topic_model.save(
        f"{OUTPUT_DIR}/flop_bertopic_model", 
        serialization="safetensors", 
        save_ctfidf=True, 
        save_embedding_model=False
    )
    print(f"  âœ“ {OUTPUT_DIR}/flop_bertopic_model/")
except Exception as e:
    print(f"  âœ— ë¹„í¥í–‰ì‘ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

# ==========================================================
# 12. ë¶„ì„ ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
# ==========================================================
print("\n" + "="*60)
print("ë¶„ì„ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
print("="*60)

report = f"""
================================================================================
                    ë“œë¼ë§ˆ í¥í–‰/ë¹„í¥í–‰ BERTopic ë¶„ì„ ë¦¬í¬íŠ¸
================================================================================

â–  ë¶„ì„ ê°œìš”
  - ë¶„ì„ ëŒ€ìƒ: hit_scoreê°€ ìˆëŠ” ë“œë¼ë§ˆ {len(df_with_score)}ê°œ
  - í¥í–‰ ê¸°ì¤€: hit_score ìƒìœ„ 20% (>= {hit_threshold:.4f})
  - ë¹„í¥í–‰ ê¸°ì¤€: hit_score í•˜ìœ„ 40% (<= {flop_threshold:.4f})
  - ì„ë² ë”© ëª¨ë¸: Qwen/Qwen3-Embedding-0.6B

================================================================================
â–  í¥í–‰ì‘ ë¶„ì„ ê²°ê³¼
================================================================================
  - ë¶„ì„ ëŒ€ìƒ ìˆ˜: {len(df_hit)}ê°œ
  - ë°œê²¬ëœ í† í”½ ìˆ˜: {len(hit_topic_info) - 1}ê°œ
  - ë…¸ì´ì¦ˆ(ë¯¸ë¶„ë¥˜) ë¬¸ì„œ ìˆ˜: {sum(1 for t in new_topics_hit if t == -1)}ê°œ
  - í´ëŸ¬ìŠ¤í„°ë§ íŒŒë¼ë¯¸í„°:
    Â· UMAP n_neighbors: {hit_n_neighbors}
    Â· HDBSCAN min_cluster_size: {hit_min_cluster}

  [í† í”½ë³„ ìš”ì•½]
"""

for _, row in hit_topic_info.iterrows():
    if row['Topic'] != -1:
        keywords = hit_topic_model.get_topic(row['Topic'])
        top_keywords = ", ".join([w for w, s in keywords[:5]])
        report += f"    Topic {row['Topic']}: {row['Count']}ê°œ ë¬¸ì„œ - {top_keywords}\n"

report += f"""
================================================================================
â–  ë¹„í¥í–‰ì‘ ë¶„ì„ ê²°ê³¼
================================================================================
  - ë¶„ì„ ëŒ€ìƒ ìˆ˜: {len(df_flop)}ê°œ
  - ë°œê²¬ëœ í† í”½ ìˆ˜: {len(flop_topic_info) - 1}ê°œ
  - ë…¸ì´ì¦ˆ(ë¯¸ë¶„ë¥˜) ë¬¸ì„œ ìˆ˜: {sum(1 for t in new_topics_flop if t == -1)}ê°œ
  - í´ëŸ¬ìŠ¤í„°ë§ íŒŒë¼ë¯¸í„°:
    Â· UMAP n_neighbors: {flop_n_neighbors}
    Â· HDBSCAN min_cluster_size: {flop_min_cluster}

  [í† í”½ë³„ ìš”ì•½]
"""

for _, row in flop_topic_info.iterrows():
    if row['Topic'] != -1:
        keywords = flop_topic_model.get_topic(row['Topic'])
        top_keywords = ", ".join([w for w, s in keywords[:5]])
        report += f"    Topic {row['Topic']}: {row['Count']}ê°œ ë¬¸ì„œ - {top_keywords}\n"

report += f"""
================================================================================
â–  ì¶œë ¥ íŒŒì¼ ëª©ë¡
================================================================================

[ì‹œê°í™” íŒŒì¼ - HTML (ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°)]
  í¥í–‰ì‘:
    - hit_topics_barchart.html    : í† í”½ë³„ ìƒìœ„ í‚¤ì›Œë“œ ë§‰ëŒ€ê·¸ë˜í”„
    - hit_topics_intertopic.html  : í† í”½ ê°„ ê±°ë¦¬/ìœ ì‚¬ë„ ë§µ
    - hit_topics_hierarchy.html   : í† í”½ ê³„ì¸µ êµ¬ì¡° (ë´ë“œë¡œê·¸ë¨)
    - hit_topics_heatmap.html     : í† í”½ ê°„ ìœ ì‚¬ë„ íˆíŠ¸ë§µ
    - hit_topics_documents.html   : ë¬¸ì„œ ë¶„í¬ ì‹œê°í™” (2D ì‚°ì ë„)

  ë¹„í¥í–‰ì‘:
    - flop_topics_barchart.html   : í† í”½ë³„ ìƒìœ„ í‚¤ì›Œë“œ ë§‰ëŒ€ê·¸ë˜í”„
    - flop_topics_intertopic.html : í† í”½ ê°„ ê±°ë¦¬/ìœ ì‚¬ë„ ë§µ
    - flop_topics_hierarchy.html  : í† í”½ ê³„ì¸µ êµ¬ì¡° (ë´ë“œë¡œê·¸ë¨)
    - flop_topics_heatmap.html    : í† í”½ ê°„ ìœ ì‚¬ë„ íˆíŠ¸ë§µ
    - flop_topics_documents.html  : ë¬¸ì„œ ë¶„í¬ ì‹œê°í™” (2D ì‚°ì ë„)

[ë°ì´í„° íŒŒì¼ - CSV/Parquet]
  í¥í–‰ì‘:
    - hit_drama_topics.csv/parquet : ê° ë“œë¼ë§ˆì˜ í† í”½ í• ë‹¹ ê²°ê³¼
    - hit_topic_info.csv           : í† í”½ ìš”ì•½ ì •ë³´ (ë¬¸ì„œ ìˆ˜, ëŒ€í‘œ í‚¤ì›Œë“œ)
    - hit_topic_keywords.csv       : í† í”½ë³„ ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ ë° ì ìˆ˜

  ë¹„í¥í–‰ì‘:
    - flop_drama_topics.csv/parquet : ê° ë“œë¼ë§ˆì˜ í† í”½ í• ë‹¹ ê²°ê³¼
    - flop_topic_info.csv           : í† í”½ ìš”ì•½ ì •ë³´ (ë¬¸ì„œ ìˆ˜, ëŒ€í‘œ í‚¤ì›Œë“œ)
    - flop_topic_keywords.csv       : í† í”½ë³„ ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ ë° ì ìˆ˜

[ëª¨ë¸ íŒŒì¼ - ì¬ì‚¬ìš© ê°€ëŠ¥]
    - hit_bertopic_model/  : í¥í–‰ì‘ BERTopic ëª¨ë¸
    - flop_bertopic_model/ : ë¹„í¥í–‰ì‘ BERTopic ëª¨ë¸

================================================================================
â–  íŒŒì¼ ì„¤ëª…
================================================================================

1. *_topics_barchart.html
   - ê° í† í”½ì˜ ëŒ€í‘œ í‚¤ì›Œë“œì™€ c-TF-IDF ì ìˆ˜ë¥¼ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ í‘œì‹œ
   - í† í”½ì˜ ì£¼ì œë¥¼ ë¹ ë¥´ê²Œ íŒŒì•…í•  ë•Œ ì‚¬ìš©

2. *_topics_intertopic.html
   - í† í”½ë“¤ì„ 2D ê³µê°„ì— ë°°ì¹˜í•˜ì—¬ ìœ ì‚¬í•œ í† í”½ë¼ë¦¬ ê°€ê¹Œì´ ìœ„ì¹˜
   - ì›ì˜ í¬ê¸°ëŠ” í•´ë‹¹ í† í”½ì˜ ë¬¸ì„œ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ„
   - í† í”½ ê°„ ê´€ê³„ë¥¼ íŒŒì•…í•  ë•Œ ì‚¬ìš©

3. *_topics_hierarchy.html
   - í† í”½ë“¤ì˜ ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ (ë´ë“œë¡œê·¸ë¨)
   - ìœ ì‚¬í•œ í† í”½ë“¤ì´ ì–´ë–»ê²Œ ê·¸ë£¹í™”ë˜ëŠ”ì§€ í™•ì¸

4. *_topics_heatmap.html
   - í† í”½ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ í‘œì‹œ
   - ì–´ë–¤ í† í”½ë“¤ì´ ì„œë¡œ ìœ ì‚¬í•œì§€ ì •ëŸ‰ì ìœ¼ë¡œ íŒŒì•…

5. *_topics_documents.html
   - ëª¨ë“  ë¬¸ì„œë¥¼ 2D ê³µê°„ì— ì‹œê°í™” (UMAP ì°¨ì› ì¶•ì†Œ)
   - ê° ì ì€ í•˜ë‚˜ì˜ ë“œë¼ë§ˆ, ìƒ‰ìƒì€ í• ë‹¹ëœ í† í”½
   - í´ëŸ¬ìŠ¤í„°ë§ì´ ì˜ ë˜ì—ˆëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ í™•ì¸

6. *_drama_topics.csv
   - imdb_id, title: ë“œë¼ë§ˆ ì‹ë³„ ì •ë³´
   - combined_text: ë¶„ì„ì— ì‚¬ìš©ëœ í…ìŠ¤íŠ¸ (ì¤„ê±°ë¦¬+ì¥ë¥´)
   - hit_score: í¥í–‰ ì ìˆ˜
   - topic: í• ë‹¹ëœ í† í”½ ë²ˆí˜¸ (-1ì€ ë…¸ì´ì¦ˆ/ë¯¸ë¶„ë¥˜)
   - topic_prob: í•´ë‹¹ í† í”½ì— ì†í•  í™•ë¥ 

7. *_topic_info.csv
   - Topic: í† í”½ ë²ˆí˜¸
   - Count: í•´ë‹¹ í† í”½ì— ì†í•œ ë¬¸ì„œ ìˆ˜
   - Name: í† í”½ ëŒ€í‘œ í‚¤ì›Œë“œ ì¡°í•©

8. *_topic_keywords.csv
   - topic: í† í”½ ë²ˆí˜¸
   - rank: í‚¤ì›Œë“œ ìˆœìœ„ (1~20)
   - keyword: í‚¤ì›Œë“œ
   - score: c-TF-IDF ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ í•´ë‹¹ í† í”½ì—ì„œ ì¤‘ìš”)

================================================================================
"""

with open(f"{OUTPUT_DIR}/analysis_report.txt", 'w', encoding='utf-8') as f:
    f.write(report)
print(f"  âœ“ {OUTPUT_DIR}/analysis_report.txt")

# ==========================================================
# ì™„ë£Œ
# ==========================================================
print("\n" + "="*60)
print(f"ë¶„ì„ ì™„ë£Œ! ëª¨ë“  ê²°ê³¼ê°€ '{OUTPUT_DIR}/' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("="*60)

# ì €ì¥ëœ íŒŒì¼ ëª©ë¡ ì¶œë ¥
print(f"\nì €ì¥ëœ íŒŒì¼ ëª©ë¡:")
for item in sorted(os.listdir(OUTPUT_DIR)):
    item_path = os.path.join(OUTPUT_DIR, item)
    if os.path.isdir(item_path):
        print(f"  ğŸ“ {item}/")
    else:
        size = os.path.getsize(item_path)
        if size > 1024*1024:
            print(f"{item} ({size/1024/1024:.1f} MB)")
        elif size > 1024:
            print(f"{item} ({size/1024:.1f} KB)")
        else:
            print(f"{item} ({size} bytes)")