{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b679e757",
   "metadata": {},
   "source": [
    "# 1Îã®Í≥Ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3ba9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 0) hit ÌååÏùº Î°úÎìú\n",
    "# -----------------------------\n",
    "hit = pd.read_parquet('hit_score.parquet')\n",
    "\n",
    "print(\"hit shape:\", hit.shape)\n",
    "print(\"hit cols:\", hit.columns.tolist())\n",
    "\n",
    "# ÌòπÏãú Î™®Î•º imdb_id ÌÉÄÏûÖ ÌÜµÏùº\n",
    "hit[\"imdb_id\"] = hit[\"imdb_id\"].astype(str).str.strip()\n",
    "\n",
    "# -----------------------------\n",
    "# 1) ÎìúÎùºÎßà Î©îÏù∏ ÌååÏùº Î°úÎìú\n",
    "# -----------------------------\n",
    "drama = pd.read_parquet(\"00_drama_main.parquet\")\n",
    "print(\"drama shape:\", drama.shape)\n",
    "print(\"drama cols:\", drama.columns.tolist())\n",
    "\n",
    "# imdb_id ÏóÜÏúºÎ©¥ Ïó¨Í∏∞ÏÑú Î∞îÎ°ú Î©àÏ∂∞Ïïº Ìï®\n",
    "if \"imdb_id\" not in drama.columns:\n",
    "    raise KeyError(\"00_drama_main.parquet Ïóê 'imdb_id' Ïª¨ÎüºÏù¥ ÏóÜÏñ¥. Ïª¨ÎüºÎ™ÖÏùÑ ÌôïÏù∏Ìï¥Ï§ò.\")\n",
    "\n",
    "drama[\"imdb_id\"] = drama[\"imdb_id\"].astype(str).str.strip()\n",
    "\n",
    "# -----------------------------\n",
    "# 2) overview(Ï§ÑÍ±∞Î¶¨) Ïª¨Îüº ÏûêÎèô ÌÉêÏÉâ (ÏûàÏúºÎ©¥ Í∑∏Í±∏Î°ú ÏßÑÌñâ)\n",
    "# -----------------------------\n",
    "overview_candidates = [\"overview\", \"plot\", \"description\", \"summary\", \"storyline\"]\n",
    "overview_col = next((c for c in overview_candidates if c in drama.columns), None)\n",
    "\n",
    "if overview_col is None:\n",
    "    raise KeyError(\n",
    "        f\"Ï§ÑÍ±∞Î¶¨ Ïª¨ÎüºÏùÑ Î™ª Ï∞æÏïòÏñ¥. ÌõÑÎ≥¥={overview_candidates}\\n\"\n",
    "        f\"drama Ïª¨ÎüºÎì§ÏóêÏÑú Ï§ÑÍ±∞Î¶¨ Ïª¨ÎüºÎ™ÖÏù¥ Î≠îÏßÄ ÏïåÎ†§Ï§ò.\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ overview column:\", overview_col)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) hit Ï™Ω imdb_id Ï§ëÎ≥µ Ï≤¥ÌÅ¨ & Ï†ïÎ¶¨ (ÏûàÏúºÎ©¥ ÏûëÌíà Îã®ÏúÑÎ°ú 1Ìñâ ÎßåÎì§Í∏∞)\n",
    "#    - Í∏∞Î≥∏: Í∞ôÏùÄ imdb_idÍ∞Ä Ïó¨Îü¨ ÌñâÏù¥Î©¥ ÌèâÍ∑†ÏúºÎ°ú ÏßëÍ≥Ñ\n",
    "# -----------------------------\n",
    "dup_cnt = hit.duplicated(\"imdb_id\").sum()\n",
    "print(\"hit duplicated imdb_id rows:\", dup_cnt)\n",
    "\n",
    "if dup_cnt > 0:\n",
    "    hit_agg = (\n",
    "        hit.groupby(\"imdb_id\", as_index=False)[[\"rating\", \"num_votes_log\", \"sentiment_score\", \"hit_score\"]]\n",
    "           .mean()\n",
    "    )\n",
    "else:\n",
    "    hit_agg = hit.copy()\n",
    "\n",
    "print(\"hit_agg shape:\", hit_agg.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) merge (ÎìúÎùºÎßà Í∏∞Ï§ÄÏúºÎ°ú hit_score Î∂ôÏù¥Í∏∞)\n",
    "# -----------------------------\n",
    "master = drama.merge(hit_agg[[\"imdb_id\", \"hit_score\"]], on=\"imdb_id\", how=\"left\")\n",
    "\n",
    "print(\"master shape:\", master.shape)\n",
    "print(\"hit_score null rate:\", master[\"hit_score\"].isna().mean())\n",
    "\n",
    "# Î∂ÑÏÑùÏö©ÏúºÎ°ú ÏµúÏÜå Ïª¨ÎüºÎßå Îî∞Î°ú ÎßåÎì† Î≤ÑÏ†ÑÎèÑ ÌïòÎÇò ÎßåÎì§Ïñ¥ÎëêÎ©¥ Ìé∏Ìï®\n",
    "master_min = master[[\"imdb_id\", overview_col, \"hit_score\"]].copy()\n",
    "master_min = master_min.rename(columns={overview_col: \"overview\"})\n",
    "\n",
    "print(\"master_min shape:\", master_min.shape)\n",
    "print(master_min.head())\n",
    "\n",
    "# 5) Ï†ÄÏû•\n",
    "\n",
    "# master.to_parquet(\"01_drama_hit_master_full.parquet\", index=False)\n",
    "master_min.to_parquet(\"01_drama_hit_master_min.parquet\", index=False)\n",
    "\n",
    "print(\"‚úÖ saved:\")\n",
    "print(\" - 01_drama_hit_master_min.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5677e981",
   "metadata": {},
   "source": [
    "# 2Îã®Í≥Ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 0) ÎßàÏä§ÌÑ∞ Î°úÎìú\n",
    "# -----------------------------\n",
    "PATH_MASTER = \"01_drama_hit_master_min.parquet\"   # Ï†ÄÏû•Ìïú ÎßàÏä§ÌÑ∞ ÌååÏùºÎ™ÖÏóê ÎßûÏ∂∞ ÏàòÏ†ï Í∞ÄÎä•\n",
    "df = pd.read_parquet(PATH_MASTER)\n",
    "\n",
    "print(\"loaded:\", df.shape)\n",
    "print(\"cols:\", df.columns.tolist())\n",
    "\n",
    "# -----------------------------\n",
    "# 1) ÌïÑÏàò Ïª¨Îüº Ï≤¥ÌÅ¨\n",
    "# -----------------------------\n",
    "required = [\"imdb_id\", \"overview\", \"hit_score\"]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"ÌïÑÏàò Ïª¨ÎüºÏù¥ ÏóÜÏñ¥: {missing} / ÌòÑÏû¨ Ïª¨Îüº: {df.columns.tolist()}\")\n",
    "\n",
    "# imdb_id Ï†ïÎ¶¨ (ÏïàÏ†Ñ)\n",
    "df[\"imdb_id\"] = df[\"imdb_id\"].astype(str).str.strip()\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Í∏∏Ïù¥ Ïª¨Îüº ÏÉùÏÑ±\n",
    "# -----------------------------\n",
    "# overviewÍ∞Ä NaNÏù¥Î©¥ Í∏∏Ïù¥Î•º 0ÏúºÎ°ú\n",
    "df[\"overview\"] = df[\"overview\"].fillna(\"\").astype(str)\n",
    "df[\"len_overview\"] = df[\"overview\"].str.len()\n",
    "\n",
    "print(\"\\nlen_overview summary:\")\n",
    "print(df[\"len_overview\"].describe(percentiles=[.01,.05,.10,.25,.50,.75,.90,.95,.99]))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) ÌõÑÎ≥¥ Ïª∑(Threshold)Î≥ÑÎ°ú ÌïÑÌÑ∞ÎßÅ Í≤∞Í≥º ÎπÑÍµê\n",
    "# -----------------------------\n",
    "THRESHOLDS = [100, 150, 200]\n",
    "\n",
    "def summarize_for_threshold(data: pd.DataFrame, t: int) -> dict:\n",
    "    # 2Îã®Í≥Ñ ÌïÑÌÑ∞ÎßÅ Í∑úÏπô:\n",
    "    # - overview Í≤∞Ï∏°/ÎπàÎ¨∏ÏûêÏó¥ Ï†úÍ±∞ (len_overview >= tÎ°ú Ïª§Î≤Ñ)\n",
    "    # - hit_score Í≤∞Ï∏° Ï†úÍ±∞\n",
    "    filtered = data.loc[(data[\"len_overview\"] >= t) & (data[\"hit_score\"].notna())].copy()\n",
    "\n",
    "    return {\n",
    "        \"threshold\": t,\n",
    "        \"rows_after\": len(filtered),\n",
    "        \"unique_imdb_id\": filtered[\"imdb_id\"].nunique(),\n",
    "        \"overview_empty_rate_before\": (data[\"len_overview\"].eq(0)).mean(),\n",
    "        \"hit_score_null_rate_before\": data[\"hit_score\"].isna().mean(),\n",
    "        \"hit_score_null_rate_after\": filtered[\"hit_score\"].isna().mean(),\n",
    "        \"len_overview_p50_after\": filtered[\"len_overview\"].median() if len(filtered) else None,\n",
    "        \"len_overview_p10_after\": filtered[\"len_overview\"].quantile(0.10) if len(filtered) else None,\n",
    "    }\n",
    "\n",
    "summary_rows = [summarize_for_threshold(df, t) for t in THRESHOLDS]\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(\"threshold\")\n",
    "\n",
    "print(\"\\nüìå Threshold comparison (Step2 filtering result)\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# -----------------------------\n",
    "# 4) (ÏÑ†ÌÉù) Í∏∞Ï§Ä ÌïòÎÇòÎ•º Í≥®Îùº 2Îã®Í≥Ñ ÏÇ∞Ï∂úÎ¨ºÎ°ú Ï†ÄÏû•\n",
    "# -----------------------------\n",
    "# ÏùºÎã® Í∏∞Î≥∏Í∞íÏùÄ 150ÏúºÎ°ú ÏÑ§Ï†ï (ÎÇòÏ§ëÏóê Í≤ÄÏ¶ù ÌõÑ Î∞îÍæ∏Î©¥ Îê®)\n",
    "SELECTED_T = 150\n",
    "\n",
    "df_step2 = df.loc[(df[\"len_overview\"] >= SELECTED_T) & (df[\"hit_score\"].notna()),\n",
    "                  [\"imdb_id\", \"overview\", \"hit_score\", \"len_overview\"]].copy()\n",
    "\n",
    "OUT_PATH = f\"02_drama_filtered_len{SELECTED_T}.parquet\"\n",
    "df_step2.to_parquet(OUT_PATH, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ saved step2 file: {OUT_PATH}\")\n",
    "print(\"step2 shape:\", df_step2.shape)\n",
    "print(df_step2.head(3))\n",
    "\n",
    "# df1 = pd.read_parquet('02_drama_filtered_len100.parquet')\n",
    "df2 = pd.read_parquet('02_drama_filtered_len150.parquet')\n",
    "# df3 = pd.read_parquet('02_drama_filtered_len200.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067f5ce",
   "metadata": {},
   "source": [
    "# 3Îã®Í≥Ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# =========================================================\n",
    "# ‚úÖ 4Îã®Í≥Ñ CONFIG (Ïó¨Í∏∞Îßå Í±¥ÎìúÎ¶¨Î©¥ Îê®)\n",
    "# =========================================================\n",
    "T = 150                 # overview ÏµúÏÜå Í∏∏Ïù¥\n",
    "P = 0.20                # ÏÉÅ/ÌïòÏúÑ Í∑πÎã® ÎπÑÏú® (hit/nonhit)\n",
    "\n",
    "TOPN = 30               # ÌÇ§ÏõåÎìú Î™á Í∞ú ÎΩëÏùÑÏßÄ\n",
    "MAX_FEATURES = 50000    # Îã®Ïñ¥ ÏµúÎåÄ Í∞úÏàò (ÎßéÏùÑÏàòÎ°ù ÎäêÎ¶º)\n",
    "NGRAM_RANGE = (1, 2)    # (1,1) or (1,2) Ï∂îÏ≤ú\n",
    "MIN_DF = 3              # ÎÑàÎ¨¥ Ìù¨Í∑ÄÌïú Îã®Ïñ¥ Ï†úÍ±∞ (2~5 Ï∂îÏ≤ú)\n",
    "MAX_DF = 0.80           # ÎÑàÎ¨¥ ÌùîÌïú Îã®Ïñ¥ Ï†úÍ±∞ (0.7~0.9 Ï∂îÏ≤ú)\n",
    "\n",
    "# ÌÜ†ÌÅ∞ Í∑úÏπô: 3Í∏ÄÏûê Ïù¥ÏÉÅ ÏòÅÏñ¥Îßå (la, tv Í∞ôÏùÄ 2Í∏ÄÏûê Ï†úÍ±∞)\n",
    "TOKEN_PATTERN = r\"(?u)\\b[a-z]{3,}\\b\"\n",
    "\n",
    "# ‚úÖ Ïª§Ïä§ÌÖÄ Stopwords (ÌïÑÏöîÌïòÎ©¥ Í≥ÑÏÜç Ï∂îÍ∞Ä)\n",
    "CUSTOM_STOPWORDS = {\n",
    "    # ÎÑàÎ¨¥ ÏùºÎ∞òÏ†Å\n",
    "    \"set\",\"new\",\"help\",\"team\",\"one\",\"two\",\"three\",\"also\",\"may\",\"like\",\"makes\",\"find\",\n",
    "    \"based\",\"story\",\"series\",\"show\",\"episode\",\"season\",\"drama\",\n",
    "\n",
    "    # ÏÑúÏÇ¨ÏóêÏÑú ÏùòÎØ∏ ÏïΩÌïú Î©îÌÉÄ Îã®Ïñ¥\n",
    "    \"life\",\"family\",\"world\",\"time\",\"day\",\"years\",\"year\",\"man\",\"woman\",\"girl\",\"boy\",\n",
    "\n",
    "    # Ï†úÏûë/ÏÑ§Î™Ö Í≥ÑÏó¥\n",
    "    \"produced\",\"production\",\"television\",\"tv\",\n",
    "\n",
    "    # ÏûêÏ£º ÌäÄÎäî ÏßßÏùÄ ÌÜ†ÌÅ∞/Î°úÎßàÏà´Ïûê\n",
    "    \"iii\",\"ii\",\"iv\",\"vi\",\"vii\",\"viii\",\"ix\",\"la\",\n",
    "}\n",
    "\n",
    "# ‚úÖ Ï∂îÍ∞ÄÎ°ú ÏßÄÎ™Ö/Ïù∏Î™ÖÏù¥ Í≥ÑÏÜç ÌäÄÎ©¥ Ïó¨Í∏∞Ïóê Í≥ÑÏÜç ÎÑ£Í∏∞\n",
    "CUSTOM_STOPWORDS_EXTRA = {\n",
    "    # ÏßÄÎ™Ö/Íµ≠Í∞Ä/ÏßÄÏó≠/Ïù∏Î™Ö(ÏßÄÍ∏à ÌäÄÎäî Í≤ÉÎì§)\n",
    "    \"york\",\"america\",\"england\",\"mexico\",\"colombia\",\"john\",\"jimmy\",\n",
    "    \"city\",  # york city Î∂ÑÌï¥ Î∞©ÏßÄÏö©(ÏõêÌïòÎ©¥ ÎπºÎèÑ Îê®)\n",
    "\n",
    "    # Í∏∞Í¥Ä/Î©îÌÉÄ\n",
    "    \"fbi\",\"televisa\",\"remake\",\n",
    "\n",
    "    # ÎÑàÎ¨¥ Ïï†Îß§Ìïú ÏùºÎ∞ò Îã®Ïñ¥(ÏÉÅÌô© Îî∞Îùº)\n",
    "    \"epic\",\"boss\",\"century\",\"force\",\"history\",\"abilities\",\"goes\",\"lost\",\"money\",\n",
    "}\n",
    "\n",
    "# ÌååÏùº Í≤ΩÎ°ú\n",
    "IN_PATH = f\"02_drama_filtered_len{T}.parquet\"\n",
    "\n",
    "OUT_LABELED = f\"04_labeled_len{T}_p{int(P*100)}.parquet\"\n",
    "OUT_HIT_TOP = f\"04_keywords_hit_len{T}_p{int(P*100)}.csv\"\n",
    "OUT_NON_TOP = f\"04_keywords_nonhit_len{T}_p{int(P*100)}.csv\"\n",
    "OUT_DELTA   = f\"04_keywords_delta_len{T}_p{int(P*100)}.csv\"\n",
    "\n",
    "# =========================================================\n",
    "# 1) ÌÖçÏä§Ìä∏ ÌÅ¥Î¶∞ (Í∞ÄÎ≥çÍ≥† Ïû¨ÌòÑÏÑ± Ï¢ãÍ≤å)\n",
    "# =========================================================\n",
    "def clean_text(s: str) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)   # ÏòÅÏñ¥/Ïà´Ïûê/Í≥µÎ∞±Îßå\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2) Îç∞Ïù¥ÌÑ∞ Î°úÎìú + ÌíàÏßà Ï≤¥ÌÅ¨\n",
    "# =========================================================\n",
    "df = pd.read_parquet(IN_PATH)\n",
    "\n",
    "need_cols = [\"imdb_id\", \"overview\", \"hit_score\"]\n",
    "missing = [c for c in need_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"ÌïÑÏàò Ïª¨Îüº ÎàÑÎùΩ: {missing} / ÌòÑÏû¨ Ïª¨Îüº: {df.columns.tolist()}\")\n",
    "\n",
    "keep_cols = need_cols + ([c for c in [\"len_overview\"] if c in df.columns])\n",
    "df = df[keep_cols].copy()\n",
    "df = df.dropna(subset=[\"imdb_id\", \"overview\", \"hit_score\"]).copy()\n",
    "\n",
    "df[\"overview_clean\"] = df[\"overview\"].map(clean_text)\n",
    "df = df[df[\"overview_clean\"].str.len() > 0].copy()\n",
    "\n",
    "print(\"‚úÖ loaded rows:\", len(df))\n",
    "print(\"‚úÖ unique imdb_id:\", df[\"imdb_id\"].nunique())\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3) ÎùºÎ≤®ÎßÅ Ï†ÅÏö© (ÌôïÏ†ï Í∏∞Ï§Ä P)\n",
    "# =========================================================\n",
    "q_low = df[\"hit_score\"].quantile(P)\n",
    "q_high = df[\"hit_score\"].quantile(1 - P)\n",
    "\n",
    "nonhit = df[df[\"hit_score\"] <= q_low].copy()\n",
    "hit = df[df[\"hit_score\"] >= q_high].copy()\n",
    "\n",
    "nonhit[\"label\"] = 0\n",
    "hit[\"label\"] = 1\n",
    "\n",
    "labeled = pd.concat([nonhit, hit], ignore_index=True)\n",
    "\n",
    "hit_n = int((labeled[\"label\"] == 1).sum())\n",
    "non_n = int((labeled[\"label\"] == 0).sum())\n",
    "\n",
    "print(\"\\nüìå labeling report\")\n",
    "print(f\"- T: {T} (min overview length)\")\n",
    "print(f\"- P: {P}\")\n",
    "print(f\"- q_low: {q_low:.6f}\")\n",
    "print(f\"- q_high: {q_high:.6f}\")\n",
    "print(f\"- hit_n: {hit_n}\")\n",
    "print(f\"- nonhit_n: {non_n}\")\n",
    "print(f\"- total_after: {len(labeled)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 4) TF-IDF ÌïôÏäµ (Ïª§Ïä§ÌÖÄ Í∞ÄÎä•Ìïú Î≤ÑÏ†Ñ)\n",
    "# =========================================================\n",
    "stop_words = set(ENGLISH_STOP_WORDS) | set(CUSTOM_STOPWORDS) | set(CUSTOM_STOPWORDS_EXTRA)\n",
    "\n",
    "vec = TfidfVectorizer(\n",
    "    stop_words=list(stop_words),\n",
    "    max_features=MAX_FEATURES,\n",
    "    ngram_range=NGRAM_RANGE,\n",
    "    min_df=MIN_DF,\n",
    "    max_df=MAX_DF,\n",
    "    token_pattern=TOKEN_PATTERN,\n",
    ")\n",
    "\n",
    "X = vec.fit_transform(labeled[\"overview_clean\"].tolist())\n",
    "terms = np.array(vec.get_feature_names_out())\n",
    "\n",
    "labels = labeled[\"label\"].to_numpy()\n",
    "hit_mask = labels == 1\n",
    "non_mask = labels == 0\n",
    "\n",
    "hit_mean = X[hit_mask].mean(axis=0).A1\n",
    "non_mean = X[non_mask].mean(axis=0).A1\n",
    "delta = hit_mean - non_mean\n",
    "\n",
    "# =========================================================\n",
    "# 5) Top ÌÇ§ÏõåÎìú Ï∂îÏ∂ú + Ï†ÄÏû•\n",
    "# =========================================================\n",
    "def top_df(scores: np.ndarray, n: int = TOPN) -> pd.DataFrame:\n",
    "    idx = np.argsort(scores)[::-1][:n]\n",
    "    return pd.DataFrame({\"keyword\": terms[idx], \"score\": scores[idx]})\n",
    "\n",
    "hit_top_df = top_df(hit_mean, TOPN).rename(columns={\"score\": \"hit_mean_tfidf\"})\n",
    "non_top_df = top_df(non_mean, TOPN).rename(columns={\"score\": \"nonhit_mean_tfidf\"})\n",
    "\n",
    "delta_pos_df = top_df(delta, TOPN).rename(columns={\"score\": \"delta(hit-nonhit)\"})\n",
    "delta_neg_df = top_df(-delta, TOPN).rename(columns={\"score\": \"delta(nonhit-hit)\"})\n",
    "\n",
    "delta_out = pd.DataFrame({\n",
    "    \"keyword\": pd.concat([delta_pos_df[\"keyword\"], delta_neg_df[\"keyword\"]], ignore_index=True),\n",
    "    \"direction\": ([\"hit+\"] * TOPN) + ([\"nonhit+\"] * TOPN),\n",
    "    \"delta_value\": pd.concat([delta_pos_df[\"delta(hit-nonhit)\"], delta_neg_df[\"delta(nonhit-hit)\"]], ignore_index=True),\n",
    "})\n",
    "\n",
    "# =========================================================\n",
    "# 6) ÌîÑÎ¶¨Î∑∞ Ï∂úÎ†•\n",
    "# =========================================================\n",
    "print(\"\\nüèÅ Preview (delta hit+ top 15)\")\n",
    "print(delta_pos_df.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\nüèÅ Preview (delta nonhit+ top 15)\")\n",
    "print(delta_neg_df.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\nüß© Debug tips\")\n",
    "print(f\"- vocab size: {len(terms)}\")\n",
    "print(f\"- NGRAM_RANGE: {NGRAM_RANGE}, MIN_DF: {MIN_DF}, MAX_DF: {MAX_DF}\")\n",
    "print(f\"- stopwords size: {len(stop_words)}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7) ÌååÏùºÏ†ÄÏû•\n",
    "# =========================================================\n",
    "\n",
    "labeled.to_parquet(OUT_LABELED, index=False)\n",
    "hit_top_df.to_csv(OUT_HIT_TOP, index=False)\n",
    "non_top_df.to_csv(OUT_NON_TOP, index=False)\n",
    "delta_out.to_csv(OUT_DELTA, index=False)\n",
    "\n",
    "print(\"‚úÖ saved labeled:\", OUT_LABELED)\n",
    "print(\"\\n‚úÖ saved outputs\")\n",
    "print(\"-\", OUT_HIT_TOP)\n",
    "print(\"-\", OUT_NON_TOP)\n",
    "print(\"-\", OUT_DELTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5bc62b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd150aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta = pd.read_csv('04_keywords_delta_len150_p20.csv')\n",
    "df_hit = pd.read_csv('04_keywords_hit_len150_p20.csv')\n",
    "df_non = pd.read_csv('04_keywords_nonhit_len150_p20.csv')\n",
    "df = pd.read_parquet('04_labeled_len150_p20.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "456e6b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(934, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bffdc6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_delta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efc7bca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>hit_mean_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lives</td>\n",
       "      <td>0.017419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>young</td>\n",
       "      <td>0.013895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people</td>\n",
       "      <td>0.012907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>police</td>\n",
       "      <td>0.012423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>town</td>\n",
       "      <td>0.012106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>murder</td>\n",
       "      <td>0.011275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>high</td>\n",
       "      <td>0.011081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>way</td>\n",
       "      <td>0.010748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>crime</td>\n",
       "      <td>0.010346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>detective</td>\n",
       "      <td>0.010235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>school</td>\n",
       "      <td>0.009895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mysterious</td>\n",
       "      <td>0.009658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>old</td>\n",
       "      <td>0.009655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>love</td>\n",
       "      <td>0.009329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>group</td>\n",
       "      <td>0.009308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>make</td>\n",
       "      <td>0.009102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>past</td>\n",
       "      <td>0.008871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>american</td>\n",
       "      <td>0.008850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>small</td>\n",
       "      <td>0.008782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>father</td>\n",
       "      <td>0.008724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>relationship</td>\n",
       "      <td>0.008709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>follows</td>\n",
       "      <td>0.008576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>war</td>\n",
       "      <td>0.008565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>secrets</td>\n",
       "      <td>0.008288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>journey</td>\n",
       "      <td>0.008230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>following</td>\n",
       "      <td>0.008229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>death</td>\n",
       "      <td>0.008221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>events</td>\n",
       "      <td>0.008117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>women</td>\n",
       "      <td>0.008078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>power</td>\n",
       "      <td>0.008031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         keyword  hit_mean_tfidf\n",
       "0          lives        0.017419\n",
       "1          young        0.013895\n",
       "2         people        0.012907\n",
       "3         police        0.012423\n",
       "4           town        0.012106\n",
       "5         murder        0.011275\n",
       "6           high        0.011081\n",
       "7            way        0.010748\n",
       "8          crime        0.010346\n",
       "9      detective        0.010235\n",
       "10        school        0.009895\n",
       "11    mysterious        0.009658\n",
       "12           old        0.009655\n",
       "13          love        0.009329\n",
       "14         group        0.009308\n",
       "15          make        0.009102\n",
       "16          past        0.008871\n",
       "17      american        0.008850\n",
       "18         small        0.008782\n",
       "19        father        0.008724\n",
       "20  relationship        0.008709\n",
       "21       follows        0.008576\n",
       "22           war        0.008565\n",
       "23       secrets        0.008288\n",
       "24       journey        0.008230\n",
       "25     following        0.008229\n",
       "26         death        0.008221\n",
       "27        events        0.008117\n",
       "28         women        0.008078\n",
       "29         power        0.008031"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hit.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d7d7e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>nonhit_mean_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>0.023442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>young</td>\n",
       "      <td>0.017597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lives</td>\n",
       "      <td>0.014105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>telenovela</td>\n",
       "      <td>0.013258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>group</td>\n",
       "      <td>0.012688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>school</td>\n",
       "      <td>0.012409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>death</td>\n",
       "      <td>0.011935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>daughter</td>\n",
       "      <td>0.011872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>old</td>\n",
       "      <td>0.011614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>murder</td>\n",
       "      <td>0.011508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>town</td>\n",
       "      <td>0.011320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>people</td>\n",
       "      <td>0.010701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>friends</td>\n",
       "      <td>0.010524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mother</td>\n",
       "      <td>0.010488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>way</td>\n",
       "      <td>0.010213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>secret</td>\n",
       "      <td>0.010172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mysterious</td>\n",
       "      <td>0.009925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>past</td>\n",
       "      <td>0.009606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>father</td>\n",
       "      <td>0.009531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>star</td>\n",
       "      <td>0.009468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>finds</td>\n",
       "      <td>0.009193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mexican</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>high</td>\n",
       "      <td>0.009094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>women</td>\n",
       "      <td>0.008488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>police</td>\n",
       "      <td>0.008476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>accident</td>\n",
       "      <td>0.008210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>turn</td>\n",
       "      <td>0.008099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>husband</td>\n",
       "      <td>0.007907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>secrets</td>\n",
       "      <td>0.007862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>best</td>\n",
       "      <td>0.007794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       keyword  nonhit_mean_tfidf\n",
       "0         love           0.023442\n",
       "1        young           0.017597\n",
       "2        lives           0.014105\n",
       "3   telenovela           0.013258\n",
       "4        group           0.012688\n",
       "5       school           0.012409\n",
       "6        death           0.011935\n",
       "7     daughter           0.011872\n",
       "8          old           0.011614\n",
       "9       murder           0.011508\n",
       "10        town           0.011320\n",
       "11      people           0.010701\n",
       "12     friends           0.010524\n",
       "13      mother           0.010488\n",
       "14         way           0.010213\n",
       "15      secret           0.010172\n",
       "16  mysterious           0.009925\n",
       "17        past           0.009606\n",
       "18      father           0.009531\n",
       "19        star           0.009468\n",
       "20       finds           0.009193\n",
       "21     mexican           0.009100\n",
       "22        high           0.009094\n",
       "23       women           0.008488\n",
       "24      police           0.008476\n",
       "25    accident           0.008210\n",
       "26        turn           0.008099\n",
       "27     husband           0.007907\n",
       "28     secrets           0.007862\n",
       "29        best           0.007794"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6c2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a50ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
