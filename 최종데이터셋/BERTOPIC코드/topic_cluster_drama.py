"""
ë“œë¼ë§ˆ í¥í–‰/ë¹„í¥í–‰ BERTopic ìœ ì‚¬ í† í”½ í†µí•© ë¶„ì„
- ê¸°ì¡´ BERTopic ë¶„ì„ ê²°ê³¼ë¥¼ ë¡œë“œí•˜ì—¬ ìœ ì‚¬ í† í”½ì„ í´ëŸ¬ìŠ¤í„°ë§
- í¥í–‰ì‘: n_groups=4, ë¹„í¥í–‰ì‘: n_groups=8
- ê²°ê³¼ë¥¼ 'BERTOPIC_SIMP' í´ë”ì— ì €ì¥
"""

import pandas as pd
import numpy as np
import os
from bertopic import BERTopic
from sklearn.cluster import AgglomerativeClustering
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings('ignore')

# ==========================================================
# 0. ì¶œë ¥ í´ë” ìƒì„±
# ==========================================================
INPUT_DIR = "ë“œë¼ë§ˆë°ì´í„°BERTOPIC"
OUTPUT_DIR = "BERTOPIC_SIMP"
os.makedirs(OUTPUT_DIR, exist_ok=True)
print(f"ì…ë ¥ í´ë”: {INPUT_DIR}/")
print(f"ì¶œë ¥ í´ë” ìƒì„±: {OUTPUT_DIR}/")

# ==========================================================
# 1. BERTopic ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
# ==========================================================
print("\n" + "="*60)
print("BERTopic ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ ì¤‘...")
print("="*60)

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ëª¨ë¸ ë¡œë“œ ì‹œ í•„ìš”)
embedding_model = SentenceTransformer('Qwen/Qwen3-Embedding-0.6B')

# BERTopic ëª¨ë¸ ë¡œë“œ
hit_topic_model = BERTopic.load(
    f"{INPUT_DIR}/hit_bertopic_model",
    embedding_model=embedding_model
)
print("  âœ“ í¥í–‰ì‘ BERTopic ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

flop_topic_model = BERTopic.load(
    f"{INPUT_DIR}/flop_bertopic_model",
    embedding_model=embedding_model
)
print("  âœ“ ë¹„í¥í–‰ì‘ BERTopic ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# ë“œë¼ë§ˆ ë°ì´í„° ë¡œë“œ
df_hit = pd.read_csv(f"{INPUT_DIR}/hit_drama_topics.csv")
df_flop = pd.read_csv(f"{INPUT_DIR}/flop_drama_topics.csv")
print(f"  âœ“ í¥í–‰ì‘ ë°ì´í„°: {len(df_hit)}ê°œ")
print(f"  âœ“ ë¹„í¥í–‰ì‘ ë°ì´í„°: {len(df_flop)}ê°œ")

# ==========================================================
# 2. í† í”½ í´ëŸ¬ìŠ¤í„° ë¶„ì„ í•¨ìˆ˜
# ==========================================================

def analyze_topic_clusters(topic_model, n_groups, label=""):
    """
    í† í”½ ê°„ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ê³  ìœ ì‚¬í•œ í† í”½ë¼ë¦¬ ê·¸ë£¹í™”
    
    Args:
        topic_model: í•™ìŠµëœ BERTopic ëª¨ë¸
        n_groups: ì›í•˜ëŠ” ê·¸ë£¹ ìˆ˜
        label: ì¶œë ¥ ì‹œ í‘œì‹œí•  ë¼ë²¨ (í¥í–‰ì‘/ë¹„í¥í–‰ì‘)
    
    Returns:
        topic_clusters: í† í”½ë³„ í´ëŸ¬ìŠ¤í„° ì •ë³´ DataFrame
    """
    # í† í”½ ì„ë² ë”©(ì¢Œí‘œ) ì¶”ì¶œ
    topic_embeddings = topic_model.topic_embeddings_
    
    # í† í”½ ì •ë³´ (outlier -1 ì œì™¸)
    topic_info = topic_model.get_topic_info()
    valid_topics = topic_info[topic_info['Topic'] != -1]['Topic'].tolist()
    
    # outlier(-1)ëŠ” ì¸ë±ìŠ¤ 0ì— ìˆìœ¼ë¯€ë¡œ, ì‹¤ì œ í† í”½ì€ ì¸ë±ìŠ¤ 1ë¶€í„°
    valid_embeddings = topic_embeddings[1:len(valid_topics)+1]
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š {label} í† í”½ í´ëŸ¬ìŠ¤í„° ë¶„ì„")
    print(f"{'='*70}")
    print(f"í† í”½ ìˆ˜: {len(valid_topics)}ê°œ, ê·¸ë£¹ ìˆ˜: {n_groups}ê°œ")
    
    # ê·¸ë£¹ ìˆ˜ê°€ í† í”½ ìˆ˜ë³´ë‹¤ ë§ìœ¼ë©´ ì¡°ì •
    actual_n_groups = min(n_groups, len(valid_topics))
    if actual_n_groups != n_groups:
        print(f"âš ï¸ í† í”½ ìˆ˜({len(valid_topics)})ê°€ ê·¸ë£¹ ìˆ˜({n_groups})ë³´ë‹¤ ì ì–´ {actual_n_groups}ê°œë¡œ ì¡°ì •")
    
    # ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§
    clustering = AgglomerativeClustering(
        n_clusters=actual_n_groups,
        metric='cosine',
        linkage='average'
    )
    cluster_labels = clustering.fit_predict(valid_embeddings)
    
    # ê²°ê³¼ ì •ë¦¬
    topic_clusters = pd.DataFrame({
        'í† í”½ë²ˆí˜¸': valid_topics,
        'í´ëŸ¬ìŠ¤í„°': cluster_labels,
        'ë“œë¼ë§ˆìˆ˜': [topic_info[topic_info['Topic'] == t]['Count'].values[0] for t in valid_topics],
        'í‚¤ì›Œë“œ': [', '.join([w for w, s in topic_model.get_topic(t)[:5]]) for t in valid_topics]
    })
    
    # í´ëŸ¬ìŠ¤í„°ë³„ ì¶œë ¥
    print(f"\n[ìœ ì‚¬ í† í”½ ê·¸ë£¹]")
    
    for cluster_id in sorted(topic_clusters['í´ëŸ¬ìŠ¤í„°'].unique()):
        cluster_topics = topic_clusters[topic_clusters['í´ëŸ¬ìŠ¤í„°'] == cluster_id]
        topic_nums = cluster_topics['í† í”½ë²ˆí˜¸'].tolist()
        total_dramas = cluster_topics['ë“œë¼ë§ˆìˆ˜'].sum()
        
        print(f"\nğŸ“Œ ê·¸ë£¹ {cluster_id}: í† í”½ {topic_nums} (ì´ {total_dramas}í¸)")
        print("-" * 60)
        
        for _, row in cluster_topics.iterrows():
            print(f"   í† í”½ {row['í† í”½ë²ˆí˜¸']:2d} ({row['ë“œë¼ë§ˆìˆ˜']:3d}í¸): {row['í‚¤ì›Œë“œ']}")
    
    # í´ëŸ¬ìŠ¤í„°ë³„ ìš”ì•½
    cluster_summary = topic_clusters.groupby('í´ëŸ¬ìŠ¤í„°').agg({
        'í† í”½ë²ˆí˜¸': lambda x: list(x),
        'ë“œë¼ë§ˆìˆ˜': 'sum'
    }).reset_index()
    cluster_summary.columns = ['í´ëŸ¬ìŠ¤í„°', 'í¬í•¨_í† í”½', 'ì´_ë“œë¼ë§ˆìˆ˜']
    
    print(f"\n[í´ëŸ¬ìŠ¤í„° ìš”ì•½]")
    print(cluster_summary.to_string(index=False))
    
    return topic_clusters, cluster_summary

# ==========================================================
# 3. í¥í–‰ì‘ í† í”½ í´ëŸ¬ìŠ¤í„° ë¶„ì„
# ==========================================================
print("\n" + "="*60)
print("í¥í–‰ì‘ í† í”½ í´ëŸ¬ìŠ¤í„° ë¶„ì„")
print("="*60)

hit_topic_clusters, hit_cluster_summary = analyze_topic_clusters(
    topic_model=hit_topic_model,
    n_groups=4,  # í¥í–‰ì‘ 4ê°œ ê·¸ë£¹
    label="í¥í–‰ì‘"
)

# ==========================================================
# 4. ë¹„í¥í–‰ì‘ í† í”½ í´ëŸ¬ìŠ¤í„° ë¶„ì„
# ==========================================================
print("\n" + "="*60)
print("ë¹„í¥í–‰ì‘ í† í”½ í´ëŸ¬ìŠ¤í„° ë¶„ì„")
print("="*60)

flop_topic_clusters, flop_cluster_summary = analyze_topic_clusters(
    topic_model=flop_topic_model,
    n_groups=8,  # ë¹„í¥í–‰ì‘ 8ê°œ ê·¸ë£¹
    label="ë¹„í¥í–‰ì‘"
)

# ==========================================================
# 5. í† í”½ ë¶„ì„ ìš”ì•½ í•¨ìˆ˜
# ==========================================================

def create_topic_summary(topic_model, df_subset, label):
    """í† í”½ ë¶„ì„ ê²°ê³¼ ìš”ì•½"""
    topic_info = topic_model.get_topic_info()
    results = []
    
    for topic_id in sorted(topic_info['Topic'].unique()):
        if topic_id != -1:
            keywords = topic_model.get_topic(topic_id)
            top_keywords = [word for word, score in keywords[:5]]
            topic_dramas = df_subset[df_subset['topic'] == topic_id]
            
            # hit_score í‰ê·  ê³„ì‚° (vote_average ëŒ€ì‹ )
            avg_hit_score = topic_dramas['hit_score'].mean() if len(topic_dramas) > 0 else 0
            
            results.append({
                'label': label,
                'topic_id': topic_id,
                'drama_count': len(topic_dramas),
                'avg_hit_score': round(avg_hit_score, 4) if not pd.isna(avg_hit_score) else 0,
                'keywords': ', '.join(top_keywords),
                'sample_dramas': ', '.join(topic_dramas['title'].head(3).tolist()) if 'title' in topic_dramas.columns else ''
            })
    
    return results

# ==========================================================
# 6. í‚¤ì›Œë“œ ì°¨ì§‘í•© ë¶„ì„
# ==========================================================
print("\n" + "="*60)
print("í‚¤ì›Œë“œ ì°¨ì§‘í•© ë¶„ì„ ì¤‘...")
print("="*60)

def get_all_keywords(topic_model):
    """ëª¨ë¸ì˜ ëª¨ë“  í† í”½ì—ì„œ í‚¤ì›Œë“œì™€ ì ìˆ˜ ì¶”ì¶œ"""
    all_keywords = {}
    topic_info = topic_model.get_topic_info()
    
    for topic_id in topic_info['Topic'].values:
        if topic_id != -1:
            keywords = topic_model.get_topic(topic_id)
            for word, score in keywords:
                if word in all_keywords:
                    all_keywords[word] = max(all_keywords[word], score)
                else:
                    all_keywords[word] = score
    
    return all_keywords

# í¥í–‰ì‘/ë¹„í¥í–‰ì‘ í‚¤ì›Œë“œ ì¶”ì¶œ
hit_keywords = get_all_keywords(hit_topic_model)
flop_keywords = get_all_keywords(flop_topic_model)

# ì°¨ì§‘í•© ê³„ì‚°
hit_unique_words = set(hit_keywords.keys()) - set(flop_keywords.keys())
flop_unique_words = set(flop_keywords.keys()) - set(hit_keywords.keys())

# ì ìˆ˜ìˆœ ì •ë ¬
hit_unique_keywords = sorted([(w, hit_keywords[w]) for w in hit_unique_words], key=lambda x: -x[1])
flop_unique_keywords = sorted([(w, flop_keywords[w]) for w in flop_unique_words], key=lambda x: -x[1])

print(f"\ní¥í–‰ì‘ì—ë§Œ ìˆëŠ” í‚¤ì›Œë“œ (ìƒìœ„ 20ê°œ):")
for word, score in hit_unique_keywords[:20]:
    print(f"  {word}: {score:.4f}")

print(f"\në¹„í¥í–‰ì‘ì—ë§Œ ìˆëŠ” í‚¤ì›Œë“œ (ìƒìœ„ 20ê°œ):")
for word, score in flop_unique_keywords[:20]:
    print(f"  {word}: {score:.4f}")

# ==========================================================
# 7. ê²°ê³¼ ì €ì¥
# ==========================================================
print("\n" + "="*60)
print("ê²°ê³¼ ì €ì¥ ì¤‘...")
print("="*60)

# 1) í† í”½ ë¶„ì„ ìš”ì•½
hit_summary = create_topic_summary(hit_topic_model, df_hit, 'hit')
flop_summary = create_topic_summary(flop_topic_model, df_flop, 'flop')
summary_df = pd.DataFrame(hit_summary + flop_summary)
summary_df.to_csv(f'{OUTPUT_DIR}/topic_analysis_summary.csv', index=False, encoding='utf-8-sig')
print(f"  âœ“ {OUTPUT_DIR}/topic_analysis_summary.csv")

# 2) ë“œë¼ë§ˆë³„ í† í”½ ë°ì´í„°
df_hit.to_csv(f'{OUTPUT_DIR}/hit_dramas_with_topics.csv', index=False, encoding='utf-8-sig')
df_flop.to_csv(f'{OUTPUT_DIR}/flop_dramas_with_topics.csv', index=False, encoding='utf-8-sig')
print(f"  âœ“ {OUTPUT_DIR}/hit_dramas_with_topics.csv")
print(f"  âœ“ {OUTPUT_DIR}/flop_dramas_with_topics.csv")

# 3) í† í”½ í´ëŸ¬ìŠ¤í„° ê²°ê³¼
hit_topic_clusters.to_csv(f'{OUTPUT_DIR}/hit_topic_clusters.csv', index=False, encoding='utf-8-sig')
flop_topic_clusters.to_csv(f'{OUTPUT_DIR}/flop_topic_clusters.csv', index=False, encoding='utf-8-sig')
print(f"  âœ“ {OUTPUT_DIR}/hit_topic_clusters.csv")
print(f"  âœ“ {OUTPUT_DIR}/flop_topic_clusters.csv")

# 4) í´ëŸ¬ìŠ¤í„° ìš”ì•½
hit_cluster_summary.to_csv(f'{OUTPUT_DIR}/hit_cluster_summary.csv', index=False, encoding='utf-8-sig')
flop_cluster_summary.to_csv(f'{OUTPUT_DIR}/flop_cluster_summary.csv', index=False, encoding='utf-8-sig')
print(f"  âœ“ {OUTPUT_DIR}/hit_cluster_summary.csv")
print(f"  âœ“ {OUTPUT_DIR}/flop_cluster_summary.csv")

# 5) í‚¤ì›Œë“œ ë¹„êµ ì €ì¥ (ì°¨ì§‘í•© í¬í•¨)
max_len = max(20, len(hit_unique_keywords), len(flop_unique_keywords))

# ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ ë§ì¶”ê¸°
hit_words = [w for w, s in hit_unique_keywords[:max_len]] + [''] * (max_len - min(max_len, len(hit_unique_keywords)))
hit_scores = [round(s, 4) for w, s in hit_unique_keywords[:max_len]] + [None] * (max_len - min(max_len, len(hit_unique_keywords)))
flop_words = [w for w, s in flop_unique_keywords[:max_len]] + [''] * (max_len - min(max_len, len(flop_unique_keywords)))
flop_scores = [round(s, 4) for w, s in flop_unique_keywords[:max_len]] + [None] * (max_len - min(max_len, len(flop_unique_keywords)))

keyword_comparison = pd.DataFrame({
    'hit_unique_keyword': hit_words[:max_len],
    'hit_unique_score': hit_scores[:max_len],
    'flop_unique_keyword': flop_words[:max_len],
    'flop_unique_score': flop_scores[:max_len],
})
keyword_comparison.to_csv(f'{OUTPUT_DIR}/keyword_comparison.csv', index=False, encoding='utf-8-sig')
print(f"  âœ“ {OUTPUT_DIR}/keyword_comparison.csv")

# ==========================================================
# 8. ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
# ==========================================================
print("\n" + "="*60)
print("ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
print("="*60)

report = f"""
================================================================================
                ë“œë¼ë§ˆ í¥í–‰/ë¹„í¥í–‰ í† í”½ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ë¦¬í¬íŠ¸
================================================================================

â–  ë¶„ì„ ê°œìš”
  - í¥í–‰ì‘: {len(df_hit)}ê°œ ë“œë¼ë§ˆ
  - ë¹„í¥í–‰ì‘: {len(df_flop)}ê°œ ë“œë¼ë§ˆ
  - í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•: Agglomerative Clustering (cosine, average linkage)

================================================================================
â–  í¥í–‰ì‘ í† í”½ í´ëŸ¬ìŠ¤í„° ë¶„ì„ (n_groups=4)
================================================================================
"""

for cluster_id in sorted(hit_topic_clusters['í´ëŸ¬ìŠ¤í„°'].unique()):
    cluster_topics = hit_topic_clusters[hit_topic_clusters['í´ëŸ¬ìŠ¤í„°'] == cluster_id]
    topic_nums = cluster_topics['í† í”½ë²ˆí˜¸'].tolist()
    total_dramas = cluster_topics['ë“œë¼ë§ˆìˆ˜'].sum()
    
    report += f"\nğŸ“Œ ê·¸ë£¹ {cluster_id}: í† í”½ {topic_nums} (ì´ {total_dramas}í¸)\n"
    report += "-" * 60 + "\n"
    
    for _, row in cluster_topics.iterrows():
        report += f"   í† í”½ {row['í† í”½ë²ˆí˜¸']:2d} ({row['ë“œë¼ë§ˆìˆ˜']:3d}í¸): {row['í‚¤ì›Œë“œ']}\n"

report += f"""
================================================================================
â–  ë¹„í¥í–‰ì‘ í† í”½ í´ëŸ¬ìŠ¤í„° ë¶„ì„ (n_groups=8)
================================================================================
"""

for cluster_id in sorted(flop_topic_clusters['í´ëŸ¬ìŠ¤í„°'].unique()):
    cluster_topics = flop_topic_clusters[flop_topic_clusters['í´ëŸ¬ìŠ¤í„°'] == cluster_id]
    topic_nums = cluster_topics['í† í”½ë²ˆí˜¸'].tolist()
    total_dramas = cluster_topics['ë“œë¼ë§ˆìˆ˜'].sum()
    
    report += f"\nğŸ“Œ ê·¸ë£¹ {cluster_id}: í† í”½ {topic_nums} (ì´ {total_dramas}í¸)\n"
    report += "-" * 60 + "\n"
    
    for _, row in cluster_topics.iterrows():
        report += f"   í† í”½ {row['í† í”½ë²ˆí˜¸']:2d} ({row['ë“œë¼ë§ˆìˆ˜']:3d}í¸): {row['í‚¤ì›Œë“œ']}\n"

report += f"""
================================================================================
â–  í‚¤ì›Œë“œ ì°¨ì§‘í•© ë¶„ì„
================================================================================

[í¥í–‰ì‘ì—ë§Œ ìˆëŠ” í‚¤ì›Œë“œ (ìƒìœ„ 20ê°œ)]
"""

for i, (word, score) in enumerate(hit_unique_keywords[:20], 1):
    report += f"  {i:2d}. {word}: {score:.4f}\n"

report += f"""
[ë¹„í¥í–‰ì‘ì—ë§Œ ìˆëŠ” í‚¤ì›Œë“œ (ìƒìœ„ 20ê°œ)]
"""

for i, (word, score) in enumerate(flop_unique_keywords[:20], 1):
    report += f"  {i:2d}. {word}: {score:.4f}\n"

report += f"""
================================================================================
â–  ì¶œë ¥ íŒŒì¼ ëª©ë¡
================================================================================

[ë¶„ì„ ê²°ê³¼ CSV]
  - topic_analysis_summary.csv      : í† í”½ë³„ ë¶„ì„ ìš”ì•½
  - hit_dramas_with_topics.csv      : í¥í–‰ì‘ ë“œë¼ë§ˆë³„ í† í”½ í• ë‹¹
  - flop_dramas_with_topics.csv     : ë¹„í¥í–‰ì‘ ë“œë¼ë§ˆë³„ í† í”½ í• ë‹¹
  - hit_topic_clusters.csv          : í¥í–‰ì‘ í† í”½ í´ëŸ¬ìŠ¤í„° ê²°ê³¼
  - flop_topic_clusters.csv         : ë¹„í¥í–‰ì‘ í† í”½ í´ëŸ¬ìŠ¤í„° ê²°ê³¼
  - hit_cluster_summary.csv         : í¥í–‰ì‘ í´ëŸ¬ìŠ¤í„° ìš”ì•½
  - flop_cluster_summary.csv        : ë¹„í¥í–‰ì‘ í´ëŸ¬ìŠ¤í„° ìš”ì•½
  - keyword_comparison.csv          : í¥í–‰/ë¹„í¥í–‰ í‚¤ì›Œë“œ ì°¨ì§‘í•©

================================================================================
"""

with open(f"{OUTPUT_DIR}/cluster_analysis_report.txt", 'w', encoding='utf-8') as f:
    f.write(report)
print(f"  âœ“ {OUTPUT_DIR}/cluster_analysis_report.txt")

# ==========================================================
# ì™„ë£Œ
# ==========================================================
print("\n" + "="*60)
print(f"ë¶„ì„ ì™„ë£Œ! ëª¨ë“  ê²°ê³¼ê°€ '{OUTPUT_DIR}/' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("="*60)

# ì €ì¥ëœ íŒŒì¼ ëª©ë¡ ì¶œë ¥
print(f"\nì €ì¥ëœ íŒŒì¼ ëª©ë¡:")
for item in sorted(os.listdir(OUTPUT_DIR)):
    item_path = os.path.join(OUTPUT_DIR, item)
    size = os.path.getsize(item_path)
    if size > 1024*1024:
        print(f"  ğŸ“„ {item} ({size/1024/1024:.1f} MB)")
    elif size > 1024:
        print(f"  ğŸ“„ {item} ({size/1024:.1f} KB)")
    else:
        print(f"  ğŸ“„ {item} ({size} bytes)")
