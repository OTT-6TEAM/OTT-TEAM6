from .set_up import *

stats = {
    "series_total": 0,
    "series_success": 0,
    "series_failed": 0,
    "requests": 0,
    "start_time": None
}


# ==========================================================
# HTTP í˜¸ì¶œ í•¨ìˆ˜
# ==========================================================
async def get_html(session, url, retry=0):
    """IMDB title HTMLìš©"""
    if retry >= MAX_RETRIES:
        return None

    await rate_limiter.acquire()
    stats["requests"] += 1

    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
    }

    try:
        async with session.get(url, headers=headers, timeout=TIMEOUT) as resp:
            if resp.status == 429 and retry < MAX_RETRIES - 1:
                wait_time = 5 * (retry + 1)
                print(f"âš ï¸  Rate limited, waiting {wait_time}s...")
                await asyncio.sleep(wait_time)
                return await get_html(session, url, retry + 1)

            if resp.status != 200:
                if retry < MAX_RETRIES - 1:
                    await asyncio.sleep(2 ** retry)
                    return await get_html(session, url, retry + 1)
                return None

            return await resp.text()
    except asyncio.TimeoutError:
        if retry < MAX_RETRIES - 1:
            await asyncio.sleep(2 ** retry)
            return await get_html(session, url, retry + 1)
        return None
    except Exception:
        if retry < MAX_RETRIES - 1:
            await asyncio.sleep(2 ** retry)
            return await get_html(session, url, retry + 1)
        return None


# ==========================================================
# IMDB Rating ì¶”ì¶œ
# ==========================================================
def parse_rating_from_html(imdb_id, html_text):
    """
    IMDB title HTMLì—ì„œ ratingValue, ratingCount ì¶”ì¶œ (JSON-LD)
    """
    imdb_rating = None
    imdb_rating_count = None

    # JSON-LD ë¸”ë¡ ì¶”ì¶œ
    ld_match = re.search(
        r'<script type="application/ld\+json">(.*?)</script>',
        html_text,
        re.S
    )
    if ld_match:
        try:
            data = json.loads(ld_match.group(1))
            agg = data.get("aggregateRating", {})
            imdb_rating = agg.get("ratingValue")
            imdb_rating_count = agg.get("ratingCount")
        except Exception as e:
            print(f"âš ï¸  JSON-LD parse error ({imdb_id}): {e}")

    return {
        "imdb_id": imdb_id,
        "imdb_rating": imdb_rating,
        "imdb_rating_count": imdb_rating_count,
    }


async def fetch_imdb_rating(session, imdb_id):
    url = f"https://www.imdb.com/title/{imdb_id}/"
    html_text = await get_html(session, url)
    if not html_text:
        print(f"âš ï¸  {imdb_id}: HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")
        return {
            "imdb_id": imdb_id,
            "imdb_rating": None,
            "imdb_rating_count": None,
        }
    return parse_rating_from_html(imdb_id, html_text)


# ==========================================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
# ==========================================================
def save_checkpoint(processed_ids):
    checkpoint = {
        'processed_ids': list(processed_ids),
        'timestamp': datetime.now().isoformat()
    }
    with open(CHECKPOINT_FILE, 'w') as f:
        json.dump(checkpoint, f)


def load_checkpoint(output_csv_path):
    processed_ids = set()

    # 1) ì²´í¬í¬ì¸íŠ¸ íŒŒì¼
    if Path(CHECKPOINT_FILE).exists():
        try:
            with open(CHECKPOINT_FILE, 'r') as f:
                checkpoint = json.load(f)
                processed_ids.update(checkpoint.get('processed_ids', []))
                print(f"ğŸ“Œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ {len(checkpoint.get('processed_ids', [])):,}ê°œ ID ë¡œë“œ")
        except (json.JSONDecodeError, Exception) as e:
            print(f"âš ï¸  ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì†ìƒë¨, ì‚­ì œí•˜ê³  ì§„í–‰: {e}")
            try:
                Path(CHECKPOINT_FILE).unlink()
            except:
                pass

    # 2) ê¸°ì¡´ CSV ê¸°ì¤€
    if Path(output_csv_path).exists():
        try:
            df_existing = pd.read_csv(output_csv_path)
            if 'imdb_id' in df_existing.columns:
                existing_ids = df_existing['imdb_id'].unique()
                processed_ids.update(existing_ids)
                print(f"ğŸ“Œ ê¸°ì¡´ CSVì—ì„œ {len(existing_ids):,}ê°œ ì‹œë¦¬ì¦ˆ ë°œê²¬")
        except Exception as e:
            print(f"âš ï¸  ê¸°ì¡´ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")

    return processed_ids


# ==========================================================
# ë‚ ì§œ í•„í„°ë§
# ==========================================================
def filter_by_date_range(df, start_date='2005-01-01', end_date='2015-12-31'):
    """
    first_air_date ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ í•„í„°ë§
    """
    if 'first_air_date' not in df.columns:
        print("âš ï¸  'first_air_date' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë‚ ì§œ í•„í„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df

    df['first_air_date'] = pd.to_datetime(df['first_air_date'], errors='coerce')
    start = pd.to_datetime(start_date)
    end = pd.to_datetime(end_date)

    df_filtered = df[(df['first_air_date'] >= start) & (df['first_air_date'] <= end)]

    print(f"ğŸ“… ë‚ ì§œ í•„í„°ë§: {start_date} ~ {end_date}")
    print(f"   ì›ë³¸: {len(df):,}ê°œ â†’ í•„í„°ë§ í›„: {len(df_filtered):,}ê°œ")

    return df_filtered


# ==========================================================
# ë©”ì¸ ì‹¤í–‰
# ==========================================================
async def collect_imdb_ratings(input_csv_path, output_csv_path, vote_threshold=30):
    print("=" * 90)
    print("ğŸš€ IMDB RATING COLLECTOR (2005-2015)")
    print("=" * 90)

    stats["start_time"] = datetime.now()
    t0 = datetime.now()

    # 1. ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df = pd.read_csv(input_csv_path)

    # ë‚ ì§œ í•„í„°ë§ (2005-2015)
    df = filter_by_date_range(df, '2005-01-01', '2015-12-31')

    # vote_count í•„í„°ë§
    df_filtered = df[(df['vote_count'] >= vote_threshold) & (df['imdb_id'].notna())]
    df_filtered = df_filtered.drop_duplicates(subset=['imdb_id'])

    print(f"âœ… ìµœì¢… í•„í„°ë§ (vote_count>={vote_threshold} & imdb_id ì¡´ì¬): {len(df_filtered):,}ê°œ")

    if len(df_filtered) == 0:
        print("âš ï¸  ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    processed_ids = load_checkpoint(output_csv_path)
    series_list = df_filtered[['imdb_id']].to_dict('records')

    if processed_ids:
        print(f"ğŸ“Œ ì´ë¯¸ ì²˜ë¦¬ëœ ì‹œë¦¬ì¦ˆ: {len(processed_ids):,}ê°œ")
        series_list = [s for s in series_list if s['imdb_id'] not in processed_ids]
        print(f"ğŸ“Œ ë‚¨ì€ ì‘ì—…: {len(series_list):,}ê°œ")

    if len(series_list) == 0:
        print("âœ… ëª¨ë“  ë°ì´í„°ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    stats["series_total"] = len(series_list)

    # 3. í¬ë¡¤ë§ ì„¤ì •
    print(f"\nğŸš€ í¬ë¡¤ë§ ì‹œì‘")
    print(f"âš™ï¸  Rate Limit: {MAX_CALLS_PER_SECOND}íšŒ/ì´ˆ")

    estimated_time = len(series_list) / MAX_CALLS_PER_SECOND / 60
    print(f"â±ï¸  ì˜ˆìƒ ì‹œê°„: {estimated_time:.0f}ë¶„")

    connector = aiohttp.TCPConnector(
        limit=20,
        force_close=False,
        enable_cleanup_closed=True
    )

    all_results = []
    batch_size = 10

    async with aiohttp.ClientSession(connector=connector, timeout=TIMEOUT) as session:
        for i in range(0, len(series_list), batch_size):
            batch = series_list[i:i + batch_size]

            tasks = [fetch_imdb_rating(session, s['imdb_id']) for s in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in batch_results:
                if isinstance(result, Exception):
                    stats["series_failed"] += 1
                    continue

                if isinstance(result, dict):
                    all_results.append(result)
                    processed_ids.add(result['imdb_id'])
                    stats["series_success"] += 1
                else:
                    stats["series_failed"] += 1

            # ì£¼ê¸°ì  ì €ì¥
            if (i + batch_size) % 50 == 0:
                save_checkpoint(processed_ids)

                if all_results:
                    df_batch = pd.DataFrame(all_results)
                    df_batch = df_batch.drop_duplicates(subset=['imdb_id'])
                    file_exists = Path(output_csv_path).exists()
                    df_batch.to_csv(
                        output_csv_path,
                        mode='a',
                        header=not file_exists,
                        index=False,
                        encoding='utf-8-sig'
                    )
                    all_results.clear()
                    print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ ({len(df_batch):,}ê°œ)")

            # ì§„í–‰ ìƒí™© ì¶œë ¥
            elapsed = (datetime.now() - t0).total_seconds() / 60
            progress = stats["series_success"] + stats["series_failed"]
            rate = progress / elapsed if elapsed > 0 else 0
            eta = (stats["series_total"] - progress) / rate if rate > 0 else 0

            print(
                f"ğŸ“Š ì§„í–‰: {progress}/{stats['series_total']} "
                f"({progress / stats['series_total'] * 100:.1f}%) | "
                f"ì„±ê³µ: {stats['series_success']} | ì‹¤íŒ¨: {stats['series_failed']} | "
                f"ìš”ì²­: {stats['requests']:,}íšŒ | "
                f"ì†ë„: {rate:.1f}ê°œ/ë¶„ | ETA: {eta:.0f}ë¶„"
            )

    # 4. ìµœì¢… ì €ì¥
    print("\nğŸ’¾ ìµœì¢… ì €ì¥ ì¤‘...")

    if all_results:
        df_batch = pd.DataFrame(all_results)
        df_batch = df_batch.drop_duplicates(subset=['imdb_id'])
        file_exists = Path(output_csv_path).exists()
        df_batch.to_csv(
            output_csv_path,
            mode='a',
            header=not file_exists,
            index=False,
            encoding='utf-8-sig'
        )

    # ì „ì²´ ì¤‘ë³µ ì œê±°
    if Path(output_csv_path).exists():
        df_results = pd.read_csv(output_csv_path)
        df_results = df_results.drop_duplicates(subset=['imdb_id'])
        df_results.to_csv(output_csv_path, index=False, encoding='utf-8-sig')
    else:
        df_results = pd.DataFrame()

    # ì²´í¬í¬ì¸íŠ¸ ì œê±°
    if Path(CHECKPOINT_FILE).exists():
        Path(CHECKPOINT_FILE).unlink()

    # 5. ìµœì¢… í†µê³„ ì¶œë ¥
    elapsed = (datetime.now() - t0).total_seconds() / 60

    print("\n" + "=" * 90)
    print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 90)
    print(f"ğŸ“Œ ì‹œë¦¬ì¦ˆ: {stats['series_success']:,}/{stats['series_total']:,}ê°œ ì„±ê³µ")

    if not df_results.empty:
        print(f"ğŸ“Œ ì´ ìˆ˜ì§‘: {len(df_results):,}ê°œ (ì¤‘ë³µ ì œê±° í›„)")

        # ratingì´ ìˆëŠ” ë°ì´í„° í†µê³„
        has_rating = df_results['imdb_rating'].notna().sum()
        print(f"ğŸ“Œ Rating ë³´ìœ : {has_rating:,}ê°œ ({has_rating / len(df_results) * 100:.1f}%)")

        if has_rating > 0:
            print(f"ğŸ“Œ í‰ê·  Rating: {df_results['imdb_rating'].mean():.2f}")
            print(f"ğŸ“Œ í‰ê·  Rating Count: {df_results['imdb_rating_count'].mean():.0f}")
    else:
        print("ğŸ“Œ ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")

    print(f"ğŸ“Œ ì´ ìš”ì²­: {stats['requests']:,}íšŒ")
    print(f"â±ï¸  ì´ ì‹œê°„: {elapsed:.1f}ë¶„ ({elapsed / 60:.2f}ì‹œê°„)")

    if stats['series_success'] > 0 and elapsed > 0:
        print(f"ğŸ“Š ì†ë„: {stats['series_success'] / elapsed:.1f}ê°œ/ë¶„")

    print("=" * 90)

    # ìƒ˜í”Œ ì¶œë ¥
    if not df_results.empty:
        print("\nğŸ“Š ê²°ê³¼ ìƒ˜í”Œ:")
        print(df_results.head(10).to_string())
        print(f"\nâœ… ê²°ê³¼ íŒŒì¼: {output_csv_path}")
