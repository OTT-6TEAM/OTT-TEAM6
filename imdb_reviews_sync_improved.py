# ==========================================================
# IMDB ì „ì²´ ë¦¬ë·° í¬ë¡¤ëŸ¬ - ê°œì„ ëœ ë™ê¸° ë²„ì „
# ì‚¬ìš©ì ì œê³µ ì½”ë“œ ê¸°ë°˜ + ì—ëŸ¬ ì²˜ë¦¬ + ì²´í¬í¬ì¸íŠ¸ ì¶”ê°€
# ==========================================================

import requests
from bs4 import BeautifulSoup
import pandas as pd
from tqdm import tqdm
import time
import json
from pathlib import Path
from datetime import datetime

# ==========================================================
# ì„¤ì •
# ==========================================================

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
}

SLEEP_BETWEEN_SERIES = 1.5  # ì‹œë¦¬ì¦ˆ ê°„ ëŒ€ê¸° ì‹œê°„
SLEEP_BETWEEN_PAGES = 0.5   # í˜ì´ì§€ ê°„ ëŒ€ê¸° ì‹œê°„
MAX_RETRIES = 3              # ì¬ì‹œë„ íšŸìˆ˜

OUTPUT_CSV = "imdb_reviews_full_sync.csv"
CHECKPOINT_FILE = "imdb_checkpoint_sync.json"
FAILED_FILE = "imdb_failed_ids.txt"

# í†µê³„
stats = {
    "total": 0,
    "success": 0,
    "failed": 0,
    "reviews": 0,
    "start_time": None
}

# ==========================================================
# ë¦¬ë·° íŒŒì‹±
# ==========================================================

def parse_review_block(soup, imdb_id):
    """
    soupì—ì„œ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ íŒŒì‹± (ê°œì„ )
    """
    review_blocks = soup.select(".review-container")
    reviews = []
    
    for block in review_blocks:
        # ì œëª©
        title = block.select_one(".title")
        
        # ë‚´ìš©
        content = block.select_one(".text.show-more__control")
        if not content:
            content = block.select_one(".text")
        
        # í‰ì 
        rating = block.select_one(".rating-other-user-rating span")
        
        # ì‘ì„±ì
        author = block.select_one(".display-name-link a")
        if not author:
            author = block.select_one(".display-name-link")
        
        # ë‚ ì§œ
        date = block.select_one(".review-date")
        
        # Helpful íˆ¬í‘œ
        helpful = None
        actions = block.select_one(".actions")
        if actions:
            import re
            match = re.search(r'(\d+)\s+out of\s+(\d+)', actions.get_text())
            if match:
                helpful = f"{match.group(1)}/{match.group(2)}"
        
        # Spoiler ì—¬ë¶€
        spoiler = "spoiler-warning" in str(block)
        
        reviews.append({
            "imdb_id": imdb_id,
            "review_title": title.get_text(strip=True) if title else None,
            "review_text": content.get_text(strip=True) if content else None,
            "review_rating": rating.get_text(strip=True) if rating else None,
            "review_author": author.get_text(strip=True) if author else None,
            "review_date": date.get_text(strip=True) if date else None,
            "helpful_votes": helpful,
            "is_spoiler": spoiler,
        })
    
    return reviews

# ==========================================================
# ì „ì²´ ë¦¬ë·° ìˆ˜ì§‘ (ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)
# ==========================================================

def fetch_all_imdb_reviews(imdb_id, series_title="", max_pages=None):
    """
    IMDb ì „ì²´ ë¦¬ë·° í¬ë¡¤ë§ (paginationKey ì´ìš©)
    
    Args:
        imdb_id: IMDB ID
        series_title: ì‹œë¦¬ì¦ˆ ì œëª© (ë¡œê¹…ìš©)
        max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (Noneì´ë©´ ì „ì²´)
    
    Returns:
        list: ë¦¬ë·° ë¦¬ìŠ¤íŠ¸
    """
    base_url = f"https://www.imdb.com/title/{imdb_id}/reviews"
    ajax_url = f"https://www.imdb.com/title/{imdb_id}/reviews/_ajax"
    all_reviews = []
    page_count = 0
    
    try:
        # 1. ì²« í˜ì´ì§€ ìš”ì²­ (ì¬ì‹œë„ í¬í•¨)
        for attempt in range(MAX_RETRIES):
            try:
                res = requests.get(base_url, headers=HEADERS, timeout=15)
                res.raise_for_status()
                break
            except requests.exceptions.RequestException as e:
                if attempt == MAX_RETRIES - 1:
                    raise
                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        soup = BeautifulSoup(res.text, "html.parser")
        
        # ì²« ë¦¬ë·° íŒŒì‹±
        all_reviews.extend(parse_review_block(soup, imdb_id))
        page_count += 1
        
        # ì²« pagination key
        load_more = soup.select_one("div.load-more-data")
        if load_more is None:
            return all_reviews
        
        pagination_key = load_more.get("data-key")
        
        # 2. Ajax ìš”ì²­ ë°˜ë³µ
        while pagination_key:
            if max_pages and page_count >= max_pages:
                break
            
            # POST ìš”ì²­ (ì¬ì‹œë„ í¬í•¨)
            for attempt in range(MAX_RETRIES):
                try:
                    payload = {"paginationKey": pagination_key}
                    res = requests.post(ajax_url, headers=HEADERS, data=payload, timeout=15)
                    res.raise_for_status()
                    break
                except requests.exceptions.RequestException as e:
                    if attempt == MAX_RETRIES - 1:
                        print(f"âš ï¸  {series_title}: í˜ì´ì§€ {page_count+1} ì‹¤íŒ¨")
                        return all_reviews
                    time.sleep(2 ** attempt)
            
            ajax_soup = BeautifulSoup(res.text, "html.parser")
            
            # ë¦¬ë·° ì¶”ê°€
            new_reviews = parse_review_block(ajax_soup, imdb_id)
            if not new_reviews:  # ë” ì´ìƒ ì—†ìœ¼ë©´ ì¢…ë£Œ
                break
            
            all_reviews.extend(new_reviews)
            page_count += 1
            
            # ë‹¤ìŒ í‚¤ íƒìƒ‰
            load_more = ajax_soup.select_one("div.load-more-data")
            pagination_key = load_more.get("data-key") if load_more else None
            
            time.sleep(SLEEP_BETWEEN_PAGES)  # IMDb block ë°©ì§€
        
        return all_reviews
    
    except Exception as e:
        print(f"âŒ {series_title} ({imdb_id}): {str(e)[:100]}")
        return all_reviews

# ==========================================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
# ==========================================================

def save_checkpoint(processed_ids, failed_ids):
    """ì§„í–‰ ìƒí™© ì €ì¥"""
    checkpoint = {
        'processed_ids': list(processed_ids),
        'failed_ids': list(failed_ids),
        'stats': stats.copy(),
        'timestamp': datetime.now().isoformat()
    }
    with open(CHECKPOINT_FILE, 'w') as f:
        json.dump(checkpoint, f, indent=2)
    
    # ì‹¤íŒ¨ ëª©ë¡ ë³„ë„ ì €ì¥
    if failed_ids:
        with open(FAILED_FILE, 'w') as f:
            f.write('\n'.join(failed_ids))

def load_checkpoint():
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    if Path(CHECKPOINT_FILE).exists():
        with open(CHECKPOINT_FILE, 'r') as f:
            checkpoint = json.load(f)
            return (
                set(checkpoint.get('processed_ids', [])),
                set(checkpoint.get('failed_ids', []))
            )
    return set(), set()

# ==========================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ==========================================================

def collect_all_reviews(input_csv, vote_threshold=10, max_pages=None, save_interval=20):
    """
    ì „ì²´ TV ì‹œë¦¬ì¦ˆ ë¦¬ë·° ìˆ˜ì§‘
    
    Args:
        input_csv: TMDB CSV íŒŒì¼ ê²½ë¡œ
        vote_threshold: ìµœì†Œ vote_count
        max_pages: ì‹œë¦¬ì¦ˆë‹¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        save_interval: ì¤‘ê°„ ì €ì¥ ê°„ê²©
    """
    print("=" * 90)
    print("ğŸ¬ IMDB ì „ì²´ ë¦¬ë·° í¬ë¡¤ëŸ¬ (ê°œì„ ëœ ë™ê¸° ë²„ì „)")
    print("=" * 90)
    
    stats["start_time"] = datetime.now()
    t0 = datetime.now()
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df_series = pd.read_csv(input_csv)
    df_target = df_series[
        (df_series['vote_count'] >= vote_threshold) & 
        (df_series['imdb_id'].notna())
    ]
    
    print(f"âœ… ì „ì²´ ì‹œë¦¬ì¦ˆ: {len(df_series):,}ê°œ")
    print(f"âœ… í•„í„°ë§ (vote_count>={vote_threshold} & imdb_id ì¡´ì¬): {len(df_target):,}ê°œ")
    
    if len(df_target) == 0:
        print("âš ï¸  ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    processed_ids, failed_ids = load_checkpoint()
    
    if processed_ids:
        print(f"ğŸ“Œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {len(processed_ids):,}ê°œ ì²˜ë¦¬ ì™„ë£Œ, {len(failed_ids)}ê°œ ì‹¤íŒ¨")
        df_target = df_target[~df_target['imdb_id'].isin(processed_ids)]
        print(f"ğŸ“Œ ë‚¨ì€ ì‘ì—…: {len(df_target):,}ê°œ")
    
    if len(df_target) == 0:
        print("âœ… ëª¨ë“  ë°ì´í„°ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    stats["total"] = len(df_target)
    
    # 3. í¬ë¡¤ë§
    print(f"\nğŸš€ í¬ë¡¤ë§ ì‹œì‘")
    print(f"âš™ï¸  ëŒ€ê¸° ì‹œê°„: ì‹œë¦¬ì¦ˆ ê°„ {SLEEP_BETWEEN_SERIES}ì´ˆ, í˜ì´ì§€ ê°„ {SLEEP_BETWEEN_PAGES}ì´ˆ")
    print(f"â±ï¸  ì˜ˆìƒ ì‹œê°„: {len(df_target) * SLEEP_BETWEEN_SERIES / 60:.0f}ë¶„ (ìµœì†Œ)")
    
    all_reviews = []
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (ì´ì–´ì„œ ì €ì¥í•˜ê¸° ìœ„í•´)
    if Path(OUTPUT_CSV).exists():
        existing_df = pd.read_csv(OUTPUT_CSV)
        all_reviews = existing_df.to_dict('records')
        print(f"ğŸ“Œ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ: {len(all_reviews):,}ê°œ ë¦¬ë·°")
    
    # tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
    for idx, row in tqdm(df_target.iterrows(), total=len(df_target), desc="ìˆ˜ì§‘ ì¤‘"):
        imdb_id = row['imdb_id']
        title = row.get('title', 'Unknown')
        
        try:
            reviews = fetch_all_imdb_reviews(imdb_id, title, max_pages)
            
            if reviews:
                all_reviews.extend(reviews)
                processed_ids.add(imdb_id)
                stats["success"] += 1
                stats["reviews"] += len(reviews)
                tqdm.write(f"âœ… {title}: {len(reviews):,}ê°œ ë¦¬ë·°")
            else:
                processed_ids.add(imdb_id)
                failed_ids.add(imdb_id)
                stats["failed"] += 1
                tqdm.write(f"âš ï¸  {title}: ë¦¬ë·° ì—†ìŒ")
        
        except Exception as e:
            failed_ids.add(imdb_id)
            stats["failed"] += 1
            tqdm.write(f"âŒ {title}: {str(e)[:50]}")
        
        # ì£¼ê¸°ì  ì €ì¥
        if (stats["success"] + stats["failed"]) % save_interval == 0:
            df_temp = pd.DataFrame(all_reviews)
            df_temp.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
            save_checkpoint(processed_ids, failed_ids)
            
            # ì§„í–‰ ìƒí™©
            elapsed = (datetime.now() - t0).total_seconds() / 60
            progress = stats["success"] + stats["failed"]
            rate = progress / elapsed if elapsed > 0 else 0
            eta = (stats["total"] - progress) / rate if rate > 0 else 0
            
            tqdm.write(f"\nğŸ“Š {progress}/{stats['total']} ({progress/stats['total']*100:.1f}%) | "
                      f"ì„±ê³µ: {stats['success']} | ì‹¤íŒ¨: {stats['failed']} | "
                      f"ë¦¬ë·°: {stats['reviews']:,}ê°œ | ETA: {eta:.0f}ë¶„\n")
        
        time.sleep(SLEEP_BETWEEN_SERIES)
    
    # 4. ìµœì¢… ì €ì¥
    print("\nğŸ’¾ ìµœì¢… ì €ì¥ ì¤‘...")
    df_reviews = pd.DataFrame(all_reviews)
    df_reviews.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
    
    try:
        df_reviews.to_parquet(OUTPUT_CSV.replace('.csv', '.parquet'), index=False)
    except:
        pass
    
    # ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
    if Path(CHECKPOINT_FILE).exists():
        Path(CHECKPOINT_FILE).unlink()
    
    # 5. í†µê³„
    elapsed = (datetime.now() - t0).total_seconds() / 60
    
    print("\n" + "=" * 90)
    print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 90)
    print(f"ğŸ“Œ ì²˜ë¦¬: {stats['success'] + stats['failed']}/{stats['total']}ê°œ")
    print(f"ğŸ“Œ ì„±ê³µ: {stats['success']}ê°œ ({stats['success']/(stats['success']+stats['failed'])*100:.1f}%)")
    print(f"ğŸ“Œ ì‹¤íŒ¨: {stats['failed']}ê°œ")
    print(f"ğŸ“Œ ì´ ë¦¬ë·°: {len(df_reviews):,}ê°œ")
    print(f"ğŸ“Œ í‰ê· : {len(df_reviews)/stats['success']:.1f}ê°œ/ì‹œë¦¬ì¦ˆ")
    print(f"â±ï¸  ì´ ì‹œê°„: {elapsed:.1f}ë¶„ ({elapsed/60:.2f}ì‹œê°„)")
    print(f"ğŸ“Š ì†ë„: {stats['success']/elapsed:.1f}ê°œ/ë¶„")
    print("=" * 90)
    
    # ìƒ˜í”Œ
    print("\nğŸ“Š ìƒ˜í”Œ ë°ì´í„°:")
    print(df_reviews.head(3))
    print(f"\nâœ… ê²°ê³¼ íŒŒì¼: {OUTPUT_CSV}")
    
    if failed_ids:
        print(f"âš ï¸  ì‹¤íŒ¨ ëª©ë¡: {FAILED_FILE}")

# ==========================================================
# ì‹¤í–‰
# ==========================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='IMDB ë¦¬ë·° í¬ë¡¤ëŸ¬ (ë™ê¸°)')
    parser.add_argument('--input', '-i', default='tv_series_2013_0101_0215_FULL.csv',
                        help='ì…ë ¥ CSV íŒŒì¼')
    parser.add_argument('--vote', '-v', type=int, default=10,
                        help='ìµœì†Œ vote_count')
    parser.add_argument('--max-pages', '-m', type=int, default=None,
                        help='ì‹œë¦¬ì¦ˆë‹¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜')
    parser.add_argument('--save-interval', '-s', type=int, default=20,
                        help='ì¤‘ê°„ ì €ì¥ ê°„ê²©')
    
    args = parser.parse_args()
    
    if not Path(args.input).exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
    else:
        collect_all_reviews(
            args.input,
            args.vote,
            args.max_pages,
            args.save_interval
        )
