import pandas as pd
import aiohttp
import asyncio
from bs4 import BeautifulSoup
import time
import re
from tqdm.asyncio import tqdm
import argparse
from datetime import datetime


# ============================================================
# Rate Limiter
# ============================================================
class RateLimiter:
    """ì´ˆë‹¹ ìš”ì²­ ìˆ˜ë¥¼ ì œí•œí•˜ëŠ” Rate Limiter"""
    def __init__(self, rate):
        self.rate = rate
        self.tokens = rate
        self.updated_at = time.monotonic()
        self.lock = asyncio.Lock()

    async def acquire(self):
        async with self.lock:
            now = time.monotonic()
            elapsed = now - self.updated_at
            
            self.tokens = min(self.rate, self.tokens + elapsed * self.rate)
            self.updated_at = now

            if self.tokens < 1:
                sleep_time = (1 - self.tokens) / self.rate
                await asyncio.sleep(sleep_time)
                self.tokens = 1

            self.tokens -= 1


# ============================================================
# ë¹„ë™ê¸° ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜
# ============================================================
async def get_imdb_data_async(session, imdb_id, semaphore, rate_limiter, max_retries=3):
    """ë¹„ë™ê¸°ë¡œ IMDB ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    url = f"https://www.imdb.com/title/{imdb_id}/"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive',
    }
    
    async with semaphore:
        for attempt in range(max_retries):
            try:
                await rate_limiter.acquire()
                
                async with session.get(url, headers=headers) as response:
                    response.raise_for_status()
                    html = await response.text()
                    
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # IMDB Rating ì¶”ì¶œ
                    imdb_rating = None
                    rating_span = soup.find('span', class_='sc-4dc495c1-1')
                    if rating_span:
                        imdb_rating = rating_span.text.strip()
                    
                    if not imdb_rating:
                        rating_span = soup.find('span', {'data-testid': 'rating-value'})
                        if rating_span:
                            imdb_rating = rating_span.text.strip()
                    
                    if not imdb_rating:
                        rating_pattern = re.compile(r'(\d+\.\d+)/10')
                        match = rating_pattern.search(html)
                        if match:
                            imdb_rating = match.group(1)
                    
                    # Metascore ì¶”ì¶œ
                    metascore = None
                    metascore_span = soup.find('span', class_='sc-9fe7b0ef-0')
                    if metascore_span:
                        metascore = metascore_span.text.strip()
                    
                    if not metascore:
                        metascore_span = soup.find('span', class_=re.compile('metacritic-score'))
                        if metascore_span:
                            metascore = metascore_span.text.strip()
                    
                    return {
                        'imdb_id': imdb_id,
                        'imdb_rating': imdb_rating,
                        'metascore': metascore,
                        'status': 'success',
                        'url': url
                    }
                    
            except aiohttp.ClientError as e:
                if attempt < max_retries - 1:
                    await asyncio.sleep((attempt + 1) * 2)
                    continue
                else:
                    return {
                        'imdb_id': imdb_id,
                        'imdb_rating': None,
                        'metascore': None,
                        'status': f'error: {str(e)}',
                        'url': url
                    }
            except asyncio.TimeoutError:
                if attempt < max_retries - 1:
                    await asyncio.sleep((attempt + 1) * 2)
                    continue
                else:
                    return {
                        'imdb_id': imdb_id,
                        'imdb_rating': None,
                        'metascore': None,
                        'status': 'error: timeout',
                        'url': url
                    }
            except Exception as e:
                return {
                    'imdb_id': imdb_id,
                    'imdb_rating': None,
                    'metascore': None,
                    'status': f'parsing error: {str(e)}',
                    'url': url
                }
        
        return {
            'imdb_id': imdb_id,
            'imdb_rating': None,
            'metascore': None,
            'status': 'error: max retries exceeded',
            'url': url
        }


# ============================================================
# ë°°ì¹˜ ì²˜ë¦¬
# ============================================================
async def process_batch(session, batch_data, semaphore, rate_limiter, pbar, max_retries):
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    tasks = []
    for row_data in batch_data:
        imdb_id = row_data['imdb_id']
        task = get_imdb_data_async(session, imdb_id, semaphore, rate_limiter, max_retries)
        tasks.append(task)
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    processed_results = []
    for row_data, scraping_result in zip(batch_data, results):
        if isinstance(scraping_result, dict):
            result = {
                'imdb_id': row_data['imdb_id'],
                'series_name': row_data.get('name', ''),
                'original_vote_count': row_data.get('vote_count', ''),
                'imdb_rating': scraping_result['imdb_rating'],
                'metascore': scraping_result['metascore'],
                'url': scraping_result['url'],
                'status': scraping_result['status']
            }
            processed_results.append(result)
            pbar.update(1)
        else:
            result = {
                'imdb_id': row_data['imdb_id'],
                'series_name': row_data.get('name', ''),
                'original_vote_count': row_data.get('vote_count', ''),
                'imdb_rating': None,
                'metascore': None,
                'url': f"https://www.imdb.com/title/{row_data['imdb_id']}/",
                'status': f'exception: {str(scraping_result)}'
            }
            processed_results.append(result)
            pbar.update(1)
    
    return processed_results


# ============================================================
# ë©”ì¸ ë¹„ë™ê¸° í•¨ìˆ˜
# ============================================================
async def main_async(df_filtered, config):
    """ë©”ì¸ ë¹„ë™ê¸° ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜"""
    print()
    print("ğŸš€ ë¹„ë™ê¸° ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    print(f"âš™ï¸  ë™ì‹œ ìš”ì²­ ìˆ˜: {config['concurrent']}")
    print(f"âš™ï¸  ì´ˆë‹¹ ìš”ì²­ ìˆ˜: {config['rate']}")
    print(f"âš™ï¸  ì¬ì‹œë„ íšŸìˆ˜: {config['retries']}")
    print(f"âš™ï¸  ë°°ì¹˜ í¬ê¸°: {config['batch_size']}")
    print("-" * 60)
    
    semaphore = asyncio.Semaphore(config['concurrent'])
    rate_limiter = RateLimiter(config['rate'])
    
    data_list = df_filtered.to_dict('records')
    all_results = []
    
    connector = aiohttp.TCPConnector(
        limit=config['concurrent'] * 2,
        limit_per_host=config['concurrent'],
        force_close=False,
        enable_cleanup_closed=True
    )
    
    timeout = aiohttp.ClientTimeout(total=config['timeout'])
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        with tqdm(total=len(data_list), desc="ì§„í–‰ ìƒí™©", unit="ê°œ") as pbar:
            for i in range(0, len(data_list), config['batch_size']):
                batch = data_list[i:i + config['batch_size']]
                
                batch_results = await process_batch(
                    session, batch, semaphore, rate_limiter, pbar, config['retries']
                )
                all_results.extend(batch_results)
                
                # ì¤‘ê°„ ì €ì¥
                if len(all_results) % config['batch_size'] == 0:
                    temp_df = pd.DataFrame(all_results)
                    temp_df.to_csv('imdb_scraping_temp.csv', index=False, encoding='utf-8-sig')
    
    return all_results


# ============================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================
def main():
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='IMDB ë¹„ë™ê¸° ìŠ¤í¬ë˜í•‘ (ê³ ì„±ëŠ¥)')
    parser.add_argument('--concurrent', '-c', type=int, default=15,
                       help='ë™ì‹œ ìš”ì²­ ìˆ˜ (ê¸°ë³¸: 15)')
    parser.add_argument('--rate', '-r', type=int, default=5,
                       help='ì´ˆë‹¹ ìš”ì²­ ìˆ˜ (ê¸°ë³¸: 5)')
    parser.add_argument('--retries', type=int, default=3,
                       help='ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸: 3)')
    parser.add_argument('--batch-size', '-b', type=int, default=100,
                       help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 100)')
    parser.add_argument('--timeout', '-t', type=int, default=30,
                       help='íƒ€ì„ì•„ì›ƒ ì´ˆ (ê¸°ë³¸: 30)')
    parser.add_argument('--input', '-i', default='tv_series_2005_2015_FULL.csv',
                       help='ì…ë ¥ CSV íŒŒì¼')
    parser.add_argument('--output', '-o', default='imdb_ratings_metascores_async.csv',
                       help='ì¶œë ¥ CSV íŒŒì¼')
    parser.add_argument('--vote-threshold', '-v', type=int, default=30,
                       help='ìµœì†Œ vote_count (ê¸°ë³¸: 30)')
    
    # í”„ë¦¬ì…‹ ì˜µì…˜
    parser.add_argument('--preset', choices=['safe', 'balanced', 'fast'],
                       help='í”„ë¦¬ì…‹ ì„¤ì • (safe/balanced/fast)')
    
    args = parser.parse_args()
    
    # í”„ë¦¬ì…‹ ì ìš©
    if args.preset == 'safe':
        args.concurrent = 5
        args.rate = 2
    elif args.preset == 'balanced':
        args.concurrent = 15
        args.rate = 5
    elif args.preset == 'fast':
        args.concurrent = 25
        args.rate = 8
    
    # ì„¤ì • ë”•ì…”ë„ˆë¦¬
    config = {
        'concurrent': args.concurrent,
        'rate': args.rate,
        'retries': args.retries,
        'batch_size': args.batch_size,
        'timeout': args.timeout
    }
    
    print("=" * 60)
    print("IMDB Rating & Metascore ë¹„ë™ê¸° ìŠ¤í¬ë˜í•‘ (ê³ ê¸‰)")
    print("âš¡ ì„¤ì • ê°€ëŠ¥í•œ ê³ ì„±ëŠ¥ ë²„ì „")
    print("=" * 60)
    print()
    
    # ì„¤ì • ì¶œë ¥
    print("í˜„ì¬ ì„¤ì •:")
    for key, value in config.items():
        print(f"  - {key}: {value}")
    print()
    
    start_time = datetime.now()
    
    # CSV íŒŒì¼ ì½ê¸°
    try:
        df_series = pd.read_csv(args.input)
        print(f"âœ“ CSV íŒŒì¼ ë¡œë”© ì™„ë£Œ: {args.input}")
        print(f"  - ì „ì²´ ì‹œë¦¬ì¦ˆ: {len(df_series)}ê°œ")
    except FileNotFoundError as e:
        print(f"âœ— CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return None
    
    # í•„í„°ë§
    df_filtered = df_series[(df_series['vote_count'] >= args.vote_threshold) & 
                           (df_series['imdb_id'].notna())]
    
    print(f"âœ“ í•„í„°ë§ ì™„ë£Œ (vote_count >= {args.vote_threshold} & imdb_id ì¡´ì¬)")
    print(f"  - í•„í„°ë§ëœ ì‹œë¦¬ì¦ˆ: {len(df_filtered)}ê°œ")
    print()
    
    # ì˜ˆìƒ ì‹œê°„
    estimated_time = (len(df_filtered) / config['concurrent']) / config['rate'] / 60
    print(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {estimated_time:.1f}ë¶„")
    print()
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    try:
        results = asyncio.run(main_async(df_filtered, config))
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return None
    
    print()
    print("-" * 60)
    
    # ê²°ê³¼ ì €ì¥
    result_df = pd.DataFrame(results)
    result_df.to_csv(args.output, index=False, encoding='utf-8-sig')
    
    end_time = datetime.now()
    elapsed = (end_time - start_time).total_seconds()
    
    print()
    print("=" * 60)
    print("ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
    print("=" * 60)
    print()
    print(f"âœ“ ì´ {len(result_df)}ê°œì˜ ì‹œë¦¬ì¦ˆ ì²˜ë¦¬")
    print(f"âœ“ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
    print(f"âœ“ í‰ê·  ì†ë„: {len(result_df)/elapsed:.1f}ê°œ/ì´ˆ")
    print(f"âœ“ ê²°ê³¼ íŒŒì¼: {args.output}")
    print()
    
    # í†µê³„
    success_count = len(result_df[result_df['status'] == 'success'])
    rating_count = result_df['imdb_rating'].notna().sum()
    metascore_count = result_df['metascore'].notna().sum()
    
    print("=" * 60)
    print("í†µê³„")
    print("=" * 60)
    print(f"ì„±ê³µë¥ : {success_count/len(result_df)*100:.1f}% ({success_count}/{len(result_df)})")
    print(f"IMDB Rating: {rating_count}ê°œ ({rating_count/len(result_df)*100:.1f}%)")
    print(f"Metascore: {metascore_count}ê°œ ({metascore_count/len(result_df)*100:.1f}%)")
    
    # ì„±ëŠ¥ ë¹„êµ
    sync_time = len(result_df) * 1.5
    speedup = sync_time / elapsed if elapsed > 0 else 0
    print()
    print("=" * 60)
    print("âš¡ ì„±ëŠ¥ ë¹„êµ")
    print("=" * 60)
    print(f"ë™ê¸° ë°©ì‹ ì˜ˆìƒ: {sync_time/60:.1f}ë¶„")
    print(f"ë¹„ë™ê¸° ì‹¤ì œ: {elapsed/60:.1f}ë¶„")
    print(f"ì†ë„ í–¥ìƒ: {speedup:.1f}ë°° âš¡")
    
    return result_df


if __name__ == "__main__":
    result_df = main()
    
    if result_df is not None:
        print()
        print("=" * 60)
        print("ìƒ˜í”Œ ê²°ê³¼ (ì²˜ìŒ 5ê°œ)")
        print("=" * 60)
        print(result_df[['imdb_id', 'series_name', 'imdb_rating', 'metascore', 'status']].head(5).to_string())
