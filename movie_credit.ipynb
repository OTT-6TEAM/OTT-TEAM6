{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMDB APIë¡œ ë°ì´í„° ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TMDB ì˜í™” ë°ì´í„° ì¶”ê°€ ìˆ˜ì§‘\n",
    "\n",
    "1. ëŒ€ìƒ:Â TMDB ì˜í™” ë°ì´í„°\n",
    "2. ê¸°ê°„:Â 2005ë…„ 1ì›” 1ì¼ ~ 2025ë…„ 11ì›” 30ì¼ê¹Œì§€ (21ë…„ì¹˜)\n",
    "3. ë²”ìœ„:Â ì „ì²´ ë°ì´í„° ì¤‘ vote_count >= 30, imdb_idê°€ ìˆëŠ” movie_id ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì§‘\n",
    "4. ì¡°ê±´:Â ëˆ„ë½ëœ í˜ì´ì§€ê°€ 0%ì—¬ì•¼ í•¨, TMDB API ì´ˆë‹¹ ìš”ì²­ ìˆ˜(40 req/s) ì œí•œ ëŒ€ì‘, 500í˜ì´ì§€ ì œí•œ ëŒ€ì‘, ì¤‘ë‹¨/ì¬ê°œ ì§€ì›, ìˆ˜ì§‘ íŒŒì¼ì€ í•˜ë‚˜ì˜ Parquetë¡œ ì €ì¥, Jupyterì—ì„œ ê·¸ëŒ€ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ, 5~6ì‹œê°„ ë‚´ ìˆ˜ì§‘ ì™„ë£Œ\n",
    "5. í•„ìˆ˜ ì»¬ëŸ¼ (ìˆœì„œëŒ€ë¡œ): 19ê°œ ì»¬ëŸ¼\n",
    "- ê³µí†µ í•„ìˆ˜ ì»¬ëŸ¼\n",
    "imdb_id\n",
    "movie_id\n",
    "vote_count\n",
    "title\n",
    "\n",
    "(1) ê°ë…(directors) ì „ìš© í•„ìˆ˜ ì»¬ëŸ¼\n",
    "director_idsÂ Â Â Â  = \"525; 6193\"\n",
    "directorsÂ Â Â Â Â Â  = \"Christopher Nolan; Denis Villeneuve\"\n",
    "director_genderÂ Â Â  = \"2; 2\"\n",
    "director_profile_path\n",
    "\n",
    "(2) ì‘ê°€(writer) ë¶„ì„ìš© í•„ìˆ˜ ì»¬ëŸ¼\n",
    "writer_idsÂ Â Â  = \"12452; 142521\"\n",
    "writersÂ Â Â Â Â  = \"Jonathan Nolan; Lisa Joy\"\n",
    "writer_rolesÂ  = job ê¸°ì¤€ ì—­í• \n",
    "writer_genderÂ Â  = \"2; 1\"\n",
    "writer_profile_path\n",
    "\n",
    "(3) ì£¼ì—° ë°°ìš°(top_cast) ìƒìœ„ 5ëª… ì „ìš© í•„ìˆ˜ ì»¬ëŸ¼\n",
    "top_cast_idsÂ Â  = \"1190668; 505710; 61512\"\n",
    "top_cast_orderÂ  = \"0; 1; 2; 3; 4\"\n",
    "top_castÂ Â Â Â  = \"TimothÃ©e Chalamet; Zendaya; Rebecca Ferguson\"\n",
    "character (ë°°ì—­ëª…)\n",
    "top_cast_genderÂ  = \"2; 1; 1\"\n",
    "top_cast_profile_path\n",
    "\n",
    "6. ì£¼ì˜ì‚¬í•­:\n",
    "- crewì—ëŠ” ê°ë…/ì‘ê°€ ì™¸ ë‹¤ì–‘í•œ ì§êµ°(department/job)ì´ í¬í•¨ë˜ë¯€ë¡œ, í•„í„° ì •í™•íˆ í•´ì•¼ í•¨\n",
    "- ê°ë…:Â crew ì•ˆì—ì„œ job == \"Director\",Â ê³µë™ ê°ë…Â ìˆì„ ìˆ˜ ìˆìŒ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„)\n",
    "- ì‘ê°€:Â writers = [\n",
    "Â Â Â Â c for c in crew_list\n",
    "Â Â Â Â if c.get(\"job\") in (\"Writer\", \"Screenplay\", \"Story\", \"Creator\", \"Novel\", \"Comic Book\")\n",
    "Â Â Â Â or c.get(\"department\") == \"Writing\"\n",
    "Â Â ]\n",
    "Â -> ê·¸ë˜ì•¼ ì›ì•ˆ, ê°ë³¸, ì§‘í•„ì ëª¨ë‘ ì»¤ë²„ë¨,Â ê³µë™ ì‘ê°€Â ìˆì„ ìˆ˜ ìˆìŒ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„)\n",
    "-Â ì£¼ì—° ë°°ìš° top 5:Â \n",
    "filtered = [c for c in sorted_cast if c.get(\"character\")]\n",
    "top_5 = filtered[:5]\n",
    "-> cast ë°°ì—´ì„ order í•„ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ (ë°˜ë“œì‹œ ì£¼ì—° ë°°ìš°ë¶€í„° 5ëª… ì´ë¦„ìœ¼ë¡œ ìˆ˜ì§‘, ì ˆëŒ€ popularity ê¸°ì¤€ ì“°ë©´ ì•ˆ ë¨)\n",
    "\n",
    "7. ê³µì‹ë¬¸ì„œ:Â https://developer.themoviedb.org/reference/movie-credits\n",
    "8. ìˆ˜ì§‘ ë°©ì‹: ë¹„ë™ê¸° ë°©ì‹\n",
    "9. ìˆ˜ì§‘ ê²°ê³¼: 28290 rows Ã— 19 columns, ì•½ 16ë¶„ ì†Œìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Discover ë‹¨ê³„ ì‹œì‘ (movie_id ìˆ˜ì§‘)\n",
      "ğŸ“… ë¶„í• ëœ ë‚ ì§œ êµ¬ê°„: 4\n",
      "  â–· [1/4] 2005-01-01 ~ 2010-03-25 (pages=266)\n",
      "  â–· [2/4] 2010-03-26 ~ 2015-06-17 (pages=364)\n",
      "  â–· [3/4] 2015-06-18 ~ 2020-09-08 (pages=460)\n",
      "  â–· [4/4] 2020-09-09 ~ 2025-11-30 (pages=331)\n",
      "ğŸ‰ Discover ì™„ë£Œ: ê³ ìœ  movie_id 28,384ê°œ í™•ë³´\n",
      "â³ ì²˜ë¦¬ ëŒ€ìƒ: 23,984\n",
      "   ì™„ë£Œ: 4,399, ì‹¤íŒ¨: 1\n",
      "  â–· Batch 1/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =400, 11.4s\n",
      "  â–· Batch 2/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =799, 11.4s\n",
      "  â–· Batch 3/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =1199, 12.1s\n",
      "  â–· Batch 4/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =1599, 11.8s\n",
      "  â–· Batch 5/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =1999, 12.1s\n",
      "  â–· Batch 6/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =2399, 12.0s\n",
      "  â–· Batch 7/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =2798, 12.0s\n",
      "  â–· Batch 8/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =3197, 12.0s\n",
      "  â–· Batch 9/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =3595, 12.0s\n",
      "  â–· Batch 10/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =3993, 12.1s\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (batch=10)\n",
      "  â–· Batch 11/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =4393, 12.1s\n",
      "  â–· Batch 12/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =4793, 11.9s\n",
      "  â–· Batch 13/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =5193, 11.9s\n",
      "  â–· Batch 14/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =5593, 11.9s\n",
      "  â–· Batch 15/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =5993, 11.8s\n",
      "  â–· Batch 16/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =6392, 11.8s\n",
      "  â–· Batch 17/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =6792, 11.8s\n",
      "  â–· Batch 18/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =7191, 12.0s\n",
      "  â–· Batch 19/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =7590, 11.8s\n",
      "  â–· Batch 20/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =7989, 12.0s\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (batch=20)\n",
      "  â–· Batch 21/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =8389, 11.9s\n",
      "  â–· Batch 22/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =8788, 11.9s\n",
      "  â–· Batch 23/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =9188, 12.0s\n",
      "  â–· Batch 24/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =9588, 12.0s\n",
      "  â–· Batch 25/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =9987, 12.0s\n",
      "  â–· Batch 26/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =10386, 12.0s\n",
      "  â–· Batch 27/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =10786, 11.9s\n",
      "  â–· Batch 28/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =11186, 11.9s\n",
      "  â–· Batch 29/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =11586, 11.8s\n",
      "  â–· Batch 30/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =11986, 11.8s\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (batch=30)\n",
      "  â–· Batch 31/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =12386, 11.9s\n",
      "  â–· Batch 32/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =12785, 12.1s\n",
      "  â–· Batch 33/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =13185, 12.7s\n",
      "  â–· Batch 34/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =13583, 11.9s\n",
      "  â–· Batch 35/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =13981, 12.0s\n",
      "  â–· Batch 36/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =14380, 11.8s\n",
      "  â–· Batch 37/60: 400ê°œ ì²˜ë¦¬, ëˆ„ì =14780, 11.9s\n",
      "  â–· Batch 38/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =15179, 11.9s\n",
      "  â–· Batch 39/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =15577, 12.0s\n",
      "  â–· Batch 40/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =15976, 11.8s\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (batch=40)\n",
      "  â–· Batch 41/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =16374, 11.8s\n",
      "  â–· Batch 42/60: 397ê°œ ì²˜ë¦¬, ëˆ„ì =16771, 12.2s\n",
      "  â–· Batch 43/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =17170, 11.9s\n",
      "  â–· Batch 44/60: 397ê°œ ì²˜ë¦¬, ëˆ„ì =17567, 11.8s\n",
      "  â–· Batch 45/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =17966, 11.8s\n",
      "  â–· Batch 46/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =18364, 11.9s\n",
      "  â–· Batch 47/60: 395ê°œ ì²˜ë¦¬, ëˆ„ì =18759, 11.8s\n",
      "  â–· Batch 48/60: 397ê°œ ì²˜ë¦¬, ëˆ„ì =19156, 12.0s\n",
      "  â–· Batch 49/60: 395ê°œ ì²˜ë¦¬, ëˆ„ì =19551, 11.9s\n",
      "  â–· Batch 50/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =19950, 11.9s\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (batch=50)\n",
      "  â–· Batch 51/60: 391ê°œ ì²˜ë¦¬, ëˆ„ì =20341, 11.8s\n",
      "  â–· Batch 52/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =20740, 12.0s\n",
      "  â–· Batch 53/60: 396ê°œ ì²˜ë¦¬, ëˆ„ì =21136, 11.9s\n",
      "  â–· Batch 54/60: 398ê°œ ì²˜ë¦¬, ëˆ„ì =21534, 12.0s\n",
      "  â–· Batch 55/60: 397ê°œ ì²˜ë¦¬, ëˆ„ì =21931, 12.0s\n",
      "  â–· Batch 56/60: 396ê°œ ì²˜ë¦¬, ëˆ„ì =22327, 11.9s\n",
      "  â–· Batch 57/60: 399ê°œ ì²˜ë¦¬, ëˆ„ì =22726, 11.9s\n",
      "  â–· Batch 58/60: 391ê°œ ì²˜ë¦¬, ëˆ„ì =23117, 12.1s\n",
      "  â–· Batch 59/60: 397ê°œ ì²˜ë¦¬, ëˆ„ì =23514, 11.8s\n",
      "  â–· Batch 60/60: 377ê°œ ì²˜ë¦¬, ëˆ„ì =23891, 11.4s\n",
      "âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: [Errno 2] No such file or directory: 'tmdb_movie_full/checkpoint.tmp' -> 'tmdb_movie_full/checkpoint.json'\n",
      "ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (batch=60)\n",
      "\n",
      "ğŸ“¦ JSONL â†’ Parquet ë³€í™˜ ì¤‘...\n",
      "\n",
      "ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! (15.9ë¶„)\n",
      "ğŸ“ ì €ì¥ íŒŒì¼: tmdb_movie_full/tmdb_movies_2005_2025_FULL.parquet\n",
      "   ì´ ì˜í™” ìˆ˜: 28,290í¸\n",
      "   ì‹¤íŒ¨ ì˜í™” ìˆ˜: 94í¸\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# TMDB MOVIE FULL COLLECTOR â€” PRODUCTION V9 (Extreme-Day FULL)\n",
    "# (2005-01-01 ~ 2025-11-30)\n",
    "#\n",
    "# - ëŒ€ìƒ: TMDB ì˜í™” (vote_count >= 30 + imdb_id í•„ìˆ˜)\n",
    "# - íŠ¹ì§•:\n",
    "#   * discover/movie ê¸°ê°„ ìë™ ë¶„í•  (500í˜ì´ì§€ ì œí•œ ëŒ€ì‘)\n",
    "#   * âœ… ì–´ë–¤ êµ¬ê°„ë„ 500í˜ì´ì§€ ì´ˆê³¼ ìƒíƒœë¡œ ìˆ˜ì§‘í•˜ì§€ ì•ŠìŒ\n",
    "#   * âœ… ë§Œì•½ \"ë‹¨ì¼ ë‚ ì§œ\"ì—ì„œ total_pages > 500 ë°œìƒ ì‹œ â†’ RuntimeError ë°œìƒ\n",
    "#       â†’ TMDB API í•œê³„ë¡œ ê·¸ ë‚ ì§œëŠ” ì „ìˆ˜ ìˆ˜ì§‘ ë¶ˆê°€í•˜ë‹¤ëŠ” ê²ƒì„ ëª…ì‹œì ìœ¼ë¡œ ì•Œë ¤ì¤Œ\n",
    "#       â†’ ì¦‰, \"ì¡°ìš©í•œ ëˆ„ë½\"ì€ ì ˆëŒ€ ì—†ìŒ (0% ëˆ„ë½ ë³´ì¥ or ì‹¤íŒ¨)\n",
    "#   * ê°ë… / ì‘ê°€ / ì£¼ì—°ë°°ìš°(Top 5) í¬ë ˆë”§ ì „ìˆ˜ ìˆ˜ì§‘\n",
    "#   * ì´ˆë‹¹ ìš”ì²­ ì œí•œ(35 req/s) + ì¬ì‹œë„(backoff)\n",
    "#   * JSONL ìŠ¤íŠ¸ë¦¼ ì €ì¥ + Parquet ìµœì¢… ì €ì¥\n",
    "#   * ì¤‘ë‹¨/ì¬ê°œ: checkpoint + JSONL + failed_ids\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIG\n",
    "# ==========================================================\n",
    "DATA_DIR = Path(\"./tmdb_movie_full\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MOVIE_IDS_PATH     = DATA_DIR / \"movie_ids_2005_2025.json\"\n",
    "DETAILS_JSONL_PATH = DATA_DIR / \"movie_details_2005_2025.jsonl\"\n",
    "CHECKPOINT_PATH    = DATA_DIR / \"checkpoint.json\"\n",
    "FAILED_IDS_PATH    = DATA_DIR / \"failed_ids.json\"\n",
    "FINAL_PARQUET_PATH = DATA_DIR / \"tmdb_movies_2005_2025_FULL.parquet\"\n",
    "\n",
    "API_KEY = os.getenv(\"TMDB_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"í™˜ê²½ë³€ìˆ˜ TMDB_API_KEY ì„¤ì • í•„ìš”!\")\n",
    "\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "HEADERS  = {\"accept\": \"application/json\"}\n",
    "\n",
    "TIMEOUT   = 10\n",
    "MAX_RETRIES = 5\n",
    "MAX_CALLS_PER_SECOND   = 35\n",
    "MAX_CONCURRENT_REQUESTS = 150  # íš¨ìœ¨ì„±\n",
    "\n",
    "START_DATE = datetime(2005, 1, 1)\n",
    "END_DATE   = datetime(2025, 11, 30)\n",
    "\n",
    "WRITER_JOBS = {\n",
    "    \"Writer\", \"Screenplay\", \"Story\", \"Creator\",\n",
    "    \"Novel\", \"Comic Book\", \"Adaptation\", \"Screenstory\"\n",
    "}\n",
    "\n",
    "jsonl_lock      = asyncio.Lock()\n",
    "checkpoint_lock = asyncio.Lock()\n",
    "\n",
    "# ==========================================================\n",
    "# Rate Limiter\n",
    "# ==========================================================\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_calls=35, period=1.0):\n",
    "        self.max_calls = max_calls\n",
    "        self.period = period\n",
    "        self.calls = deque()\n",
    "        self.lock  = asyncio.Lock()\n",
    "\n",
    "    async def acquire(self):\n",
    "        async with self.lock:\n",
    "            loop = asyncio.get_running_loop()\n",
    "            now = loop.time()\n",
    "\n",
    "            # ì˜¤ë˜ëœ í˜¸ì¶œ ì œê±°\n",
    "            while self.calls and now - self.calls[0] > self.period:\n",
    "                self.calls.popleft()\n",
    "\n",
    "            if len(self.calls) >= self.max_calls:\n",
    "                wait = self.period - (now - self.calls[0]) + 0.02\n",
    "                if wait > 0:\n",
    "                    await asyncio.sleep(wait)\n",
    "\n",
    "                now = loop.time()\n",
    "                while self.calls and now - self.calls[0] > self.period:\n",
    "                    self.calls.popleft()\n",
    "\n",
    "            self.calls.append(loop.time())\n",
    "\n",
    "\n",
    "rate_limiter = RateLimiter(MAX_CALLS_PER_SECOND)\n",
    "\n",
    "# ==========================================================\n",
    "# fetch_json\n",
    "# ==========================================================\n",
    "async def fetch_json(session, url, params, desc=\"\", retries=MAX_RETRIES):\n",
    "    for attempt in range(1, retries + 1):\n",
    "        await rate_limiter.acquire()\n",
    "\n",
    "        try:\n",
    "            async with session.get(url, params=params, timeout=TIMEOUT) as resp:\n",
    "                if resp.status == 200:\n",
    "                    return await resp.json()\n",
    "\n",
    "                if resp.status == 404:\n",
    "                    return None\n",
    "\n",
    "                if resp.status in {429, 500, 502, 503, 504}:\n",
    "                    if resp.status == 429:\n",
    "                        retry_after = resp.headers.get(\"Retry-After\", \"2\")\n",
    "                        try:\n",
    "                            retry_after = int(retry_after)\n",
    "                        except Exception:\n",
    "                            retry_after = 2\n",
    "                        await asyncio.sleep(retry_after)\n",
    "                    else:\n",
    "                        await asyncio.sleep(attempt * 2)\n",
    "                    continue\n",
    "\n",
    "                # ê¸°íƒ€ ì—ëŸ¬ëŠ” ì‹¤íŒ¨ë¡œ ì²˜ë¦¬\n",
    "                return None\n",
    "\n",
    "        except asyncio.TimeoutError:\n",
    "            await asyncio.sleep(attempt * 2)\n",
    "        except Exception:\n",
    "            await asyncio.sleep(attempt * 2)\n",
    "\n",
    "    return None\n",
    "\n",
    "# ==========================================================\n",
    "# ë¬¸ìì—´ ì •ì œ ìœ í‹¸\n",
    "# ==========================================================\n",
    "def sanitize_text_mild(text):\n",
    "    \"\"\"\n",
    "    ê²½ë¯¸í•œ ì •ì œ (ì´ë¦„, ì œëª©ìš©)\n",
    "    - ì¤„ë°”ê¿ˆ, HTML íƒœê·¸, ì—°ì† ê³µë°±ë§Œ ì œê±°\n",
    "    - ìœ ë‹ˆì½”ë“œ ë³´ì¡´ (í•œê¸€, ì¼ë³¸ì–´, ìœ ëŸ½ì–´ ë“±)\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # HTML íƒœê·¸ ì œê±°\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    # ì¤„ë°”ê¿ˆ, íƒ­ ì œê±°\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    # ì—°ì† ê³µë°± ì œê±°\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def sanitize_text_strong(text):\n",
    "    \"\"\"\n",
    "    ê°•ë ¥í•œ ì •ì œ (characterìš© - Parquet ì•ˆì •ì„± ìš°ì„ )\n",
    "    - ì œì–´ ë¬¸ì, íŠ¹ìˆ˜ ìœ ë‹ˆì½”ë“œ ì œê±°\n",
    "    - ì¼ë°˜ ìœ ë‹ˆì½”ë“œëŠ” ë³´ì¡´\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # HTML íƒœê·¸ ì œê±°\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    # HTML ì—”í‹°í‹° ì œê±°\n",
    "    text = re.sub(r\"&[a-zA-Z]+;\", \" \", text)\n",
    "    # ì œì–´ ë¬¸ì ì œê±°\n",
    "    text = re.sub(r\"[\\x00-\\x1F\\x7F-\\x9F]\", \" \", text)\n",
    "    # ì—°ì† ê³µë°± ì œê±°\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def join_values(arr, use_strong_sanitize=False):\n",
    "    \"\"\"\n",
    "    ë¦¬ìŠ¤íŠ¸ë¥¼ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ join\n",
    "    - use_strong_sanitize=True: characterìš© (ê°•ë ¥í•œ ì •ì œ)\n",
    "    - use_strong_sanitize=False: ì´ë¦„/ì œëª©ìš© (ê²½ë¯¸í•œ ì •ì œ)\n",
    "    \"\"\"\n",
    "    if not arr:\n",
    "        return \"\"\n",
    "    \n",
    "    sanitize_func = sanitize_text_strong if use_strong_sanitize else sanitize_text_mild\n",
    "    cleaned = [sanitize_func(x) if x is not None else \"\" for x in arr]\n",
    "    return \"; \".join(cleaned)\n",
    "\n",
    "# ==========================================================\n",
    "# DISCOVER\n",
    "# ==========================================================\n",
    "async def discover_total_pages(session, start_date: datetime, end_date: datetime):\n",
    "    params = {\n",
    "        \"api_key\": API_KEY,\n",
    "        \"language\": \"en-US\",\n",
    "        \"sort_by\": \"primary_release_date.asc\",\n",
    "        \"primary_release_date.gte\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"primary_release_date.lte\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"vote_count.gte\": 30,\n",
    "        \"include_adult\": \"false\",\n",
    "        \"page\": 1,\n",
    "    }\n",
    "    url = f\"{BASE_URL}/discover/movie\"\n",
    "    data = await fetch_json(session, url, params)\n",
    "    if not data:\n",
    "        return 0\n",
    "    return int(data.get(\"total_pages\", 0))\n",
    "\n",
    "\n",
    "async def split_ranges(session, start_dt: datetime, end_dt: datetime, limit: int = 500):\n",
    "    \"\"\"\n",
    "    ë‚ ì§œ êµ¬ê°„ì„ ì—¬ëŸ¬ ê°œë¡œ ë‚˜ëˆ„ì–´\n",
    "    ê° êµ¬ê°„ì˜ discover total_pagesê°€ limit ì´í•˜ê°€ ë˜ë„ë¡ ë¶„í• .\n",
    "\n",
    "    - ë§Œì•½ \"ë‹¨ì¼ ë‚ ì§œ(s == e)\"ì—ì„œì¡°ì°¨ total_pages > limitê°€ ë‚˜ì˜¤ë©´\n",
    "      TMDB API í•œê³„ë¡œ ì¸í•´ ê·¸ ë‚ ì§œëŠ” ì „ìˆ˜ ìˆ˜ì§‘ ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ\n",
    "      RuntimeErrorë¥¼ ë°œìƒì‹œì¼œ ì¡°ìš©í•œ ëˆ„ë½ì„ ë§‰ìŒ.\n",
    "    \"\"\"\n",
    "    queue = deque([(start_dt, end_dt)])\n",
    "    out = []\n",
    "\n",
    "    while queue:\n",
    "        s, e = queue.popleft()\n",
    "        if s > e:\n",
    "            continue\n",
    "\n",
    "        total_pages = await discover_total_pages(session, s, e)\n",
    "        if total_pages == 0:\n",
    "            continue\n",
    "\n",
    "        if total_pages <= limit:\n",
    "            out.append((s, e, total_pages))\n",
    "            continue\n",
    "\n",
    "        days = (e - s).days\n",
    "\n",
    "        # ë‹¨ì¼ ë‚ ì§œì¸ë°ë„ 500í˜ì´ì§€ ì´ˆê³¼ â†’ TMDB discover í•œê³„ë¡œ ì „ìˆ˜ ìˆ˜ì§‘ ë¶ˆê°€\n",
    "        if days <= 0:\n",
    "            msg = (\n",
    "                f\"âŒ TMDB discover í•œê³„ ì´ˆê³¼: {s.date()} ì˜ total_pages={total_pages} > {limit}\\n\"\n",
    "                f\"   ì´ ë‚ ì§œì— ëŒ€í•´ discover/movie APIë¡œëŠ” 500í˜ì´ì§€(10,000í¸) ì´ìƒì„ ì „ìˆ˜ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\\n\"\n",
    "                f\"   (primary_release_dateëŠ” ë‚ ì§œ ë‹¨ìœ„ë§Œ ì§€ì›, ì‹œê°„ ë‹¨ìœ„ ë¶„í•  ë¶ˆê°€)\\n\"\n",
    "                f\"   â†’ í•„í„° ì¡°ê±´ì„ ì™„í™”/ë³€ê²½í•˜ê±°ë‚˜, ì´ ë‚ ì§œë¥¼ ë¶„ì„ ë²”ìœ„ì—ì„œ ì œì™¸í•´ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "            )\n",
    "            raise RuntimeError(msg)\n",
    "\n",
    "        # ë‚ ì§œ êµ¬ê°„ì„ ë°˜ì”© ë‚˜ëˆ„ì–´ total_pagesê°€ 500 ì´í•˜ê°€ ë˜ë„ë¡ ì¬ê·€ ë¶„í• \n",
    "        mid_days = days // 2\n",
    "        mid = s + timedelta(days=mid_days)\n",
    "\n",
    "        # ì™¼ìª½: s ~ mid\n",
    "        queue.append((s, mid))\n",
    "        # ì˜¤ë¥¸ìª½: mid+1 ~ e\n",
    "        if mid < e:\n",
    "            queue.append((mid + timedelta(days=1), e))\n",
    "\n",
    "    return sorted(out, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "async def discover_movie_ids(session):\n",
    "    print(\"ğŸ“Œ Discover ë‹¨ê³„ ì‹œì‘ (movie_id ìˆ˜ì§‘)\")\n",
    "\n",
    "    ranges = await split_ranges(session, START_DATE, END_DATE, limit=500)\n",
    "    print(f\"ğŸ“… ë¶„í• ëœ ë‚ ì§œ êµ¬ê°„: {len(ranges)}\")\n",
    "\n",
    "    all_ids = set()\n",
    "    url = f\"{BASE_URL}/discover/movie\"\n",
    "\n",
    "    for idx, (s, e, pages) in enumerate(ranges, start=1):\n",
    "        print(f\"  â–· [{idx}/{len(ranges)}] {s.date()} ~ {e.date()} (pages={pages})\")\n",
    "\n",
    "        for p in range(1, pages + 1):\n",
    "            params = {\n",
    "                \"api_key\": API_KEY,\n",
    "                \"language\": \"en-US\",\n",
    "                \"sort_by\": \"primary_release_date.asc\",\n",
    "                \"primary_release_date.gte\": s.strftime(\"%Y-%m-%d\"),\n",
    "                \"primary_release_date.lte\": e.strftime(\"%Y-%m-%d\"),\n",
    "                \"vote_count.gte\": 30,\n",
    "                \"include_adult\": \"false\",\n",
    "                \"page\": p,\n",
    "            }\n",
    "            data = await fetch_json(session, url, params)\n",
    "            if not data:\n",
    "                continue\n",
    "\n",
    "            for item in data.get(\"results\", []):\n",
    "                if item.get(\"vote_count\", 0) >= 30:\n",
    "                    all_ids.add(item[\"id\"])\n",
    "\n",
    "    ids = sorted(all_ids)\n",
    "    MOVIE_IDS_PATH.write_text(json.dumps(ids))\n",
    "    print(f\"ğŸ‰ Discover ì™„ë£Œ: ê³ ìœ  movie_id {len(ids):,}ê°œ í™•ë³´\")\n",
    "\n",
    "    return ids\n",
    "\n",
    "# ==========================================================\n",
    "# EXTRACTORS\n",
    "# ==========================================================\n",
    "def extract_directors(crew):\n",
    "    if not crew:\n",
    "        return \"\", \"\", \"\", \"\"\n",
    "\n",
    "    seen = {}\n",
    "    for c in crew:\n",
    "        if c.get(\"job\") == \"Director\":\n",
    "            pid = c.get(\"id\")\n",
    "            if pid and pid not in seen:\n",
    "                seen[pid] = c\n",
    "\n",
    "    if not seen:\n",
    "        return \"\", \"\", \"\", \"\"\n",
    "\n",
    "    arr = sorted(seen.values(), key=lambda x: x[\"id\"])\n",
    "\n",
    "    names    = join_values([a.get(\"name\") for a in arr], use_strong_sanitize=False)\n",
    "    ids      = join_values([a.get(\"id\") for a in arr], use_strong_sanitize=False)\n",
    "    genders  = join_values([a.get(\"gender\") for a in arr], use_strong_sanitize=False)\n",
    "    profiles = join_values([a.get(\"profile_path\") for a in arr], use_strong_sanitize=False)\n",
    "\n",
    "    return names, ids, genders, profiles\n",
    "\n",
    "\n",
    "def extract_writers(crew):\n",
    "    \"\"\"\n",
    "    ì‘ê°€ ì¶”ì¶œ:\n",
    "    - jobì´ WRITER_JOBSì— ìˆëŠ” ê²½ìš°\n",
    "    - ë˜ëŠ” department == \"Writing\" ì´ë©´ì„œ jobì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°\n",
    "    \"\"\"\n",
    "    if not crew:\n",
    "        return \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "    seen = {}\n",
    "\n",
    "    for c in crew:\n",
    "        job  = c.get(\"job\")\n",
    "        dept = c.get(\"department\")\n",
    "        pid  = c.get(\"id\")\n",
    "\n",
    "        if not pid:\n",
    "            continue\n",
    "\n",
    "        if job in WRITER_JOBS:\n",
    "            if pid not in seen:\n",
    "                seen[pid] = {\"person\": c, \"jobs\": set()}\n",
    "            seen[pid][\"jobs\"].add(job)\n",
    "        elif dept == \"Writing\" and job:\n",
    "            if pid not in seen:\n",
    "                seen[pid] = {\"person\": c, \"jobs\": set()}\n",
    "            seen[pid][\"jobs\"].add(job)\n",
    "\n",
    "    if not seen:\n",
    "        return \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "    arr = sorted(seen.items(), key=lambda x: x[0])\n",
    "\n",
    "    names    = join_values([v[\"person\"].get(\"name\") for _, v in arr], use_strong_sanitize=False)\n",
    "    ids      = join_values([v[\"person\"].get(\"id\") for _, v in arr], use_strong_sanitize=False)\n",
    "    roles    = join_values([\n",
    "        \"/\".join(sorted(v[\"jobs\"])) if v[\"jobs\"] else \"Writer\"\n",
    "        for _, v in arr\n",
    "    ], use_strong_sanitize=False)\n",
    "    genders  = join_values([v[\"person\"].get(\"gender\") for _, v in arr], use_strong_sanitize=False)\n",
    "    profiles = join_values([v[\"person\"].get(\"profile_path\") for _, v in arr], use_strong_sanitize=False)\n",
    "\n",
    "    return names, ids, roles, genders, profiles\n",
    "\n",
    "\n",
    "def extract_top_cast(cast):\n",
    "    \"\"\"\n",
    "    ì£¼ì—° ë°°ìš° Top 5 ì¶”ì¶œ\n",
    "    - castë¥¼ order ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬\n",
    "    - characterê°€ ìˆëŠ” í•­ëª©ë§Œ ì‚¬ìš©\n",
    "    - ìƒìœ„ 5ëª…ê¹Œì§€\n",
    "    - ì´ë¦„ì€ ê²½ë¯¸ ì •ì œ, characterëŠ” ê°•ë ¥ ì •ì œ\n",
    "    \"\"\"\n",
    "    if not cast:\n",
    "        return \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "    sorted_cast = sorted(cast, key=lambda x: x.get(\"order\", 999))\n",
    "    filtered = [c for c in sorted_cast if c.get(\"character\")]\n",
    "\n",
    "    top_5 = filtered[:5]\n",
    "    if not top_5:\n",
    "        return \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "    names      = join_values([c.get(\"name\") for c in top_5], use_strong_sanitize=False)\n",
    "    ids        = join_values([c.get(\"id\") for c in top_5], use_strong_sanitize=False)\n",
    "    orders     = join_values([c.get(\"order\") for c in top_5], use_strong_sanitize=False)\n",
    "    characters = join_values([c.get(\"character\") for c in top_5], use_strong_sanitize=True)\n",
    "    genders    = join_values([c.get(\"gender\") for c in top_5], use_strong_sanitize=False)\n",
    "    profiles   = join_values([c.get(\"profile_path\") for c in top_5], use_strong_sanitize=False)\n",
    "\n",
    "    return names, ids, orders, characters, genders, profiles\n",
    "\n",
    "# ==========================================================\n",
    "# MOVIE DETAILS FETCHER\n",
    "# ==========================================================\n",
    "async def fetch_movie_detail(session, movie_id: int):\n",
    "    url = f\"{BASE_URL}/movie/{movie_id}\"\n",
    "    params = {\n",
    "        \"api_key\": API_KEY,\n",
    "        \"language\": \"en-US\",\n",
    "        \"append_to_response\": \"credits\",\n",
    "    }\n",
    "\n",
    "    data = await fetch_json(session, url, params, desc=f\"movie/{movie_id}\")\n",
    "    if not data:\n",
    "        return None\n",
    "\n",
    "    imdb_id = data.get(\"imdb_id\")\n",
    "    if not imdb_id:\n",
    "        return None\n",
    "\n",
    "    vote_count = data.get(\"vote_count\", 0)\n",
    "    if vote_count < 30:\n",
    "        return None\n",
    "\n",
    "    title = sanitize_text_mild(data.get(\"title\"))\n",
    "\n",
    "    credits = data.get(\"credits\", {})\n",
    "    cast = credits.get(\"cast\", []) or []\n",
    "    crew = credits.get(\"crew\", []) or []\n",
    "\n",
    "    directors, director_ids, director_gender, director_profile = extract_directors(crew)\n",
    "    writers, writer_ids, writer_roles, writer_gender, writer_profile = extract_writers(crew)\n",
    "    top_cast, top_cast_ids, top_cast_order, characters, top_cast_gender, top_cast_profile = extract_top_cast(cast)\n",
    "\n",
    "    return {\n",
    "        \"imdb_id\": imdb_id,\n",
    "        \"movie_id\": movie_id,\n",
    "        \"vote_count\": vote_count,\n",
    "        \"title\": title,\n",
    "\n",
    "        \"director_ids\": director_ids,\n",
    "        \"directors\": directors,\n",
    "        \"director_gender\": director_gender,\n",
    "        \"director_profile_path\": director_profile,\n",
    "\n",
    "        \"writer_ids\": writer_ids,\n",
    "        \"writers\": writers,\n",
    "        \"writer_roles\": writer_roles,\n",
    "        \"writer_gender\": writer_gender,\n",
    "        \"writer_profile_path\": writer_profile,\n",
    "\n",
    "        \"top_cast_ids\": top_cast_ids,\n",
    "        \"top_cast_order\": top_cast_order,\n",
    "        \"top_cast\": top_cast,\n",
    "        \"character\": characters,\n",
    "        \"top_cast_gender\": top_cast_gender,\n",
    "        \"top_cast_profile_path\": top_cast_profile,\n",
    "    }\n",
    "\n",
    "# ==========================================================\n",
    "# CHECKPOINT\n",
    "# ==========================================================\n",
    "def load_checkpoint():\n",
    "    if CHECKPOINT_PATH.exists():\n",
    "        try:\n",
    "            data = json.loads(CHECKPOINT_PATH.read_text())\n",
    "            return {\n",
    "                \"done_ids\": set(data.get(\"done_ids\", [])),\n",
    "                \"failed_ids\": set(data.get(\"failed_ids\", [])),\n",
    "                \"last_index\": data.get(\"last_index\", 0),\n",
    "            }\n",
    "        except Exception:\n",
    "            return {\"done_ids\": set(), \"failed_ids\": set(), \"last_index\": 0}\n",
    "    return {\"done_ids\": set(), \"failed_ids\": set(), \"last_index\": 0}\n",
    "\n",
    "\n",
    "async def save_checkpoint(done_ids, failed_ids, last_index: int):\n",
    "    async with checkpoint_lock:\n",
    "        try:\n",
    "            data = {\n",
    "                \"done_ids\": list(done_ids),\n",
    "                \"failed_ids\": list(failed_ids),\n",
    "                \"last_index\": last_index,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "            tmp_path = CHECKPOINT_PATH.with_suffix(\".tmp\")\n",
    "            tmp_path.write_text(json.dumps(data, ensure_ascii=False, indent=2))\n",
    "            tmp_path.replace(CHECKPOINT_PATH)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "\n",
    "def load_done_from_jsonl():\n",
    "    if not DETAILS_JSONL_PATH.exists():\n",
    "        return set()\n",
    "\n",
    "    done = set()\n",
    "    with open(DETAILS_JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "                mid = rec.get(\"movie_id\")\n",
    "                if mid:\n",
    "                    done.add(mid)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return done\n",
    "\n",
    "\n",
    "def save_failed_ids(failed_ids):\n",
    "    FAILED_IDS_PATH.write_text(\n",
    "        json.dumps(sorted(list(failed_ids)), ensure_ascii=False, indent=2)\n",
    "    )\n",
    "\n",
    "# ==========================================================\n",
    "# BATCH PROCESSING\n",
    "# ==========================================================\n",
    "async def process_batch(session, batch, done_ids: set, failed_ids: set):\n",
    "    sem = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "    async def worker(mid: int):\n",
    "        if mid in done_ids or mid in failed_ids:\n",
    "            return None\n",
    "        async with sem:\n",
    "            result = await fetch_movie_detail(session, mid)\n",
    "            if result is None:\n",
    "                failed_ids.add(mid)\n",
    "                return None\n",
    "            return result\n",
    "\n",
    "    tasks = [asyncio.create_task(worker(mid)) for mid in batch]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    valid = [r for r in results if r is not None]\n",
    "\n",
    "    if valid:\n",
    "        async with jsonl_lock:\n",
    "            try:\n",
    "                lines = [json.dumps(r, ensure_ascii=False) + \"\\n\" for r in valid]\n",
    "                with open(DETAILS_JSONL_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.writelines(lines)\n",
    "\n",
    "                for r in valid:\n",
    "                    done_ids.add(r[\"movie_id\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ JSONL ì“°ê¸° ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    return len(valid)\n",
    "\n",
    "\n",
    "def chunked(arr, n):\n",
    "    for i in range(0, len(arr), n):\n",
    "        yield arr[i:i+n]\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN\n",
    "# ==========================================================\n",
    "async def main():\n",
    "    start = datetime.now()\n",
    "\n",
    "    async with aiohttp.ClientSession(headers=HEADERS) as session:\n",
    "        if MOVIE_IDS_PATH.exists():\n",
    "            movie_ids = json.loads(MOVIE_IDS_PATH.read_text())\n",
    "            print(f\"ğŸ“‚ ê¸°ì¡´ movie_ids ë¡œë“œ: {len(movie_ids):,}\")\n",
    "        else:\n",
    "            movie_ids = await discover_movie_ids(session)\n",
    "\n",
    "        checkpoint = load_checkpoint()\n",
    "        done_ids   = checkpoint[\"done_ids\"]\n",
    "        failed_ids = checkpoint[\"failed_ids\"]\n",
    "\n",
    "        done_ids |= load_done_from_jsonl()\n",
    "\n",
    "        remaining = [mid for mid in movie_ids if mid not in done_ids and mid not in failed_ids]\n",
    "        print(f\"â³ ì²˜ë¦¬ ëŒ€ìƒ: {len(remaining):,}\")\n",
    "        print(f\"   ì™„ë£Œ: {len(done_ids):,}, ì‹¤íŒ¨: {len(failed_ids):,}\")\n",
    "\n",
    "        if not DETAILS_JSONL_PATH.exists():\n",
    "            DETAILS_JSONL_PATH.touch()\n",
    "\n",
    "        BATCH_SIZE = 400\n",
    "        total_processed = 0\n",
    "\n",
    "        batches = list(chunked(remaining, BATCH_SIZE))\n",
    "\n",
    "        for idx, batch in enumerate(batches, start=1):\n",
    "            batch_start = datetime.now()\n",
    "            n = await process_batch(session, batch, done_ids, failed_ids)\n",
    "            total_processed += n\n",
    "\n",
    "            elapsed = (datetime.now() - batch_start).total_seconds()\n",
    "            print(f\"  â–· Batch {idx}/{len(batches)}: {n}ê°œ ì²˜ë¦¬, ëˆ„ì ={total_processed}, {elapsed:.1f}s\")\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                await save_checkpoint(done_ids, failed_ids, idx)\n",
    "                save_failed_ids(failed_ids)\n",
    "                print(f\"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (batch={idx})\")\n",
    "\n",
    "        await save_checkpoint(done_ids, failed_ids, len(batches))\n",
    "        save_failed_ids(failed_ids)\n",
    "\n",
    "    # JSONL â†’ Parquet\n",
    "    print(\"\\nğŸ“¦ JSONL â†’ Parquet ë³€í™˜ ì¤‘...\")\n",
    "\n",
    "    if not DETAILS_JSONL_PATH.exists():\n",
    "        print(\"âš ï¸ JSONL íŒŒì¼ì´ ì—†ì–´ Parquet ë³€í™˜ì„ ê±´ë„ˆëœ€\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_json(DETAILS_JSONL_PATH, lines=True)\n",
    "\n",
    "    df = df[df[\"imdb_id\"].notna()]\n",
    "    df = df[df[\"movie_id\"].notna()]\n",
    "    df = df.drop_duplicates(subset=[\"movie_id\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    cols = [\n",
    "        \"imdb_id\", \"movie_id\", \"vote_count\", \"title\",\n",
    "        \"director_ids\", \"directors\", \"director_gender\", \"director_profile_path\",\n",
    "        \"writer_ids\", \"writers\", \"writer_roles\", \"writer_gender\", \"writer_profile_path\",\n",
    "        \"top_cast_ids\", \"top_cast_order\", \"top_cast\", \"character\",\n",
    "        \"top_cast_gender\", \"top_cast_profile_path\",\n",
    "    ]\n",
    "\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"âš ï¸ ëˆ„ë½ ì»¬ëŸ¼ ìë™ ìƒì„±: {missing}\")\n",
    "        for c in missing:\n",
    "            df[c] = \"\"\n",
    "\n",
    "    df = df[cols]\n",
    "\n",
    "    df[\"movie_id\"]   = df[\"movie_id\"].astype(\"int32\")\n",
    "    df[\"vote_count\"] = df[\"vote_count\"].astype(\"int32\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col not in [\"movie_id\", \"vote_count\"]:\n",
    "            df[col] = df[col].astype(\"string\")\n",
    "\n",
    "    df.to_parquet(FINAL_PARQUET_PATH, index=False)\n",
    "\n",
    "    elapsed = (datetime.now() - start).total_seconds() / 60\n",
    "    print(f\"\\nğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ({elapsed:.1f}ë¶„)\")\n",
    "    print(f\"ğŸ“ ì €ì¥ íŒŒì¼: {FINAL_PARQUET_PATH}\")\n",
    "    print(f\"   ì´ ì˜í™” ìˆ˜: {len(df):,}í¸\")\n",
    "    print(f\"   ì‹¤íŒ¨ ì˜í™” ìˆ˜: {len(json.loads(FAILED_IDS_PATH.read_text())) if FAILED_IDS_PATH.exists() else 0:,}í¸\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
